{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasminiSantos/Trabalho-5-Bag-of-Words/blob/main/Trabalho_5_Bag_of_Words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Jasmini Rebecca Gomes dos Santos**"
      ],
      "metadata": {
        "id": "tSoNNHIyQBFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extração de sentenças das urls de websites com temas relacionados à linguagem de processamento natural"
      ],
      "metadata": {
        "id": "obVadIVKBWnW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5lYjzl_skyp",
        "outputId": "6041b980-f7dd-4a50-ac5e-ac450c2fb212"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***************************************\n",
            "Url: https://www.ibm.com/cloud/learn/natural-language-processing\n",
            "Number of words: 1795\n",
            "Number of sentences: 99\n",
            "List of sentences: \n",
            "['Skip to content', 'IBM Cloud Learn Hub', 'What is Natural Language Processing?', 'Natural Language Processing (NLP)', 'By:', 'IBM Cloud Education', '2 July  2020', 'Artificial intelligence', 'What is natural language processing?', 'NLP tasks', 'NLP tools and approaches', 'NLP use cases', 'Natural language processing and IBM Watson', 'Jump to ...', 'What is natural language processing?', 'NLP tasks', 'NLP tools and approaches', 'NLP use cases', 'Natural language processing and IBM Watson', 'Natural Language Processing (NLP)', 'Natural language processing strives to build machines that understand and respond to text or voice data—and respond with text or speech of their own—in much the same way humans do.', 'What is natural language processing?', 'Natural language processing (NLP) refers to the branch of computer science—and more specifically, the branch of artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.', 'NLP combines computational linguistics—rule-based modeling of human language—with statistical, machine learning, and deep learning models.', 'Together, these technologies enable computers to process human language in the form of text or voice data and to ‘understand’ its full meaning, complete with the speaker or writer’s intent and sentiment.', 'NLP drives computer programs that translate text from one language to another, respond to spoken commands, and summarize large volumes of text rapidly—even in real time.', 'There’s a good chance you’ve interacted with NLP in the form of voice-operated GPS systems, digital assistants, speech-to-text dictation software, customer service chatbots, and other consumer conveniences.', 'But NLP also plays a growing role in enterprise solutions that help streamline business operations, increase employee productivity, and simplify mission-critical business processes.', 'NLP tasks', 'Human language is filled with ambiguities that make it incredibly difficult to write software that accurately determines the intended meaning of text or voice data.', 'Homonyms, homophones, sarcasm, idioms, metaphors, grammar and usage exceptions, variations in sentence structure—these just a few of the irregularities of human language that take humans years to learn, but that programmers must teach natural language-driven applications to recognize and understand accurately from the start, if those applications are going to be useful.', \"Several NLP tasks break down human text and voice data in ways that help the computer make sense of what it's ingesting.\", 'Some of these tasks include the following:', 'Speech recognition, also called speech-to-text, is the task of reliably converting voice data into text data.', 'Speech recognition is required for any application that follows voice commands or answers spoken questions.', 'What makes speech recognition especially challenging is the way people talk—quickly, slurring words together, with varying emphasis and intonation, in different accents, and often using incorrect grammar.', 'Part of speech tagging, also called grammatical tagging, is the process of determining the part of speech of a particular word or piece of text based on its use and context.', 'Part of speech identifies ‘make’ as a verb in ‘I can make a paper plane,’ and as a noun in ‘What make of car do you own?’', 'Word sense disambiguation is the selection of the meaning of a word with multiple meanings  through a process of semantic analysis that determine the word that makes the most sense in the given context.', \"For example, word sense disambiguation helps distinguish the meaning of the verb 'make' in ‘make the grade’ (achieve) vs. ‘make a bet’ (place).\", 'Named entity recognition, or NEM, identifies words or phrases as useful entities.', \"NEM identifies ‘Kentucky’ as a location or ‘Fred’ as a man's name.\", 'Co-reference resolution is the task of identifying if and when two words refer to the same entity.', \"The most common example is determining the person or object to which a certain pronoun refers (e.g., ‘she’ = ‘Mary’),  but it can also involve identifying a metaphor or an idiom in the text  (e.g., an instance in which 'bear' isn't an animal but a large hairy person).\", 'Sentiment analysis attempts to extract subjective qualities—attitudes, emotions, sarcasm, confusion, suspicion—from text.', \"Natural language generation is sometimes described as the opposite of speech recognition or speech-to-text; it's the task of putting structured information into human language.\", 'See the blog post “NLP vs. NLU vs. NLG: the differences between three natural language processing concepts” for a deeper look into how these concepts relate.', 'NLP tools and approaches', 'Python and the Natural Language Toolkit (NLTK)', 'The Python programing language provides a wide range of tools and libraries for attacking specific NLP tasks.', 'Many of these are found in the Natural Language Toolkit, or NLTK, an open source collection of libraries, programs, and education resources for building NLP programs.', 'The NLTK includes libraries for many of the NLP tasks listed above, plus libraries for subtasks, such as sentence parsing, word segmentation, stemming and lemmatization (methods of trimming words down to their roots), and tokenization (for breaking phrases, sentences, paragraphs and passages into tokens that help the computer better understand the text).', 'It also includes libraries for implementing capabilities such as semantic reasoning, the ability to reach logical conclusions based on facts extracted from text.', 'Statistical NLP, machine learning, and deep learning', \"The earliest NLP applications were hand-coded, rules-based systems that could perform certain NLP tasks, but couldn't easily scale to accommodate a seemingly endless stream of exceptions or the increasing volumes of text and voice data.\", 'Enter statistical NLP, which combines computer algorithms with machine learning and deep learning models to automatically extract, classify, and label elements of text and voice data and then assign a statistical likelihood to each possible meaning of those elements.', \"Today, deep learning models and learning techniques based on convolutional neural networks (CNNs) and recurrent neural networks (RNNs) enable NLP systems that 'learn' as they work and extract ever more accurate meaning from huge volumes of raw, unstructured, and unlabeled text and voice data sets.\", 'For a deeper dive into the nuances between these technologies and their learning approaches, see “AI vs. Machine Learning vs. Deep Learning vs. Neural Networks: What’s the Difference?”', 'NLP use cases', 'Natural language processing is the driving force behind machine intelligence in many modern real-world applications.', 'Here are a few examples:', \"Spam detection: You may not think of spam detection as an NLP solution, but the best spam detection technologies use NLP's text classification capabilities to scan emails for language that often indicates spam or phishing.\", 'These indicators can include overuse of financial terms, characteristic bad grammar, threatening language, inappropriate urgency, misspelled company names, and more.', \"Spam detection is one of a handful of NLP problems that experts consider 'mostly solved' (although you may argue that this doesn’t match your email experience).\", 'Machine translation: Google Translate is an example of widely available NLP technology at work.', 'Truly useful machine translation involves more than replacing words in one language with words of another.', 'Effective translation has to capture accurately the meaning and tone of the input language and translate it to text with the same meaning and desired impact in the output language.', 'Machine translation tools are making good progress in terms of accuracy.', 'A great way to test any machine translation tool is to translate text to one language and then back to the original.', 'An oft-cited classic example: Not long ago, translating “The spirit is willing but the flesh is weak” from English to Russian and back yielded “The vodka is good but the meat is rotten.”', 'Today, the result is “The spirit desires, but the flesh is weak,” which isn’t perfect, but inspires much more confidence in the English-to-Russian translation.', \"Virtual agents and chatbots: Virtual agents such as Apple's Siri and Amazon's Alexa use speech recognition to recognize patterns in voice commands and natural language generation to respond with appropriate action or helpful comments.\", 'Chatbots perform the same magic in response to typed text entries.', 'The best of these also learn to recognize contextual clues about human requests and use them to provide even better responses or options over time.', 'The next enhancement for these applications is question answering, the ability to respond to our questions—anticipated or not—with relevant and helpful answers in their own words.', 'Social media sentiment analysis: NLP has become an essential business tool for uncovering hidden data insights from social media channels.', 'Sentiment analysis can analyze language used in social media posts, responses, reviews, and more to extract attitudes and emotions in response to products, promotions, and events–information companies can use in product designs, advertising campaigns, and more.', \"Text summarization: Text summarization uses NLP techniques to digest huge volumes of digital text and create summaries and synopses for indexes, research databases, or busy readers who don't have time to read full text.\", 'The best text summarization applications use semantic reasoning and natural language generation (NLG) to add useful context and conclusions to summaries.', 'Natural language processing and IBM Watson', 'IBM has innovated in the artificial intelligence space by pioneering NLP-driven tools and services that enable organizations to automate their complex business processes while gaining essential business insights.', 'These tools include:', 'Watson Discovery - Surface high-quality answers and rich insights from your complex enterprise documents - tables, PDFs, big data and more - with AI search.', 'Enable your employees to make more informed decisions and save time with real-time search engine and text mining capabilities that perform text extraction and analyze relationships and patterns buried in unstructured data.', 'Watson Discovery leverages custom NLP models and machine learning methods to provide users with AI that understands the unique language of their industry and business.', 'Explore Watson Discovery', 'Watson Natural Language Understanding (NLU) - Analyze text in unstructured data formats including HTML, webpages, social media, and more.', 'Increase your understanding of human language by leveraging this natural language tool kit to identify concepts, keywords, categories, semantics, and emotions, and to perform text classification, entity extraction, named entity recognition (NER), sentiment analysis, and summarization.', 'Explore Watson Natural Language Understanding', 'Watson Assistant - Improve the customer experience while reducing costs.', 'Watson Assistant is an AI chatbot with an easy-to-use visual builder so you can deploy virtual agents across any channel, in minutes.', 'Explore Watson Assistant', 'Purpose-built for healthcare and life sciences domains, IBM Watson Annotator for Clinical Data extracts key clinical concepts from natural language text, like conditions, medications, allergies and procedures.', 'Deep contextual insights and values for key clinical attributes develop more meaningful data.', 'Potential data sources include clinical notes, discharge summaries, clinical trial protocols and literature data.', \"For more information on how to get started with one of IBM Watson's natural language processing technologies, visit the IBM Watson Natural Language Processing page.\", 'Sign up for an IBMid and create your IBM Cloud account.', 'Featured products', 'IBM Watson Assistant']\n",
            "***************************************\n",
            "***************************************\n",
            "Url: https://www.dominodatalab.com/blog/natural-language-in-python-using-spacy\n",
            "Number of words: 3366\n",
            "Number of sentences: 428\n",
            "List of sentences: \n",
            "['Skip to content', 'Platform', 'Platform', 'Domino Enterprise MLOps Platform', 'Platform Components', 'System of Record', 'Integrated Model Factory', 'Self Service Infrastructure Portal', 'Explore', 'Pricing', 'Nexus', 'Platform Updates', 'Platform', 'Solutions', 'By Role', 'Chief Data & Analytics Executives', 'Data Science Leaders', 'Data Scientists', 'IT Leaders', 'By Industry', 'Financial Services', 'Health & Life Sciences', 'Insurance', 'See More', 'Use Cases', 'Self-Service Data Science', 'Open Data Science', 'Model Risk Management', 'Cloud Data Science', 'Solutions', 'Customers', 'Resources', 'Guides, Videos & More', 'Blog', 'Events', 'Podcast', 'Learning', 'Community', 'Documentation', 'Partners', 'Partners', 'Tools & Data', 'Infrastructure', 'Solutions', 'Implementation & Consulting', 'Become a Partner', 'Featured', 'Azure', 'Accenture', 'Snowflake', 'Partners', 'Company', 'About', 'Careers', \"We're Hiring\", 'News & Press', 'Contact', 'Watch Demo', 'Try Now', 'Platform', 'Domino Enterprise MLOps Platform', 'System of Record', 'Integrated Model Factory', 'Self Service Infrastructure Portal', 'Pricing', 'Nexus', 'Platform Updates', 'Self-Service Data Science', 'Open Data Science', 'Model Risk Management', 'Cloud Data Science', 'Solutions', 'Data Science Leaders', 'IT Science', 'Data Scientists', 'Executive', 'Financial Services', 'Insurance', 'Media & Technology', 'Health & Life Sciences', 'Manufacturing', 'Retail, eCommerce & Consumer Products', 'Customers', 'Resources', 'Guides, Videos & More', 'Data Science Blog', 'Enterprise Field Guides', 'Events', 'Podcast', 'Learning', 'Community', 'Documentation', 'Comparison Guides', 'Partners', 'Tools & Data', 'Infrastructure', 'Solutions', 'Implementation & Consulting', 'Become a Partner', 'Company', 'About', \"Careers We're Hiring\", 'News & Press', 'Contact', 'Watch Demo', 'Try Now', 'Data Science', 'Natural Language Processing in Python using spaCy: An Introduction', 'by Paco Nathan', 'September 10, 2019', '17 min read', 'This article provides a brief introduction to natural language using spaCy and related libraries in Python.', 'Introduction', 'Data science teams in the industry must work with lots of text, one of the top four categories of data used in machine learning.', \"Usually, it's human-generated text, but not always.\", 'Think about it: how does the \"operating system\" for business work?', 'Typically, there are contracts (sales contracts, work agreements, partnerships), there are invoices, there are insurance policies, there are regulations and other laws, and so on.', 'All of those are represented as text.', 'You may run across a few acronyms: natural language processing (NLP), natural language understanding (NLU), natural language generation (NLG)—which are roughly speaking \"read text\", \"understand meaning\", \"write text\" respectively.', 'Increasingly these tasks overlap and it becomes difficult to categorize any given feature.', 'What is Spacy?', 'How do data science teams go about processing unstructured text data?', 'Oftentimes teams turn to various libraries in Python to manage complex NLP tasks.', 'sPacy is an open-source Python library that provides capabilities to conduct advanced natural language processing analysis and build models that can underpin document analysis, chatbot capabilities, and all other forms of text analysis.', 'The spaCy framework—along with a wide and growing range of plug-ins and other integrations—provides features for a wide range of natural language tasks.', \"It's become one of the most widely used natural language libraries in Python for industry use cases, and has quite a large community—and with that, much support for commercialization of research advances as this area continues to evolve rapidly.\", 'This article provides a brief introduction to working with natural language (sometimes called \"text analytics\") in Python using spaCy and related libraries.', 'Getting Started', \"We have configured the default Compute Environment in Domino to include all of the packages, libraries, models, and data you'll need for this tutorial.  Check out the Domino project to run the code.\", \"If you're interested in how Domino's Compute Environments work, check out the Support Page.\", \"Now let's load spaCy and run some code:\", 'import spacy', 'nlp = spacy.load(\"en_core_web_sm\")', 'That nlp variable is now your gateway to all things spaCy and loaded with the en_core_web_sm small model for English.', 'Next, let\\'s run a small \"document\" through the natural language parser:', 'text = \"The rain in Spain falls mainly on the plain.', 'doc = nlp(text)', 'for token in doc:', 'print(token.text, token.lemma_, token.pos_, token.is_stop)', 'The DET Truerain rain NOUN Falsein in ADP', 'TrueSpain Spain PROPN Falsefalls fall VERB', 'Falsemainly mainly ADV Falseon on ADP Truethe DET Trueplain plain NOUN False. .', 'PUNCT False', 'First we created a doc from the text, which is a container for a document and all of its annotations.', 'Then we iterated through the document to see what spaCy had parsed.', \"Good, but it's a lot of info and a bit difficult to read.\", \"Let's reformat the spaCy parse of that sentence as a pandas dataframe:\", 'import pandas as pd', 'cols = (\"text\", \"lemma\", \"POS\", \"explain\", \"stopword\")', 'rows =', 'for t in doc:', 'row =', '[t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop]', 'rows.append(row)', 'df = pd.', 'DataFrame(rows, columns=cols)', 'print(df)', 'Much more readable!', 'In this simple case, the entire document is merely one short sentence.', 'For each word in that sentence spaCy has created a token, and we accessed fields in each token to show:', 'raw text', 'lemma – a root form of the word', 'part of speech', 'a flag for whether the word is a stopword—i.e., a common word that may be filtered out', \"Next, let's use the displaCy library to visualize the parse tree for that sentence:\", 'from spacy import displacy', 'displacy.render(doc, style=\"dep\")', 'Does that bring back memories of grade school?', 'Frankly, for those of us coming from more of a computational linguistics background, that diagram sparks joy.', \"But let's backup for a moment.\", 'How do you handle multiple sentences?', 'There are features for sentence boundary detection (SBD)—also known as sentence segmentation—based on the builtin/default sentencizer:', 'text = \"We were all out at the zoo one day, I was doing some acting, walking on the railing of the gorilla exhibit.', 'I fell in.', 'Everyone screamed and Tommy jumped in after me, forgetting that he had blueberries in his front pocket.', 'The gorillas just went wild.', 'doc = nlp(text)', 'for sent in doc.sents:', 'print(\">\", sent)', 'We were all out at the zoo one day, I was doing some acting, walking on the railing of the gorilla exhibit.', 'I fell in.', 'Everyone screamed and Tommy jumped in after me, forgetting that he had blueberries in his front pocket.>', 'The gorillas just went wild.', 'When spaCy creates a document, it uses a principle of non-destructive tokenization, meaning that the tokens, sentences, etc., are simply indexes into a long array.', \"In other words, they don't carve the text stream into little pieces.\", 'So each sentence is a span with a start and an end index into the document array:', 'for sent in doc.sents:', 'print(\">\", sent.start, sent.end)', '> 0', '25> 25 29> 29 48> 48 54', 'We can index into the document array to pull out the tokens for one sentence:', 'doc[48:54]', 'The gorillas just went wild.', 'Or simply index into a specific token, such as the verb went in the last sentence:', 'token = doc[51]', 'print(token.text, token.lemma_, token.pos_)', 'went go VERB', 'At this point, we can parse a document, segment that document into sentences, then look at annotations about the tokens in each sentence.', \"That's a good start.\", 'Acquiring Text', 'Now that we can parse texts, where do we get texts?', 'One quick source is to leverage the interwebs.', \"Of course, when we download web pages we'll get HTML, and then need to extract text from them.\", 'Beautiful Soup is a popular package for that.', 'First, a little housekeeping:', 'import sys', 'import warnings', 'warnings.filterwarnings(\"ignore\")', \"In the following function get_text() we'll parse the HTML to find all of the <p/>tags, then extract the text for those:\", 'from bs4 import BeautifulSoup', 'import requests', 'import traceback', 'def get_text (url):', 'buf =', 'try:', 'soup = BeautifulSoup(requests.get(url).text, \"html.parser\")', 'for p in soup.find_all(\"p\"):', 'buf.append(p.get_text())', 'return \"\".join(buf)', 'except:', 'print(traceback.format_exc())', 'sys.exit(-1)', \"Now let's grab some text from online sources.\", 'We can compare open-source licenses hosted on the Open Source Initiative site:', 'lic = {}lic[\"mit\"] = nlp(get_text(\"https://opensource.org/licenses/MIT\"))', 'lic[\"asl\"] = nlp(get_text(\"https://opensource.org/licenses/Apache-20\"))', 'lic[\"bsd\"] = nlp(get_text(\"https://opensource.org/licenses/BSD-3-Clause\"))', 'for sent in lic[\"bsd\"].sents:', 'print(\">\", sent)', '> SPDX short identifier: BSD-3-Clause> Note: This license has also been called the \"New BSD License\" or \"Modified BSD License\"> See also the 2-clause BSD License....', 'One common use case for natural language work is to compare texts.', 'For example, with those open-source licenses we can download their text, parse, then compare similarity metrics among them:', 'pairs =', '[\"mit\", \"asl\"],', '[\"asl\", \"bsd\"],', '[\"bsd\", \"mit\"]]', 'for a, b in pairs:', 'print(a, b, lic[a].similarity(lic[b]))', 'mit asl 0.9482039305669306asl bsd 0.9391555350757145bsd mit 0.9895838089575453', 'This is interesting since the BSD and MIT licenses appear to be the most similar documents.', 'In fact, they are closely related.', 'Admittedly, there was some extra text included in each document due to the OSI disclaimer in the footer—but this provides a reasonable approximation for comparing the licenses.', 'Natural Language Understanding', \"Now let's dive into some of the spaCy features for NLU.\", 'Given that we have a parse of a document, from a purely grammatical standpoint we can pull the noun chunks, i.e., each of the noun phrases:', 'text = \"Steve Jobs and Steve Wozniak incorporated Apple Computer on January 3, 1977, in Cupertino, California.', 'doc = nlp(text)', 'for chunk in doc.noun_chunks:', 'print(chunk.text)', 'Steve JobsSteve WozniakApple ComputerJanuaryCupertinoCalifornia', 'Not bad.', 'The noun phrases in a sentence generally provide more information content—as a simple filter used to reduce a long document into a more \"distilled\" representation.', 'We can take this approach further and identify named entities within the text, i.e., the proper nouns:', 'for ent in doc.ents:', 'print(ent.text, ent.label_)', 'Steve Jobs PERSONSteve Wozniak PERSONApple Computer ORGJanuary 3, 1977 DATECupertino GPECalifornia GPE', 'The displaCy library provides an excellent way to visualize named entities:', 'displacy.render(doc, style=\"ent\")', \"If you're working with knowledge graph applications and other linked data, your challenge is to construct links between the named entities in a document and other related information for the entities, which is called  entity linking.\", 'Identifying the named entities in a document is the first step in this particular kind of AI work.', 'For example, given the text above, one might link the Steve Wozniaknamed entity to a lookup in DBpedia.', 'In more general terms, one can also link lemmas to resources that describe their meanings.', 'For example, in an early section, we parsed the sentence The gorillas just went wild and were able to show that the lemma for the word went is the verb go.', \"At this point we can use a venerable project called WordNet which provides a lexical database for English—in other words, it's a computable thesaurus.\", \"There's a spaCy integration for WordNet called spacy-wordnet by Daniel Vila Suero, an expert in natural language and knowledge graph work.\", \"Then we'll load the WordNet data via NLTK (these things happen):\", 'import nltk', 'nltk.download(\"wordnet\")', '[nltk_data] Downloading package wordnet to /home/ceteri/nltk_data...', '[nltk_data] Package wordnet is already up-to-date!', 'True', 'Note that spaCy runs as a \"pipeline\" and allows means for customizing parts of the pipeline in use.', \"That's excellent for supporting really interesting workflow integrations in data science work.\", \"Here we'll add the WordnetAnnotator from the spacy-wordnet project:\", 'from spacy_wordnet.wordnet_annotator import WordnetAnnotator', 'print(\"before\", nlp.pipe_names)', 'if \"WordnetAnnotator\" not in nlp.pipe_names:', 'nlp.add_pipe(WordnetAnnotator(nlp.lang), after=\"tagger\")print(\"after\", nlp.pipe_names)', \"before ['tagger', 'parser', 'ner']after ['tagger', 'WordnetAnnotator', 'parser', 'ner']\", 'Within the English language, some words are infamous for having many possible meanings.', 'For example, click through the results online in a WordNet search to find the meanings related to the word withdraw.', \"Now let's use spaCy to perform that lookup automatically:\", 'token = nlp(\"withdraw\")[0]', 'token._.wordnet.synsets()', \"[Synset('withdraw.v.01'),Synset('retire.v.02'),Synset('disengage.v.01'),Synset('recall.v.07'),Synset('swallow.v.05'),Synset('seclude.v.01'),Synset('adjourn.v.02')Synset('bow_out.v.02'),Synset('withdraw.v.09'),Synset('retire.v.08'),Synset('retreat.v.04'),Synset('remove.v.01')]\", 'token._.wordnet.lemmas()', \"[Lemma('withdraw.v.01.withdraw'),Lemma('withdraw.v.01.retreat'),Lemma('withdraw.v.01.pull_away'),Lemma('withdraw.v.01.draw_back'),Lemma('withdraw.v.01.recede'),Lemma('withdraw.v.01.pull_back'),Lemma('withdraw.v.01.retire'),...\", 'token._.wordnet.wordnet_domains()', '[\"astronomy\",\"school\",\"telegraphy\",\"industry\",\"psychology\",\"ethnology\",\"ethnology\",\"administration\",\"school\",\"finance\",\"economy\",\"exchange\",\"banking\",\"commerce\",\"medicine\",\"ethnology\",\"university\",', 'Again, if you are working with knowledge graphs, those \"word sense\" links from WordNet could be used along with graph algorithms to help identify the meanings for a particular word.', 'This can also be used to develop summaries for larger sections of text through a technique called summarization.', \"It's beyond the scope of this tutorial, but an interesting application currently for natural language in the industry.\", 'Going in the other direction, if you know a priori that a document was about a particular domain or set of topics, then you can constrain the meanings returned from WordNet.', 'In the following example, we want to consider NLU results that are within Finance and Banking:', 'domains =', '[\"finance\", \"banking\"]', 'sentence = nlp(\"I want to withdraw 5,000 euros.\")', 'enriched_sent =', 'for token in sentence:', '# get synsets within the desired domains', 'synsets = token._.wordnet.wordnet_synsets_for_domain(domains)', 'if synsets:', 'lemmas_for_synset =', 'for s in synsets:', '# get synset variants and add to the enriched sentence', 'lemmas_for_synset.extend(s.lemma_names())', 'enriched_sent.append(\"({})\".format(\"|\".join(set(lemmas_for_synset))))', 'else:', 'enriched_sent.append(token.text)', 'print(\" \".join(enriched_sent))', 'I (require|want|need) to (draw_off|withdraw|draw|take_out) 5,000 euros .', \"That example may look simple but, if you play with the domains list, you'll find that the results have a kind of combinatorial explosion when run without reasonable constraints.\", \"Imagine having a knowledge graph with millions of elements: you'd want to constrain searches where possible to avoid having every query take days/weeks/months/years to compute.\", 'Text Comparison with spaCy and Scattertext', 'Sometimes the problems encountered when trying to understand a text—or better yet when trying to understand a corpus (a dataset with many related texts)—become so complex that you need to visualize it first.', \"Here's an interactive visualization for understanding texts: scattertext, a product of the genius of Jason Kessler.\", \"Let's analyze text data from the party conventions during the 2012 US Presidential elections.\", 'Note: this cell may take a few minutes to run but the results from all that number-crunching is worth the wait.', 'import scattertext as st', 'if \"merge_entities\" not in nlp.pipe_names:', 'nlp.add_pipe(nlp.create_pipe(\"merge_entities\"))', 'if \"merge_noun_chunks\" not in nlp.pipe_names:', 'nlp.add_pipe(nlp.create_pipe(\"merge_noun_chunks\"))', 'convention_df = st.SampleCorpora.ConventionData2012.get_data()', 'corpus = st.CorpusFromPandas(convention_df, category_col=\"party\", text_col=\"text\", nlp=nlp).build()', 'Once you have the corpus ready, generate an interactive visualization in HTML:', 'html = st.produce_scattertext_explorer(corpus, category=\"democrat\",', 'category_name=\"Democratic\",', 'not_category_name=\"Republican\",', 'width_in_pixels=1000,', 'metadata=convention_df[\"speaker\"])', \"Now we'll render the HTML—give it a minute or two to load, it's worth the wait:\", 'from IPython.display import IFrame', 'file_name = \"foo.html\"', 'with open(file_name, \"wb\") as f:', 'f.write(html.encode(\"utf-8\"))', 'IFrame(src=file_name, width = 1200, height=700)', 'Imagine if you had text from the past three years of customer support for a particular product in your organization.', 'Suppose your team needed to understand how customers have been talking about the product?', 'This scattertext library might come in quite handy!', 'You could cluster (k=2) on NPS scores (a customer evaluation metric) then replace the Democrat/Republican dimension with the top two components from the clustering.', 'Summary', \"Five years ago, if you’d asked about open source in Python for natural language, a default answer from many people working in data science would've been NLTK.\", 'That project includes just about everything but the kitchen sink and has components that are relatively academic.', 'Another popular natural language project is CoreNLP from Stanford.', 'Also quite academic, albeit powerful, though CoreNLP can be challenging to integrate with other software for production use.', 'Then a few years ago everything in this natural language corner of the world began to change.', 'The two principal authors for spaCy, Matthew Honnibal and Ines Montani, launched the project in 2015 and industry adoption was rapid.', \"They focused on an opinionated approach (do what's needed, do it well, no more, no less) which provided simple, rapid integration into data science workflows in Python, as well as faster execution and better accuracy than the alternatives.\", 'Based on these priorities, spaCy became sort of the opposite of NLTK.', 'Since 2015, spaCy has consistently focused on being an open-source project (i.e., depending on its community for directions, integrations, etc.) and being commercial-grade software (not academic research).', 'That said, spaCy has been quick to incorporate the SOTA advances in machine learning, effectively becoming a conduit for moving research into industry.', \"It's important to note that machine learning for natural language got a big boost during the mid-2000's as Google began to win international language translation competitions.\", 'Another big change occurred during 2017-2018 when, following the many successes of deep learning, those approaches began to out-perform previous machine learning models.', 'For example, see the ELMo work on language embedding by Allen AI, followed by BERT from Google, and more recently ERNIE by Baidu—in other words, the search engine giants of the world have gifted the rest of us with a Sesame Street repertoire of open-source embedded language models based on deep learning, which is now state of the art (SOTA).', 'Speaking of which, to keep track of SOTA for natural language keep an eye on NLP-Progress and Papers with Code.', 'The use cases for natural language have shifted dramatically over the past two years, after deep learning techniques arose to the fore.', 'Circa 2014, a natural language tutorial in Python might have shown word count or keyword search or sentiment detection and the target use cases were relatively underwhelming.', \"Circa 2019, we're talking about analyzing thousands of documents for vendor contracts in an industrial supply chain optimization...or hundreds of millions of documents for policyholders of an insurance company or gazillions of documents regarding financial disclosures.\", 'More contemporary natural language work tends to be in NLU, often to support the construction of knowledge graphs, and increasingly in NLG where large numbers of similar documents can be summarized at human scale.', 'The spaCy Universe is a great place to check for deep-dives into particular use cases and to see how this field is evolving.', 'Some selections from this \"universe\" include:', 'Blackstone – parsing unstructured legal texts', 'Kindred – extracting entities from biomedical texts (e.g., Pharma)', 'mordecai – parsing geographic information', 'Prodigy – human-in-the-loop annotation for labeling datasets', 'Rasa NLU – Rasa integration for chat apps', 'Also, a couple super new items to mention:', 'spacy-pytorch-transformers to fine-tune (i.e., use transfer learning with) the Sesame Street characters and friends: BERT, GPT-2, XLNet, etc.', 'spaCy IRL 2019 conference – check out videos from the talks!', \"There's so much more we can do with spaCy— hopefully, this tutorial provides an introduction.\", 'We wish you all the best in your natural language work.', 'Data Science', 'Code', 'Code Featured', 'Machine Learning', 'Practical Techniques', 'Domino', 'spaCy', 'Python', 'Deep Learning', 'Share:', 'Paco Nathan', 'Subscribe to the Domino Newsletter', 'Receive data science tips and tutorials from leading Data Science leaders, right to your inbox.', 'Other posts you might be interested in', 'Data Science', 'Enterprise-class NLP with spaCy v3', 'by David Bloch', '8 min read', 'Data Science', 'Comparing the Functionality of Open Source NLP Libraries', 'In this guest post, Maziyar Panahi and David Talby provide a cheat sheet for choosing open source...', 'by Maziyar Panahi & David Ta...', '5 min read', 'Data Science', 'Getting Data with Beautiful Soup', 'Data is all around us, from the spreadsheets we analyse on a daily basis, to the weather forecast...', 'by Dr J Rogel-Salazar', '22 min read', '135 Townsend St Floor 5San Francisco, CA 94107', '(415) 570-2425', 'Product', 'Domino Enterprise MLOps Platform', 'System of Record', 'Integrated Model Factory', 'Self Service Infrastructure Portal', 'Pricing', 'Solutions', 'Chief Data & Analytics Executives', 'Data Science Leaders', 'Data Scientists', 'IT Leaders', 'Resources', 'Documentation', 'Data Science Blog', 'Enterprise Field Guides', 'Data Science Dictionary', 'Comparison Guides', 'Podcast', 'Company', 'About', 'Contact', \"Careers We're Hiring\", 'News & Press', 'Events', 'Partners', 'Customers']\n",
            "***************************************\n",
            "***************************************\n",
            "Url: https://www.analyticsvidhya.com/blog/2020/12/understanding-text-classification-in-nlp-with-movie-review-example-example/\n",
            "Number of words: 2437\n",
            "Number of sentences: 344\n",
            "List of sentences: \n",
            "['search', 'Start Here', 'Machine Learning', 'Deep Learning', 'Articles', 'Guides', 'Machine Learning', 'Deep Learning', 'Computer Vision', 'Data Visualization', 'Interview Questions', 'More', 'Infographics', 'Jobs', 'Podcasts', 'E-Books', 'For Companies', 'Datahack Summit', 'Glossary', 'Archive', 'Write an Article', 'Courses', 'Certified AI & ML BlackBelt Plus', 'Data Science Immersive Bootcamp', 'All Courses', 'Blogathon', 'Write an Article', 'Creators Club', 'Sign in', 'Join Now', 'Manage your AV', 'Account', 'My Hackathons', 'My Bookmarks', 'My Courses', 'My Applied Jobs', 'Sign Out', 'Home', 'Interview Questions', 'Related Videos', 'Related Articles', 'Free Courses', 'Sign in', 'Join Now', 'Manage your AV Account', 'My Hackathons', 'My Bookmarks', 'My Courses', 'My Applied Jobs', 'Sign Out', 'Understanding text classification in NLP with Movie Review Example Example', 'Facebook', 'Twitter', 'Linkedin', 'Parlad Neupane —', 'Published On December 11, 2020 and Last Modified On December 11th, 2020', 'Advanced', 'Classification', 'Entertainment', 'Programming', 'Project', 'Python', 'Supervised', 'Unstructured Data', 'This article was published as a part of the Data Science Blogathon.', 'Introduction', 'Artificial intelligence has been improved tremendously without needing to change the underlying hardware infrastructure.', 'Users can run an Artificial intelligence program in an old computer system.', 'On the other hand, the beneficiary effect of machine learning is unlimited.', 'Natural Language Processing is one of the branches of AI that gives the machines the ability to read, understand, and deliver meaning.', 'NLP has been very successful in healthcare, media, finance, and human resource.', 'The most common form of unstructured data is texts and speeches.', 'It’s plenty but hard to extract useful information.', 'If not, it would take a long time to mine the information.', 'Written text and speech contain rich information.', 'It’s because we, as intelligent beings, use writing and speaking as the primary form of communication.', 'NLP can analyze these data for us and do the task like sentiment analysis, cognitive assistant, span filtering, identifying fake news, and real-time language translation.', 'This article will cover how NLP understands the texts or parts of speech.', 'Mainly we will be focusing on Words and Sequence Analysis.', 'It includes text classification, vector semantic and word embedding, probabilistic language model, sequential labeling, and speech reorganization.', 'We will look at the sentiment analysis of fifty thousand IMDB movie reviewer.', 'Our goal is to identify whether the review posted on the IMDB site by its user is positive or negative.', 'Topic List', 'Understand what NLP is?', 'What does NLP use for?', 'Words and Sequences', 'Text classification', 'Vector Semantic and Word embedding', 'Probabilistic Language Models', 'Sequence labeling', 'Parsers', 'Semantics', 'Performing Semantic Analysis on IMDB movie review data project', 'NLP has widely used in cars, smartphones, speakers, computers, websites, etc.', 'Google Translator usage machine translator which is the NLP system.', 'Google Translator wrote and spoken natural language to desire language users want to translate.', 'NLP helps google translator to understand the word in context, remove extra noises, and build CNN to understand native voice.', 'NLP is also popular in chatbots.', 'Chatbots is very useful because it reduces the human work of asking what customer needs.', 'NLP chatbot cans ask sequential questions like what the user problem is and where to find the solution.', 'Apple and AMAZON have a robust chatbot in their system.', 'When the user asks some questions, the chatbot converts them into understandable phrases in the internal system.', 'It’s call toke.', 'Then token goes into NLP to get the idea of what users are asking.', 'NLP is used in information retrieval (IR).', 'IR is a software program that deals with large storage, evaluation of information from large text documents from repositories.', 'It will retrieve only relevant information.', 'For example, it is used in google voice detection to trim unnecessary words.', 'Application of NLP', 'Machine Translation i.e. Google Translator', 'Information retrieval', 'Question Answering i.e. ChatBot', 'Summarization', 'Sentiment Analysis', 'Social Media Analysis', 'Mining large data\\\\', 'Words and Sequences', 'NLP system needs to understand text, sign, and semantic properly.', 'Many methods help the NLP system to understand text and symbols.', 'They are text classification, vector semantic, word embedding, probabilistic language model, sequence labeling, and speech reorganization.', 'Text classification', 'Text clarification is the process of categorizing the text into a group of words.', 'By using NLP, text classification can automatically analyze text and then assign a set of predefined tags or categories based on its context.', 'NLP is used for sentiment analysis, topic detection, and language detection.', 'There is mainly three text classification approach-', 'Rule-based System,', 'Machine System', 'Hybrid System.', 'In the rule-based approach, texts are separated into an organized group using a set of handicraft linguistic rules.', 'Those handicraft linguistic rules contain users to define a list of words that are characterized by groups.', 'For example, words like Donald Trump and Boris Johnson would be categorized into politics.', 'People like LeBron James and Ronaldo would be categorized into sports.', 'Machine-based classifier learns to make a classification based on past observation from the data sets.', 'User data is prelabeled as tarin and test data.', 'It collects the classification strategy from the previous inputs and learns continuously.', 'Machine-based classifier usage a bag of a word for feature extension.', 'Source', 'In a bag of words, a vector represents the frequency of words in a predefined dictionary of a word list.', 'We can perform NLP using the following machine learning algorithms: Naïve Bayer, SVM, and Deep Learning.', 'The third approach to text classification is the Hybrid Approach.', 'Hybrid approach usage combines a rule-based and machine Based approach.', 'Hybrid based approach usage of the rule-based system to create a tag and use machine learning to train the system and create a rule.', 'Then the machine-based rule list is compared with the rule-based rule list.', 'If something does not match on the tags, humans improve the list manually.', 'It is the best method to implement text classification', 'Vector Semantic', 'Vector Semantic is another way of word and sequence analysis.', 'Vector semantic defines semantic and interprets words meaning to explain features such as similar words and opposite words.', 'The main idea behind vector semantic is two words are alike if they have used in a similar context.', 'Vector semantic divide the words in a multi-dimensional vector space.', 'Vector semantic is useful in sentiment analysis.', 'Source', 'Word Embedding', 'Word embedding is another method of word and sequence analysis.', 'Embedding translates spares vectors into a low-dimensional space that preserves semantic relationships.', 'Word embedding is a type of word representation that allows words with similar meaning to have a similar representation.', 'There are two types of word embedding-', 'Word2vec', 'Doc2Vec.', 'Word2Vec is a statistical method for effectively learning a standalone word embedding from a text corpus.', 'Source', 'Doc2Vec is similar to Doc2Vec, but it analyzes a group of text like pages.', 'Source', 'Probabilistic Language Model', 'Another approach to word and sequence analysis is the probabilistic language model.', 'The goal of the probabilistic language model is to calculate the probability of a sentence of a sequence of words.', 'For example, the probability of the word “a” occurring in a given word “to” is 0.00013131 percent.', 'Source', 'Sequence Labeling', 'Sequence labeling is a typical NLP task that assigns a class or label to each token in a given input sequence.', 'If someone says “play the movie by tom hanks”.', 'In sequence, labeling will be [play, movie, tom hanks].', 'Play determines an action.', 'Movies are an instance of action.', 'Tom Hanks goes for a search entity.', 'It divides the input into multiple tokens and uses LSTM to analyze it.', 'There are two forms of sequence labeling.', 'They are token labeling and span labeling.', 'Parsing is a phase of NLP where the parser determines the syntactic structure of a text by analyzing its constituent words based on an underlying grammar.', 'For example, “tom ate an apple” will be divided into proper noun \\uf0e0 tom, verb \\uf0e0 ate, determiner \\uf0e0 , noun \\uf0e0 apple.', 'The best example is Amazon Alexa.', 'Source', 'We discuss how text is classified and how to divide the word and sequence so that the algorithm can understand and categorize it.', 'In this project, we are going to discover a sentiment analysis of fifty thousand IMDB movie reviewer.', 'Our goal is to identify whether the review posted on the IMDB site by its user is positive or negative.', 'This project covers text mining techniques like Text Embedding, Bags of Words, word context, and other things.', 'We will also cover the introduction of a bidirectional LSTM sentiment classifier.', 'We will also look at how to import a labeled dataset from TensorFlow automatically.', 'This project also covers steps like data cleaning, text processing, data balance through sampling, and train and test a deep learning model to classify text.', 'Parsing', 'Parser determines the syntactic structure of a text by analyzing its constituent words based on an underlying grammar.', 'It divides group words into component parts and separates words.', 'Source', 'For more details about parsing, check this article.', 'Semantic', 'Text is at the heart of how we communicate.', 'What is really difficult is understanding what is being said in written or spoken conversation?', 'Understanding lengthy articles and books are even more difficult.', 'Semantic is a process that seeks to understand linguistic meaning by constructing a model of the principle that the speaker uses to convey meaning.', 'It’s has been used in customer feedback analysis, article analysis, fake news detection, Semantic analysis, etc.', 'Example Application', 'Here is the code Sample:', 'Importing necessary library', 'It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python', \"# For example, here's several helpful packages to load\", 'import numpy as np # linear algebra', 'import pandas as pd # data processing, CSV file', 'I/O (e.g. pd.read_csv)', '# Input data files are available in the read-only \"../input/\" directory', '# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory', 'import os', \"for dirname, _, filenames in os.walk('/kaggle/input'):\", 'for filename in filenames:', 'print(os.path.join(dirname, filename))', '# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"', \"# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\", '#Importing require Libraries', 'import os', 'import matplotlib.pyplot as plt', 'import nltk', 'from tkinter import *', 'import seaborn as sns', 'import matplotlib.pyplot as plt', 'sns.set()', 'import scipy', 'import tensorflow as tf', 'import tensorflow_hub as hub', 'import tensorflow_datasets as tfds', 'from tensorflow.python import keras', 'from tensorflow.keras.models import Sequential', 'from tensorflow.keras.layers import Dense, Embedding, LSTM', 'from sklearn.model_selection import train_test_split', 'from sklearn.metrics import confusion_matrix', 'from sklearn.metrics import classification_report', 'Downloading necessary file', '# this cells takes time, please run once', \"# Split the training set into 60% and 40%, so we'll end up with 15,000 examples\", '# for training, 10,000 examples for validation and 25,000 examples for testing.', 'original_train_data, original_validation_data, original_test_data', '= tfds.load(', 'name=\"imdb_reviews\",', \"split=('train[:60%]', 'train[60%:]', 'test'),\", 'as_supervised=True)', 'Getting word index from Keras datasets', '#tokanizing by tensorflow', 'word_index = tf.keras.datasets.imdb.get_word_index(', \"path='imdb_word_index.json'\", 'In [8]:', '{k:v for (k,v) in word_index.items() if v < 20}', 'Out[8]:', \"{'with': 16,  'i': 10,  'as': 14,  'it': 9,  'is': 6,  'in': 8,  'but': 18,  'of': 4,  'this': 11,  'a': 3,  'for': 15,  'br': 7,  'the': 1,  'was': 13,  'and': 2,  'to': 5,  'film': 19,  'movie': 17,  'that': 12}\", 'Positive and Negative Review Comparision', 'Creating Train, Test Data', 'Model and Model Summary', 'Splitting data and fitting the model', 'Model effect Overview', 'Confusion Matrix and Correlation Report', 'Note: Data Source and Data for this model is publicly available and can be accessed by using Tensorflow.', 'For the complete code and details, please follow this GitHub Repository.', 'In conclusion, NLP is a field full of opportunities.', 'NLP has a tremendous effect on how to analyze text and speeches.', 'NLP is doing better and better every day.', 'Knowledge extraction from the large data set was impossible five years ago.', 'The rise of the NLP technique made it possible and easy.', 'There are still many opportunities to discover in NLP.', 'Related', 'blogathondoc2vevparsingtext classificationwor2vec', 'Duration:', 'Oct 21 - Oct 27', 'Table of contents', 'About the Author', 'Parlad Neupane', 'Our Top Authors', 'view more', 'Download', 'Analytics Vidhya App for the Latest blog/Article', 'Previous Post', 'Top 15 Free Data Science Courses to Kick Start your Data Science Journey!', 'Next Post', 'Pattern Recognition: The basis of Human and Machine Learning', 'Leave a Reply Your email address will not be published.', 'Required fields are marked *', 'Cancel reply', 'Notify me of follow-up comments by email.', 'Notify me of new posts by email.', 'Submit', 'Top Resources', 'Python Tutorial:', 'Working with CSV file for Data Science', 'Harika Bonthu -', 'Aug 21, 2021', 'Boost Model Accuracy of Imbalanced COVID-19 Mortality Prediction Using GAN-based..', 'Bala Gangadhar Thilak Adiboina -', 'Oct 07, 2020', 'The Most Comprehensive Guide to K-Means Clustering You’ll Ever Need', 'Pulkit Sharma -', 'Aug 19, 2019', 'Joins in Pandas: Master the Different Types of Joins in..', 'Abhishek Sharma -', 'Feb 27, 2020', 'Download App', 'Analytics Vidhya', 'About Us', 'Our Team', 'Careers', 'Contact us', 'Data Scientists', 'Blog', 'Hackathon', 'Discussions', 'Apply Jobs', 'Companies', 'Post Jobs', 'Trainings', 'Hiring Hackathons', 'Advertising', 'Visit us', '© Copyright 2013-2022 Analytics Vidhya.', 'Privacy Policy', 'Terms of Use', 'Refund Policy', 'We use cookies on Analytics Vidhya websites to deliver our services, analyze web traffic, and improve your experience on the site.', 'By using Analytics Vidhya, you agree to our Privacy Policy and Terms of Use.', 'AcceptPrivacy & Cookies Policy', 'Close', 'Privacy Overview', 'This website uses cookies to improve your experience while you navigate through the website.', 'Out of these, the cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website.', 'We also use third-party cookies that help us analyze and understand how you use this website.', 'These cookies will be stored in your browser only with your consent.', 'You also have the option to opt-out of these cookies.', 'But opting out of some of these cookies may affect your browsing experience.', 'Necessary', 'Necessary', 'Always Enabled', 'Necessary cookies are absolutely essential for the website to function properly.', 'This category only includes cookies that ensures basic functionalities and security features of the website.', 'These cookies do not store any personal information.', 'Non-necessary', 'Non-necessary', 'Any cookies that may not be particularly necessary for the website to function and is used specifically to collect user personal data via analytics, ads, other embedded contents are termed as non-necessary cookies.', 'It is mandatory to procure user consent prior to running these cookies on your website.', 'Please signup to Analytics Vidhya', 'Sign in with Google']\n",
            "***************************************\n",
            "***************************************\n",
            "Url: https://blogs.commons.georgetown.edu/cctp-607-spring2019/2019/02/27/natural-language-processing-google-translate/\n",
            "Number of words: 1074\n",
            "Number of sentences: 116\n",
            "List of sentences: \n",
            "['CCTP-607: \"Big Ideas\": AI to the Cloud', 'Spring 2019', 'Menu', 'Skip to content', 'About', 'the Course', 'Syllabus', 'Prof. Irvine’s Home Page', 'Weekly Writing Instructions', 'Final Project Instructions', 'Natural Language Processing + Google Translate', 'Language translation is more complex than a simple word-to-word replacement method.', 'As seen in the readings and videos for this module, translating a text in another language needs more context than a dictionary can provide.', 'This “context’ in language is known as grammar.', 'Because computers do not understand grammar, they need a process in which they can deconstruct sentences and reconstruct them in another language in a way that makes sense.', 'Words can have several different meanings and also depend on their structure within a sentence to make sense.', 'Natural Language Processing addresses this problem of complexity and ambiguity in language translation.', 'The PBS Crash Course video breaks down how computers use NLP methods.', 'Deconstructing sentences into smaller pieces that could be easily processed:', 'In order for computers to deconstruct sentences, grammar is necessary', 'Development of Phrase Structure Rules which encapsulate the grammar of a language', 'Using phrase structures, computers are able to construct parse trees', '*Image retrieved from: https://www.youtube.com/watch?v=fOvTtapxa9c', 'Parse Trees: link every word with a likely part of speech+ show sentence construction', 'This helps computers process information more easily and accurately', 'The PBS video also explains this is the way that Siri is able to deconstruct simple word commands.', 'Additionally, speech recognition apps with the best accuracy use deep neural networks.', 'Looking at how Google Translate’s Neural Network works, the Code Emportium video describes a neural network as a problem solver.', 'In the case of Google Translate, the neural networks job or problem to solve, is to take an English sentence (input) and turn it into a French translation (output).', 'As we learned from the data structures module, computers do not process information the way our brains do.', 'They process information using numbers (vectors).', 'So, the first step will always be to convert the language into computer language.', 'For this particular task, a Recurrent Neural Network will be used (neural network specifically for sentences).', 'Step 1.', 'Take English sentence and convert into computer language (a vector) using a recurrent neural network', 'Step 2.', 'Convert vector to French sentence (using another recurrent neural network)', 'Image retrieved from:', 'https://www.youtube.com/watch?v=AIpXjFwVdIE', 'According to research from a 2014 paper on Neural Machine Translation, the Encoder-Decoder Architecture model pictured above works best for medium length sentences with 15-20 words (Cho et al).', 'The Code Emporium video tested out the LSTM-RNN Encoder method on longer sentences, and found that the translations did not work as well.', 'This is due to the lack of complexity in this method.', 'Recurrent Neural Networks use past information to generate the present information.', 'The video gives the example:', '“While generating the 10th word of the French sentence it looks at the first nine words in the English sentence.”', 'The Recurrent Neural Network is only looking a the past words, and not the words that come after the current word.', 'In language both the words that come before and after are important to the construction of the sentence.', 'Therefore, a BiDirectional Neural Network is able to do just this.', 'Image retrieved from: https://www.youtube.com/watch?v=AIpXjFwVdIE', 'Bidirectional neural networks (looks at words that come before it and after it)', 'Vs.', 'Neural Network (only looks at words that come before it)', 'Using the BiDirectional model – which words (in the original source) should be focused on when generating the translation?', 'Now, the translator needs to learn how to align the input and output.', 'This is learned by an additional unit called an attention mechanism (which French words will be generated by which English words).', 'This is the same process that Google Translate uses – on a larger scale', 'Google Translate Process & Architecture / Layer Breakdown', 'Image retrieved from video:', 'https://www.youtube.com/watch?v=AIpXjFwVdIE', 'English translation is given to the encoder, which translates the sentence into a vector (each word gets assigned a number), then an attention mechanism is used next to determine the English words to focus on as it generated a French word, then the decoder will translate the French translation one word at a time (focusing on words determined by attention mechanism).', 'Works Cited', 'CrashCourse.', 'Data Structures: Crash Course Computer Science #14.', 'YouTube, https://www.youtube.com/watch?v=DuDz6B4cqVc.', 'CrashCourse.', 'Machine Learning & Artificial Intelligence: Crash Course Computer Science #34.', 'YouTube, https://www.youtube.com/watch?v=z-EtmaFJieY.', 'CrashCourse.', 'Natural Language Processing: Crash Course Computer Science #36.', 'YouTube, https://www.youtube.com/watch?v=fOvTtapxa9c.', 'CS Dojo Community.', 'How Google Translate Works – The Machine Learning Algorithm Explained!YouTube, https://www.youtube.com/watch?v=AIpXjFwVdIE.', 'Thierry Poibeau, Machine Translation (Cambridge, MA: MIT Press, 2017).', 'Selections', 'This entry was posted in Week 7 on February 27, 2019 by Adey Zegeye.', 'Post navigation', '← Deblackboxing Deep Learning Machine Translation', 'Using Google Translate for Pidgin English →', 'Search for:', 'Recent Posts', 'Design/Ethical Implications of Explainable AI (XAI)', 'Music To My Ears: De-Blackboxing Spotify’s Recommendation Engine', 'Gender Bias & Artificial Intelligence: Questions and Challenges', 'Machine Learning & Algorithmic Music Composition', 'Chatting their Way to High', 'Brand Equity', 'Weekly Writing', 'Final Project', 'In-Class', 'Week 10', 'Week 11', 'Week 12', 'Week 13', 'Week 2', 'Week 3', 'Week 4', 'Week 5', 'Week 6', 'Week 7', 'Week 8', 'Week 9', 'Seminar MembersKevin Ackermann (10)Linda Bardha (12)Annaliese Blank (12)Beiyuan Gu (11)Dominique Haywood (10)Yajing Hu (11)Proma Huq (10)Deborah Oliveros (9)Zachary Omer (11)Shahin Rafikian (10)Beiyue Wang (11)Adey Zegeye (11)Tianyi Zhao (9)Professor & Site AdministratorMartin Irvine (3)Copyright & Uses of This Site © 2018', 'Course files by Martin Irvine and student posts are licensed under a Creative Commons Attribution-Noncommercial-No Derivative Works 3.0, United States License.', 'All educational uses permitted with attribution and link to this page or appropriate pages.', 'Cited and quoted sources are the property of the copyright holders.', 'Writing and creative content by students in the seminar are the property of the respective writers and may be used only under the Creative Commons 3.0 license with attribution and a link to referenced page(s).', 'Martin Irvine', 'faculty.georgetown.edu/irvinem', 'irvinem@georgetown.edu', 'Creative Commons', 'Blog under the', 'Creative Commons Attribution-NonCommercial-NoDerivs 3.0 License', 'Meta', 'Log in', 'Entries RSS', 'Comments RSS']\n",
            "***************************************\n",
            "***************************************\n",
            "Url: https://towardsai.net/p/nlp/natural-language-processing-nlp-with-python-tutorial-for-beginners-1f54e610a1a0\n",
            "Number of words: 7482\n",
            "Number of sentences: 1199\n",
            "List of sentences: \n",
            "['Latest', 'Trends', 'Shop', 'Latest  Categories   Archive', 'Artificial Intelligence   Fairness', 'Social Good', 'Bioinformatics', 'Careers', 'Cloud Computing', 'Computer Science', 'Computer Vision', 'Cybersecurity', 'Data Analysis', 'Data Analytics', 'Data Engineering', 'Data Mining', 'Data Science', 'Data Visualization', 'Deep Learning', 'DevOps', 'Editorial', 'Education', 'Engineering', 'Ethics', 'Future', 'Game Theory', 'Machine Intelligence', 'Machine Learning   Automated Machine Learning', 'ML and Art', 'Mathematics', 'Natural Language Processing', 'Neuroscience', 'News', 'Newsletter', 'Opinion', 'Optimization', 'Physics', 'Privacy and Security', 'Product Management', 'Program Management', 'Project Management', 'Probability', 'Programming', 'Quantum Computing', 'Research', 'Robotics', 'Scholarly', 'Self-driving Cars', 'Software Engineering', 'Science', 'Statistics', 'Systems', 'Technology', 'Web Scraping', 'Sponsors', 'News', 'Tutorials', 'Newsletter', 'Company  Subscribe', 'For Authors', 'About', 'Contact', 'Resources   Editorial', 'eBooks', 'AI Community', 'AI Community', 'Name: Towards AI', 'Legal Name: Towards AI, Inc.', \"Description: Towards AI is the world's leading artificial intelligence (AI) and technology publication.\", 'Read by thought-leaders and decision-makers around the world.', 'Website: https://towardsai.net/', 'Publisher: https://towardsai.net/#publisher', 'Diversity Policy: https://towardsai.net/about', 'Ethics Policy: https://towardsai.net/about', 'Masthead: https://towardsai.net/about', 'Founder: Roberto Iriondo', 'Cover:', 'Logo:', 'Areas Served: Worldwide', 'Alternate Name: Towards AI, Inc.', 'Alternate Name: Towards AI Co.', 'Alternate Name: towards ai', 'Alternate Name: towardsai', 'Alternate Name: towards.ai', 'Alternate Name: tai', 'Alternate Name: toward ai', 'Alternate Name: toward.ai', 'Alternate Name: Towards AI, Inc.', 'Alternate Name: towardsai.net', 'Alternate Name: pub.towardsai.net', 'Follow us on:', 'Facebook', 'Twitter', 'LinkedIn', 'Instagram', 'Youtube', 'Github', 'Google My Business', 'Google Search', 'Google News', 'Google Maps', 'Discord', 'Shop', 'Towards AI, Medium Editorial', 'Medium', 'Flipboard', 'Publication', 'Feed', 'Sponsors', 'Sponsors', 'Contribute', '5 stars – based on', '497 reviews', 'Frequently Used, Contextual References', 'Remember to copy unique IDs whenever it needs used.', 'i.e., URL: 304b2e42315e', 'Resources', 'Join thousands of AI enthusiasts and experts at the Learn AI Community.', 'Publication', 'Home', 'Publication', 'Editorial', 'Natural Language Processing (NLP) with Python — Tutorial', 'Editorial   Natural Language Processing   Scholarly   Tutorials', 'Natural Language Processing (NLP) with Python — Tutorial', 'Roberto Iriondo', '161 likes', 'July 22, 2020', 'Share this post', 'Last Updated on October 21, 2021 by Editorial Team', 'Author(s): Pratik Shukla, Roberto Iriondo', 'Source: Pixabay', 'Natural Language Processing, Scholarly, Tutorial', 'Tutorial on the basics of natural language processing (NLP) with sample code implementation in Python', 'In this article, we explore the basics of natural language processing (NLP) with code examples.', 'We dive into the natural language toolkit (NLTK) library to present how it can be useful for natural language processing related-tasks.', 'Afterward, we will discuss the basics of other Natural Language Processing libraries and other essential methods for NLP, along with their respective coding sample implementations in Python.', '📚 Resources: Google Colab Implementation |', 'GitHub Repository 📚', 'Table of Contents:', 'What is Natural Language Processing?', 'Applications of NLP', 'Understanding Natural Language Processing (NLP)', 'Rule-based NLP vs. Statistical NLP', 'Components of Natural Language Processing (NLP)', 'Current challenges in NLP', 'Easy to Use NLP Libraries', 'Exploring Features of NLTK', 'Word Cloud', 'Stemming', 'Lemmatization', 'Part-of-Speech (PoS) tagging', 'Chunking', 'Chinking', 'Named Entity Recognition (NER)', 'WordNet', 'Bag of Words', 'What is Natural Language Processing?', 'Computers and machines are great at working with tabular data or spreadsheets.', 'However, as human beings generally communicate in words and sentences, not in the form of tables.', 'Much information that humans speak or write is unstructured.', 'So it is not very clear for computers to interpret such.', 'In natural language processing (NLP), the goal is to make computers understand the unstructured text and retrieve meaningful pieces of information from it.', 'Natural language Processing (NLP) is a subfield of artificial intelligence, in which its depth involves the interactions between computers and humans.', 'Applications of NLP:', 'Machine Translation.', 'Speech Recognition.', 'Sentiment Analysis.', 'Question Answering.', 'Summarization of Text.', 'Chatbot.', 'Intelligent Systems.', 'Text Classifications.', 'Character Recognition.', 'Spell Checking.', 'Spam Detection.', 'Autocomplete.', 'Named Entity Recognition.', 'Predictive Typing.', 'Understanding Natural Language Processing (NLP):', 'Figure 1: Revealing, listening, and understand.', 'We, as humans, perform natural language processing (NLP) considerably well, but even then, we are not perfect.', 'We often misunderstand one thing for another, and we often interpret the same sentences or words differently.', 'For instance, consider the following sentence, we will try to understand its interpretation in many different ways:', 'Example 1:', 'Figure 2: NLP example sentence with the text: “I saw a man on a hill with a telescope.”', 'These are some interpretations of the sentence shown above.', 'There is a man on the hill, and I watched him with my telescope.', 'There is a man on the hill, and he has a telescope.', 'I’m on a hill, and I saw a man using my telescope.', 'I’m on a hill, and I saw a man who has a telescope.', 'There is a man on a hill, and I saw him something with my telescope.', 'Example 2:', 'Figure 3: NLP example sentence with the text: “Can you help me with the can?”', 'In the sentence above, we can see that there are two “can” words, but both of them have different meanings.', 'Here the first “can” word is used for question formation.', 'The second “can” word at the end of the sentence is used to represent a container that holds food or liquid.', 'Hence, from the examples above, we can see that language processing is not “deterministic” (the same language has the same interpretations), and something suitable to one person might not be suitable to another.', 'Therefore, Natural Language Processing (NLP) has a non-deterministic approach.', 'In other words, Natural Language Processing can be used to create a new intelligent system that can understand how humans understand and interpret language in different situations.', 'Rule-based NLP vs. Statistical NLP:', 'Natural Language Processing is separated in two different approaches:', 'Rule-based Natural Language Processing:', 'It uses common sense reasoning for processing tasks.', 'For instance, the freezing temperature can lead to death, or hot coffee can burn people’s skin, along with other common sense reasoning tasks.', 'However, this process can take much time, and it requires manual effort.', 'Statistical Natural Language Processing:', 'It uses large amounts of data and tries to derive conclusions from it.', 'Statistical NLP uses machine learning algorithms to train NLP models.', 'After successful training on large amounts of data, the trained model will have positive outcomes with deduction.', 'Comparison:', 'Figure 4: Rule-Based NLP vs. Statistical NLP.', 'Components of Natural Language Processing (NLP):', 'Figure 5: Components of Natural Language Processing (NLP).', 'a. Lexical Analysis:', 'With lexical analysis, we divide a whole chunk of text into paragraphs, sentences, and words.', 'It involves identifying and analyzing words’ structure.', 'b. Syntactic Analysis:', 'Syntactic analysis involves the analysis of words in a sentence for grammar and arranging words in a manner that shows the relationship among the words.', 'For instance, the sentence “The shop goes to the house” does not pass.', 'c. Semantic Analysis:', 'Semantic analysis draws the exact meaning for the words, and it analyzes the text meaningfulness.', 'Sentences such as “hot ice-cream” do not pass.', 'd. Disclosure Integration:', 'Disclosure integration takes into account the context of the text.', 'It considers the meaning of the sentence before it ends.', 'For example: “He works at Google.”', 'In this sentence, “he” must be referenced in the sentence before it.', 'e. Pragmatic Analysis:', 'Pragmatic analysis deals with overall communication and interpretation of language.', 'It deals with deriving meaningful use of language in various situations.', '📚 Check out an overview of machine learning algorithms for beginners with code examples in Python.', 'Current challenges in NLP:', 'Breaking sentences into tokens.', 'Tagging parts of speech (POS).', 'Building an appropriate vocabulary.', 'Linking the components of a created vocabulary.', 'Understanding the context.', 'Extracting semantic meaning.', 'Named Entity Recognition (NER).', 'Transforming unstructured data into structured data.', 'Ambiguity in speech.', 'Easy to use NLP libraries:', 'a. NLTK (Natural Language Toolkit):', 'The NLTK Python framework is generally used as an education and research tool.', 'It’s not usually used on production applications.', 'However, it can be used to build exciting programs due to its ease of use.', 'Features:', 'Tokenization.', 'Part Of Speech tagging (POS).', 'Named Entity Recognition (NER).', 'Classification.', 'Sentiment analysis.', 'Packages of chatbots.', 'Use-cases:', 'Recommendation systems.', 'Sentiment analysis.', 'Building chatbots.', 'Figure 6: Pros and cons of using the NLTK framework.', 'b. spaCy:', 'spaCy is an open-source natural language processing Python library designed to be fast and production-ready.', 'spaCy focuses on providing software for production usage.', 'Features:', 'Tokenization.', 'Part Of Speech tagging (POS).', 'Named Entity Recognition (NER).', 'Classification.', 'Sentiment analysis.', 'Dependency parsing.', 'Word vectors.', 'Use-cases:', 'Autocomplete and autocorrect.', 'Analyzing reviews.', 'Summarization.', 'Figure 7: Pros and cons of the spaCy framework.', 'c. Gensim:', 'Gensim is an NLP Python framework generally used in topic modeling and similarity detection.', 'It is not a general-purpose NLP library, but it handles tasks assigned to it very well.', 'Features:', 'Latent semantic analysis.', 'Non-negative matrix factorization.', 'Use-cases:', 'Converting documents to vectors.', 'Finding text similarity.', 'Text summarization.', 'Figure 8: Pros and cons of the Gensim framework.', 'd. Pattern:', 'Pattern is an NLP Python framework with straightforward syntax.', 'It’s a powerful tool for scientific and non-scientific tasks.', 'It is highly valuable to students.', 'Features:', 'Tokenization.', 'Part of Speech tagging.', 'Named entity recognition.', 'Parsing.', 'Sentiment analysis.', 'Use-cases:', 'Spelling correction.', 'Search engine optimization.', 'Sentiment analysis.', 'Figure 9: Pros and cons of the Pattern framework.', 'e. TextBlob:', 'TextBlob is a Python library designed for processing textual data.', 'Features:', 'Part-of-Speech tagging.', 'Noun phrase extraction.', 'Sentiment analysis.', 'Classification.', 'Language translation.', 'Parsing.', 'Wordnet integration.', 'Use-cases:', 'Sentiment Analysis.', 'Spelling Correction.', 'Translation and Language Detection.', 'Figure 10: Pros and cons of the TextBlob library.', 'For this tutorial, we are going to focus more on the NLTK library.', 'Let’s dig deeper into natural language processing by making some examples.', 'Exploring Features of NLTK:', 'a. Open the text file for processing:', 'First, we are going to open and read the file which we want to analyze.', 'Figure 11: Small code snippet to open and read the text file and analyze it.', 'Figure 12: Text string file.', 'Next, notice that the data type of the text file read is a String.', 'The number of characters in our text file is 675.', 'b. Import required libraries:', 'For various data processing cases in NLP, we need to import some libraries.', 'In this case, we are going to use NLTK for Natural Language Processing.', 'We will use it to perform various operations on the text.', 'Figure 13: Importing the required libraries.', 'c. Sentence tokenizing:', 'By tokenizing the text with sent_tokenize( ), we can get the text as sentences.', 'Figure 14: Using sent_tokenize( ) to tokenize the text as sentences.', 'Figure 15: Text sample data.', 'In the example above, we can see the entire text of our data is represented as sentences and also notice that the total number of sentences here is 9.', 'd. Word tokenizing:', 'By tokenizing the text with word_tokenize( ), we can get the text as words.', 'Figure 16: Using word_tokenize() to tokenize the text as words.', 'Figure 17: Text sample data.', 'Next, we can see the entire text of our data is represented as words and also notice that the total number of words here is 144.', 'e. Find the frequency distribution:', 'Let’s find out the frequency of words in our text.', 'Figure 18: Using FreqDist() to find the frequency of words in our sample text.', 'Figure 19: Printing the ten most common words from the sample text.', 'Notice that the most used words are punctuation marks and stopwords.', 'We will have to remove such words to analyze the actual text.', 'f. Plot the frequency graph:', 'Let’s plot a graph to visualize the word distribution in our text.', 'Figure 20: Plotting a graph to visualize the text distribution.', 'In the graph above, notice that a period “.” is used nine times in our text.', 'Analytically speaking, punctuation marks are not that important for natural language processing.', 'Therefore, in the next step, we will be removing such punctuation marks.', 'g. Remove punctuation marks:', 'Next, we are going to remove the punctuation marks as they are not very useful for us.', 'We are going to use isalpha( ) method to separate the punctuation marks from the actual text.', 'Also, we are going to make a new list called words_no_punc, which will store the words in lower case but exclude the punctuation marks.', 'Figure 21: Using the isalpha() method to separate the punctuation marks, along with creating a list under words_no_punc to separate words with no punctuation marks.', 'Figure 22: Text sample data.', 'As shown above, all the punctuation marks from our text are excluded.', 'These can also cross-check with the number of words.', 'h. Plotting graph without punctuation marks:', 'Figure 23:', 'Printing the ten most common words from the sample text.', 'Figure 24: Plotting the graph without punctuation marks.', 'Notice that we still have many words that are not very useful in the analysis of our text file sample, such as “and,” “but,” “so,” and others.', 'Next, we need to remove coordinating conjunctions.', 'i. List of stopwords:', 'Figure 25: Importing the list of stopwords.', 'Figure 26: Text sample data.', 'j. Removing stopwords:', 'Figure 27: Cleaning the text sample data.', 'Figure 28: Cleaned data.', 'k. Final frequency distribution:', 'Figure 29: Displaying the final frequency distribution of the most common words found.', 'Figure 30: Visualization of the most common words found in the group.', 'As shown above, the final graph has many useful words that help us understand what our sample data is about, showing how essential it is to perform data cleaning on NLP.', 'Next, we will cover various topics in NLP with coding examples.', 'Word Cloud:', 'Word Cloud is a data visualization technique.', 'In which words from a given text display on the main chart.', 'In this technique, more frequent or essential words display in a larger and bolder font, while less frequent or essential words display in smaller or thinner fonts.', 'It is a beneficial technique in NLP that gives us a glance at what text should be analyzed.', 'Properties:', 'font_path: It specifies the path for the fonts we want to use.', 'width:', 'It specifies the width of the canvas.', 'height: It specifies the height of the canvas.', 'min_font_size: It specifies the smallest font size to use.', 'max_font_size: It specifies the largest font size to use.', 'font_step: It specifies the step size for the font.', 'max_words: It specifies the maximum number of words on the word cloud.', 'stopwords:', 'Our program will eliminate these words.', 'background_color: It specifies the background color for canvas.', 'normalize_plurals: It removes the trailing “s” from words.', 'Read the full documentation on WordCloud.', 'Word Cloud Python Implementation:', 'Figure 31: Python code implementation of the word cloud.', 'Figure 32: Word cloud example.', 'As shown in the graph above, the most frequent words display in larger fonts.', 'The word cloud can be displayed in any shape or image.', 'For instance: In this case, we are going to use the following circle image, but we can use any shape or any image.', 'Figure 33: Circle image shape for our word cloud.', 'Word Cloud Python Implementation:', 'Figure 34: Python code implementation of the word cloud.', 'Figure 35: Word cloud with the circle shape.', 'As shown above, the word cloud is in the shape of a circle.', 'As we mentioned before, we can use any shape or image to form a word cloud.', 'Word CloudAdvantages:', 'They are fast.', 'They are engaging.', 'They are simple to understand.', 'They are casual and visually appealing.', 'Word Cloud Disadvantages:', 'They are non-perfect for non-clean data.', 'They lack the context of words.', 'Stemming:', 'We use Stemming to normalize words.', 'In English and many other languages, a single word can take multiple forms depending upon context used.', 'For instance, the verb “study” can take many forms like “studies,” “studying,” “studied,” and others, depending on its context.', 'When we tokenize words, an interpreter considers these input words as different words even though their underlying meaning is the same.', 'Moreover, as we know that NLP is about analyzing the meaning of content, to resolve this problem, we use stemming.', 'Stemming normalizes the word by truncating the word to its stem word.', 'For example, the words “studies,” “studied,” “studying” will be reduced to “studi,” making all these word forms to refer to only one token.', 'Notice that stemming may not give us a dictionary, grammatical word for a particular set of words.', 'Let’s take an example:', 'a. Porter’s Stemmer Example 1:', 'In the code snippet below, we show that all the words truncate to their stem words.', 'However, notice that the stemmed word is not a dictionary word.', 'Figure 36: Code snippet showing a stemming example.', 'b. Porter’s Stemmer Example 2:', 'In the code snippet below, many of the words after stemming did not end up being a recognizable dictionary word.', 'Figure 37: Code snippet showing a stemming example.', 'c. SnowballStemmer:', 'SnowballStemmer generates the same output as porter stemmer, but it supports many more languages.', 'Figure 38: Code snippet showing an NLP stemming example.', 'd. Languages supported by snowball stemmer:', 'Figure 39: Code snippet showing an NLP stemming example.', 'Various Stemming Algorithms:', 'a. Porter’s Stemmer:', 'Figure 40: Porter’s Stemmer NLP algorithm, pros, and cons.', 'b. Lovin’s Stemmer:', 'Figure 41: Lovin’s Stemmer NLP algorithm, pros, and cons.', 'c. Dawson’s Stemmer:', 'Figure 42:', 'Dawson’s Stemmer NLP algorithm, pros, and cons.', 'd. Krovetz Stemmer:', 'Figure 43:', 'Krovetz Stemmer NLP algorithm, pros, and cons.', 'e. Xerox Stemmer:', 'Figure 44: Xerox Stemmer NLP algorithm, pros, and cons.', 'f. Snowball Stemmer:', 'Figure 45:', 'Snowball Stemmer NLP algorithm, pros, and cons.', '📚 Check out our tutorial on neural networks from scratch with Python code and math in detail.', 'Lemmatization:', 'Lemmatization tries to achieve a similar base “stem” for a word.', 'However, what makes it different is that it finds the dictionary word instead of truncating the original word.', 'Stemming does not consider the context of the word.', 'That is why it generates results faster, but it is less accurate than lemmatization.', 'If accuracy is not the project’s final goal, then stemming is an appropriate approach.', 'If higher accuracy is crucial and the project is not on a tight deadline, then the best option is amortization (Lemmatization has a lower processing speed, compared to stemming).', 'Lemmatization takes into account Part Of Speech (POS) values.', 'Also, lemmatization may generate different outputs for different values of POS.', 'We generally have four choices for POS:', 'Figure 46: Part of Speech (POS) values in lemmatization.', 'Difference between Stemmer and Lemmatizer:', 'a. Stemming:', 'Notice how on stemming, the word “studies” gets truncated to “studi.”', 'Figure 47:', 'Using stemming with the NLTK Python framework.', 'b. Lemmatizing:', 'During lemmatization, the word “studies” displays its dictionary word “study.”', 'Figure 48:', 'Using lemmatization with the NLTK Python framework.', 'Python Implementation:', 'a.', 'A basic example demonstrating how a lemmatizer works', 'In the following example, we are taking the PoS tag as “verb,” and when we apply the lemmatization rules, it gives us dictionary words instead of truncating the original word:', 'Figure 49: Simple lemmatization example with the NLTK framework.', 'b. Lemmatizer with default PoS value', 'The default value of PoS in lemmatization is a noun(n).', 'In the following example, we can see that it’s generating dictionary words:', 'Figure 50: Using lemmatization to generate default values.', 'c.', 'Another example demonstrating the power of lemmatizer', 'Figure 51: Lemmatization of the words: “am”, “are”, “is”, “was”, “were”', 'd. Lemmatizer with different POS values', 'Figure 52: Lemmatization with different Part-of-Speech values.', 'Part of Speech Tagging (PoS tagging):', 'Why do we need Part of Speech (POS)?', 'Figure 53: Sentence example, “can you help me with the can?”', 'Parts of speech(PoS) tagging is crucial for syntactic and semantic analysis.', 'Therefore, for something like the sentence above, the word “can” has several semantic meanings.', 'The first “can” is used for question formation.', 'The second “can” at the end of the sentence is used to represent a container.', 'The first “can” is a verb, and the second “can” is a noun.', 'Giving the word a specific meaning allows the program to handle it correctly in both semantic and syntactic analysis.', 'Below, please find a list of Part of Speech (PoS) tags with their respective examples:', '1.', 'CC: Coordinating Conjunction', 'Figure 54: Coordinating conjunction example.', '2. CD: Cardinal Digit', 'Figure 55: Cardinal digit example.', '3.', 'DT: Determiner', 'Figure 56: A determiner example.', '4.', 'EX: Existential There', 'Figure 57: Existential “there” example.', '5.', 'FW: Foreign Word', 'Figure 58: Foreign word example.', '6.', 'IN: Preposition / Subordinating Conjunction', 'Figure 59: Preposition/Subordinating conjunction.', '7. JJ: Adjective', 'Figure 60: Adjective example.', '8.', 'JJR: Adjective, Comparative', 'Figure 61: Adjective, comparative example.', '9.', 'JJS: Adjective, Superlative', 'Figure 62:', '10.', 'LS: List Marker', 'Figure 63: List marker example.', '11.', 'MD: Modal', 'Figure 64:', '12.', 'NN: Noun, Singular', 'Figure 65: Noun, singular example.', '13.', 'NNS: Noun, Plural', 'Figure 66: Noun, plural example.', '14.', 'NNP: Proper Noun, Singular', 'Figure 67: Proper noun, singular example.', '15.', 'Proper Noun, Plural', 'Figure 68: Proper noun, plural example.', '16.', 'PDT: Predeterminer', 'Figure 69: Predeterminer example.', '17.', 'Possessive Endings', 'Figure 70: Possessive endings example.', '18.', 'PRP: Personal Pronoun', 'Figure 71: Personal pronoun example.', '19. PRP$:', 'Possessive Pronoun', 'Figure 72: Possessive pronoun example.', '20.', 'RB: Adverb', 'Figure 73: Adverb example.', '21.', 'RBR: Adverb, Comparative', 'Figure 74: Adverb, comparative example.', '22.', 'RBS: Adverb, Superlative', 'Figure 75: Adverb, superlative example.', '23.', 'RP: Particle', 'Figure 76:', 'Particle example.', '24.', 'TO: To', 'Figure 77:', 'To example.', '25.', 'UH: Interjection', 'Figure 78: Interjection example.', '26.', 'VB: Verb, Base Form', 'Figure 79: Verb, base form example.', '27.', 'VBD: Verb, Past Tense', 'Figure 80: Verb, past tense example.', '28.', 'VBG: Verb, Present Participle', 'Figure 81: Verb, present participle example.', '29.', 'VBN: Verb, Past Participle', 'Figure 82: Verb, past participle.', '30.', 'VBP: Verb, Present Tense, Not Third Person Singular', 'Figure 83: Verb, present tense, not third-person singular.', '31.', 'VBZ: Verb, Present Tense, Third Person Singular', 'Figure 84: Verb, present tense, third-person singular.', '32.', 'Wh — Determiner', 'Figure 85: Determiner example.', '33.', 'WP: Wh — Pronoun', 'Figure 86: Pronoun example.', '34.', 'Possessive Wh — Pronoun', 'Figure 87: Possessive pronoun example.', '35.', 'Wh — Adverb', 'Figure 88: Adverb example.', 'Python Implementation:', 'a.', 'A simple example demonstrating PoS tagging.', 'Figure 89: PoS tagging example.', 'b.', 'A full example demonstrating the use of PoS tagging.', 'Figure 90: Full Python sample demonstrating PoS tagging.', 'Chunking:', 'Chunking means to extract meaningful phrases from unstructured text.', 'By tokenizing a book into words, it’s sometimes hard to infer meaningful information.', 'It works on top of Part of Speech(PoS) tagging.', 'Chunking takes PoS tags as input and provides chunks as output.', 'Chunking literally means a group of words, which breaks simple text into phrases that are more meaningful than individual words.', 'Figure 91: The chunking process in NLP.', 'Before working with an example, we need to know what phrases are?', 'Meaningful groups of words are called phrases.', 'There are five significant categories of phrases.', 'Noun Phrases (NP).', 'Verb Phrases (VP).', 'Adjective Phrases (ADJP).', 'Adverb Phrases (ADVP).', 'Prepositional Phrases (PP).', 'Phrase structure rules:', 'S(Sentence) → NP VP.', 'NP → {Determiner, Noun, Pronoun, Proper name}.', 'VP → V (NP)(PP)(Adverb).', 'PP → Pronoun (NP).', 'AP → Adjective (PP).', 'Example:', 'Figure 92: A chunking example in NLP.', 'Python Implementation:', 'In the following example, we will extract a noun phrase from the text.', 'Before extracting it, we need to define what kind of noun phrase we are looking for, or in other words, we have to set the grammar for a noun phrase.', 'In this case, we define a noun phrase by an optional determiner followed by adjectives and nouns.', 'Then we can define other rules to extract some other phrases.', 'Next, we are going to use RegexpParser( ) to parse the grammar.', 'Notice that we can also visualize the text with the .draw( ) function.', 'Figure 93: Code snippet to extract noun phrases from a text file.', 'In this example, we can see that we have successfully extracted the noun phrase from the text.', 'Figure 94: Successful extraction of the noun phrase from the input text.', 'Chinking:', 'Chinking excludes a part from our chunk.', 'There are certain situations where we need to exclude a part of the text from the whole text or chunk.', 'In complex extractions, it is possible that chunking can output unuseful data.', 'In such case scenarios, we can use chinking to exclude some parts from that chunked text.', 'In the following example, we are going to take the whole string as a chunk, and then we are going to exclude adjectives from it by using chinking.', 'We generally use chinking when we have a lot of unuseful data even after chunking.', 'Hence, by using this method, we can easily set that apart, also to write chinking grammar, we have to use inverted curly braces, i.e.:', '} write chinking grammar here {', 'Python Implementation:', 'Figure 95:', 'Chinking implementation with Python.', 'From the example above, we can see that adjectives separate from the other text.', 'Figure 96:', 'In this example, adjectives are excluded by using chinking.', 'Named Entity Recognition (NER):', 'Named entity recognition can automatically scan entire articles and pull out some fundamental entities like people, organizations, places, date, time, money, and GPE discussed in them.', 'Use-Cases:', 'Content classification for news channels.', 'Summarizing resumes.', 'Optimizing search engine algorithms.', 'Recommendation systems.', 'Customer support.', 'Commonly used types of named entity:', 'Figure 97: An example of commonly used types of named entity recognition (NER).', 'Python Implementation:', 'There are two options :', '1. binary = True', 'When the binary value is True, then it will only show whether a particular entity is named entity or not.', 'It will not show any further details on it.', 'Figure 98: Python implementation when a binary value is True.', 'Our graph does not show what type of named entity it is.', 'It only shows whether a particular word is named entity or not.', 'Figure 99: Graph example of when a binary value is True.', '2. binary = False', 'When the binary value equals False, it shows in detail the type of named entities.', 'Figure 100: Python implementation when a binary value is False.', 'Our graph now shows what type of named entity it is.', 'Figure 101: Graph showing the type of named entities when a binary value equals false.', 'WordNet:', 'Wordnet is a lexical database for the English language.', 'Wordnet is a part of the NLTK corpus.', 'We can use Wordnet to find meanings of words, synonyms, antonyms, and many other words.', 'a. We can check how many different definitions of a word are available in Wordnet.', 'Figure 102: Checking word definitions with Wordnet using the NLTK framework.', 'b. We can also check the meaning of those different definitions.', 'Figure 103: Gathering the meaning of the different definitions by using Wordnet.', 'c.', 'All details for a word.', 'Figure: 104: Finding all the details for a specific word.', 'd.', 'All details for all meanings of a word.', 'Figure 105:', 'Finding all details for all the meanings of a specific word.', 'e. Hypernyms:', 'Hypernyms gives us a more abstract term for a word.', 'Figure 106: Using Wordnet to find a hypernym.', 'f. Hyponyms:', 'Hyponyms gives us a more specific term for a word.', 'Figure 107: Using Wordnet to find a hyponym.', 'g. Get a name only.', 'Figure 108: Finding only a name with Wordnet.', 'h. Synonyms.', 'Figure 109: Finding synonyms with Wordnet.', 'i. Antonyms.', 'Figure 110:', 'Finding antonyms with Wordnet.', 'j. Synonyms and antonyms.', 'Figure 111: Finding synonyms and antonyms code snippet with Wordnet.', 'k.', 'Finding the similarity between words.', 'Figure 112: Finding the similarity ratio between words using Wordnet.', 'Figure 113: Finding the similarity ratio between words using Wordnet.', 'Bag of Words:', 'Figure 114: A representation of a bag of words.', 'What is the Bag-of-Words method?', 'It is a method of extracting essential features from row text so that we can use it for machine learning models.', 'We call it “Bag” of words because we discard the order of occurrences of words.', 'A bag of words model converts the raw text into words, and it also counts the frequency for the words in the text.', 'In summary, a bag of words is a collection of words that represent a sentence along with the word count where the order of occurrences is not relevant.', 'Figure 115: Structure of a bag of words.', 'Raw Text: This is the original text on which we want to perform analysis.', 'Clean Text: Since our raw text contains some unnecessary data like punctuation marks and stopwords, so we need to clean up our text.', 'Clean text is the text after removing such words.', 'Tokenize: Tokenization represents the sentence as a group of tokens or words.', 'Building Vocab: It contains total words used in the text after removing unnecessary data.', 'Generate Vocab:', 'It contains the words along with their frequencies in the sentences.', 'For instance:', 'Sentences:', 'Jim and Pam traveled by bus.', 'The train was late.', 'The flight was full.', 'Traveling by flight is expensive.', 'a. Creating a basic structure:', 'Figure 116: Example of a basic structure for a bag of words.', 'b. Words with frequencies:', 'Figure 117: Example of a basic structure for words with frequencies.', 'c. Combining all the words:', 'Figure 118: Combination of all the input words.', 'd. Final model:', 'Figure 119: The final model of our bag of words.', 'Python Implementation:', 'Figure 120: Python implementation code snippet of our bag of words.', 'Figure 121: Output of our bag of words.', 'Figure 122: Output of our bag of words.', 'Applications:', 'Natural language processing.', 'Information retrieval from documents.', 'Classifications of documents.', 'Limitations:', 'Semantic meaning: It does not consider the semantic meaning of a word.', 'It ignores the context in which the word is used.', 'Vector size: For large documents, the vector size increase, which may result in higher computational time.', 'Preprocessing: In preprocessing, we need to perform data cleansing before using it.', 'TF-IDF stands for Term Frequency — Inverse Document Frequency, which is a scoring measure generally used in information retrieval (IR) and summarization.', 'The TF-IDF score shows how important or relevant a term is in a given document.', 'The intuition behind TF and IDF:', 'If a particular word appears multiple times in a document, then it might have higher importance than the other words that appear fewer times (TF).', 'At the same time, if a particular word appears many times in a document, but it is also present many times in some other documents, then maybe that word is frequent, so we cannot assign much importance to it.', 'For instance, we have a database of thousands of dog descriptions, and the user wants to search for “a cute dog” from our database.', 'The job of our search engine would be to display the closest response to the user query.', 'How would a search engine do that?', 'The search engine will possibly use TF-IDF to calculate the score for all of our descriptions, and the result with the higher score will be displayed as a response to the user.', 'Now, this is the case when there is no exact match for the user’s query.', 'If there is an exact match for the user query, then that result will be displayed first.', 'Then, let’s suppose there are four descriptions available in our database.', 'The furry dog.', 'A cute doggo.', 'A big dog.', 'The lovely doggo.', 'Notice that the first description contains 2 out of 3 words from our user query, and the second description contains 1 word from the query.', 'The third description also contains 1 word, and the forth description contains no words from the user query.', 'As we can sense that the closest answer to our query will be description number two, as it contains the essential word “cute” from the user’s query, this is how TF-IDF calculates the value.', 'Notice that the term frequency values are the same for all of the sentences since none of the words in any sentences repeat in the same sentence.', 'So, in this case, the value of TF will not be instrumental.', 'Next, we are going to use IDF values to get the closest answer to the query.', 'Notice that the word dog or doggo can appear in many many documents.', 'Therefore, the IDF value is going to be very low.', 'Eventually, the TF-IDF value will also be lower.', 'However, if we check the word “cute” in the dog descriptions, then it will come up relatively fewer times, so it increases the TF-IDF value.', 'So the word “cute” has more discriminative power than “dog” or “doggo.”', 'Then, our search engine will find the descriptions that have the word “cute” in it, and in the end, that is what the user was looking for.', 'Simply put, the higher the TF*IDF score, the rarer or unique or valuable the term and vice versa.', 'Now we are going to take a straightforward example and understand TF-IDF in more detail.', 'Example:', 'Sentence 1: This is the first document.', 'Sentence 2: This document is the second document.', 'TF: Term Frequency', 'Figure 123: Calculation for the term frequency on TF-IDF.', 'a. Represent the words of the sentences in the table.', 'Figure 124: Table representation of the sentences.', 'b.', 'Displaying the frequency of words.', 'Figure 125: Table showing the frequency of words.', 'c. Calculating TF using a formula.', 'Figure 126: Calculating TF.', 'Figure 127: Resulting TF.', 'IDF: Inverse Document Frequency', 'Figure 128: Calculating the IDF.', 'd. Calculating IDF values from the formula.', 'Figure 129: Calculating IDF values from the formula.', 'e. Calculating TF-IDF.', 'TF-IDF is the multiplication of TF*IDF.', 'Figure 130: The resulting multiplication of TF-IDF.', 'In this case, notice that the import words that discriminate both the sentences are “first” in sentence-1 and “second” in sentence-2 as we can see, those words have a relatively higher value than other words.', 'However, there any many variations for smoothing out the values for large documents.', 'The most common variation is to use a log value for TF-IDF.', 'Let’s calculate the TF-IDF value again by using the new IDF value.', 'Figure 131: Using a log value for TF-IDF by using the new IDF value.', 'f. Calculating IDF value using log.', 'Figure 132: Calculating the IDF value using a log.', 'g. Calculating TF-IDF.', 'Figure 133:', 'Calculating TF-IDF using a log.', 'As seen above, “first” and “second” values are important words that help us to distinguish between those two sentences.', 'Now that we saw the basics of TF-IDF.', 'Next, we are going to use the sklearn library to implement TF-IDF in Python.', 'A different formula calculates the actual output from our program.', 'First, we will see an overview of our calculations and formulas, and then we will implement it in Python.', 'Actual Calculations:', 'a. Term Frequency (TF):', 'Figure 134: Actual calculation of TF.', 'b. Inverse Document Frequency (IDF):', 'Figure 135: Formula for the IDF.', 'Figure 136:', 'Applying a log to the IDF values.', 'c. Calculating final TF-IDF values:', 'Figure 137: Calculating the final IDF values.', 'Figure 138: Final TF-IDF values.', 'Python Implementation:', 'Figure 139: Python implementation of TF-IDF code snippet.', 'Figure 140: Final output.', 'Conclusion:', 'These are some of the basics for the exciting field of natural language processing (NLP).', 'We hope you enjoyed reading this article and learned something new.', 'Any suggestions or feedback is crucial to continue to improve.', 'Please let us know in the comments if you have any.', 'The views expressed in this article are those of the author(s) and do not represent the views of Carnegie Mellon University, nor other companies (directly or indirectly) associated with the author(s).', 'These writings do not intend to be final products, yet rather a reflection of current thinking, along with being a catalyst for discussion and improvement.', 'Published via Towards AI', 'Citation', 'For attribution in academic contexts, please cite this work as:', 'Shukla, et al., “Natural Language Processing (NLP) with Python — Tutorial”, Towards AI, 2020', 'BibTex citation:', '@article{pratik_iriondo_2020,', 'title={Natural Language Processing (NLP) with Python — Tutorial},', 'url={https://towardsai.net/nlp-tutorial-with-python},', 'journal={Towards AI},', 'publisher={Towards AI Co.},', 'author={Pratik, Shukla and Iriondo, Roberto},', 'year={2020},', 'month={Jul}', 'References:', '[1] The example text was gathered from American Literature, https://americanliterature.com/', '[2] Natural Language Toolkit, https://www.nltk.org/', '[3] TF-IDF, KDnuggets, https://www.kdnuggets.com/2018/08/wtf-tf-idf.html', 'Resources:', 'Google Colab Implementation.', 'Github Tutorial Full Code Repository.', 'Bag of Words, Chinking, Chunking, Lemmatization, Named Entity Recognition, natural language processing, NLTK, Part of Speech, Stemming, TF-IDF, Towards AI — Multidisciplinary Science Journal - Medium, Word Cloud, WordNet', 'Share this post', 'Roberto Iriondo', 'Roberto Iriondo is the head of content at Snorkel AI, a Silicon Valley AI unicorn that empowers data scientists and developers to build and deploy scalable AI applications end-to-end with Snorkel Flow, the data-centric AI platform for the enterprise.', 'He is also a co-founder and advisor to the CEO at Towards AI, Inc., an internationally known media company and community in the AI space, servicing AI and technology organizations, from startups to enterprises with clients in the US, Asia, and Europe.', 'Previously, he was the founder of Towards AI Co. from 2019 to 2022 (acquired in 2022), and a front-end engineer, and the marketing lead for the Machine Learning Department at Carnegie Mellon University from 2018 to 2021.', 'As a builder and strategist by heart, his work has helped several companies achieve business goals and significantly increase ROIs, from Amazon Science, MBZUAI, Gather AI, Determined AI, Lambda Labs, Udacity, and many more.', 'Post navigation', 'Setting up Alpaca API for algorithmic trading Time Series prediction using Adaptive filtering', 'Related posts', 'Tutorials', 'The Gradient Descent Algorithm and its Variants', 'Towards AI Team', '1 like', 'Tutorials', 'Mathematical Intuition behind the Gradient Descent Algorithm', 'Towards AI Team', '1 like', 'Tutorials', 'The Gradient Descent Algorithm', 'Towards AI Team', '1 like', 'Natural Language Processing', 'Which NLP Task Does NOT Benefit From Pre-trained Language Models?', 'Towards AI', '1 like', 'Comments (4)', 'Avremee August 15, 2021', 'very well explanations, thank you', 'Reply', 'Shivakumar Kokkula June 28, 2022', 'Superb content', 'Reply', 'Hemalatha Abothula June 29, 2022', 'good', 'Reply', 'javed khan June 30, 2022', 'great explanation', 'Reply', 'Feedback ↓ Cancel reply', 'Search for:', 'Search', 'Select Category', 'Artificial Intelligence', 'Automated Machine Learning', 'Bias', 'Bioinformatics', 'Business Intelligence', 'Business Science', 'Careers', 'Cloud Computing', 'Computer Science', 'Computer Vision', 'Cryptography', 'Cybersecurity', 'Data Analysis', 'Data Analytics', 'Data Engineering', 'Data Labeling', 'Data Mining', 'Data Science', 'Data Visualization', 'Data-Centric AI', 'Deep Learning', 'DevOps', 'Editorial', 'Education', 'Engineering', 'Ethics', 'Events', 'Fairness', 'Featured', 'Future', 'Futuristic', 'Game Theory', 'Information Technology', 'Innovation', 'Internet of Things', 'Latest', 'Logic', 'Machine Intelligence', 'Machine Learning', 'Mathematics', 'ML and Art', 'MLOps', 'Natural Language Processing', 'Neuroscience', 'News', 'Newsletter', 'Opinion', 'Optimization', 'Physics', 'Predictive Analytics', 'Privacy and', 'Security', 'Probability', 'Product Management', 'Program Management', 'Programming', 'Project Management', 'Quantum Computing', 'Research', 'Robotics', 'Scholarly', 'Science', 'Scientific Research', 'Self-driving Cars', 'Shop', 'Social Good', 'Software Engineering', 'Statistics', 'Systems', 'Technology', 'Tutorials', 'Weak Supervision', 'Web Scraping', 'Popular posts', 'Best Workstations for Deep Learning, Data Science, and Machine Learning (ML) for 2022', 'Descriptive Statistics for Data-driven Decision Making with Python', 'Best Machine Learning (ML) Books - Free and Paid - Editorial Recommendations for 2022', 'Best Laptops for Deep Learning, Machine Learning (ML), and Data Science for 2022', 'Best Data Science Books - Free and Paid - Editorial Recommendations for 2022', 'Updates', 'Newsletter', 'This AI newsletter is all you need #17', 'Newsletter', 'This AI newsletter is all you need #15', 'Newsletter', 'This AI newsletter is all you need #14', 'Recent Posts', 'Seal the Containerized ML Deal With Podman', 'March 24, 2022', 'Gaussian Naive Bayes Explained and Hands-On with Scikit-Learn', 'March 24, 2022', 'Manipulating Time Series Data In Python', 'March 24, 2022', 'All About Logistic Regression', 'March 22, 2022', 'Support Vector Machine (SVM) for Binary and Multiclass Classification: Hands-On with SciKit-Learn', 'March 22, 2022', 'Asia', 'Careers', 'computer vision', 'Convolutional Neural Networks', 'datasets', 'datasets finder', 'Decision Trees', 'demystifying machine learning series', 'Determined AI', 'education', 'Finance', 'Google Colab', 'Google Colab Tutorial', 'google dataset finder', 'inventory management systems', 'Japan', 'Jobs', 'Linear Algebra', 'Linear Regression', 'machine learning', 'machine learning 101', 'Machine Learning Blog', 'machine learning definitions', 'Machine learning modeling', 'machine learning terms', 'Mathematics', 'natural language processing', 'Pandas', 'Programming', 'Python', 'Reinforcement Learning', 'research', 'science', 'Shop', 'technology', 'Tensorflow', 'Towards AI', 'Towards AI - Medium', 'Towards AI —', 'Multidisciplinary Science Journal - Medium', 'Yolo', 'Yolo V5', 'The World’s Leading AI and Technology Publication.', \"Towards AI is the world's leading artificial intelligence (AI) and technology publication.\", 'Read by thought-leaders and decision-makers around the world.', 'CompanyHome', 'About', 'Publication', 'Sponsors', 'For Authors', 'Editorial', 'News', 'AI Newsletter', 'Subscribe', 'AI Community', 'Shop', '228 Park Avenue South,', 'PMB 99625,', 'New York, NY, 10003', 'Editorial ↓', '[email protected]', 'Press ↓', '[email protected]', 'Advertising/Sponsors ↓', '🚀 Check out our offerings 🚀', '[email protected]', '+1 (650) 246-9381', 'Workflow of a Data Science Project #mw  https://t.co/rjDeW8udu1  #latest', 'Towards AI on Twitter', 'October 30, 2022', 'Gradient Descent for Machine Learning (ML) 101 with Python Tutorial →  https://t.co/486IDngdMK', '#gradientdescent...  https://t.co/SqXqTxvqMi', 'Towards AI on Twitter', 'October 30, 2022', 'Stop Using Elbow Diagram To Find Best K-Value And Use This Instead via #TowardsAI →  https://t.co/pKJs82yICg ...', 'https://t.co/eLK2UheCQ4', 'Towards AI on Twitter', 'October 30, 2022', 'Hypothesis Testing In Statistics with Examples #mw  https://t.co/y12J8D8HGF  #latest', 'Towards AI on Twitter', 'October 30, 2022', 'Read Public Messages from the Ethereum Network with Simple Web Programming #mw  https://t.co/fWF9WOrpex  #latest', 'Towards AI on Twitter', 'October 30, 2022', '6 Interesting Facts You Are Less Familiar with Python Float Data Type #mw  https://t.co/2t40mKPDwz  #programming', 'Towards AI on Twitter', 'October 30, 2022', '© 2019 - 2020 Towards AI Inc. |', 'All Rights Reserved.', 'Terms of Service | Privacy Policy', 'We use cookies on our website to give you the most relevant experience by remembering your preferences and repeat visits.', 'By clicking “Accept”, you consent to the use of ALL the cookies.', 'Do not sell my personal information.', 'Cookie SettingsAcceptManage consent', 'Close', 'Privacy Overview', 'This website uses cookies to improve your experience while you navigate through the website.', 'Out of these, the cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the website.', 'We also use third-party cookies that help us analyze and understand how you use this website.', 'These cookies will be stored in your browser only with your consent.', 'You also have the option to opt-out of these cookies.', 'But opting out of some of these cookies may affect your browsing experience.', 'Necessary', 'Necessary', 'Always Enabled', 'Necessary cookies are absolutely essential for the website to function properly.', 'These cookies ensure basic functionalities and security features of the website, anonymously.', 'CookieDurationDescriptioncookielawinfo-checkbox-analytics11 monthsThis cookie is set by GDPR Cookie Consent plugin.', 'The cookie is used to store the user consent for the cookies in the category \"Analytics\".cookielawinfo-checkbox-functional11 monthsThe cookie is set by GDPR cookie consent to record the user consent for the cookies in the category \"Functional\".cookielawinfo-checkbox-necessary11 monthsThis cookie is set by GDPR Cookie Consent plugin.', 'The cookies is used to store the user consent for the cookies in the category \"Necessary\".cookielawinfo-checkbox-others11 monthsThis cookie is set by GDPR Cookie Consent plugin.', 'The cookie is used to store the user consent for the cookies in the category \"Other.cookielawinfo-checkbox-performance11 monthsThis cookie is set by GDPR Cookie Consent plugin.', 'The cookie is used to store the user consent for the cookies in the category \"Performance\".viewed_cookie_policy11 monthsThe cookie is set by the GDPR Cookie Consent plugin and is used to store whether or not user has consented to the use of cookies.', 'It does not store any personal data.', 'Functional', 'Functional', 'Functional cookies help to perform certain functionalities like sharing the content of the website on social media platforms, collect feedbacks, and other third-party features.', 'Performance', 'Performance', 'Performance cookies are used to understand and analyze the key performance indexes of the website which helps in delivering a better user experience for the visitors.', 'Analytics', 'Analytics', 'Analytical cookies are used to understand how visitors interact with the website.', 'These cookies help provide information on metrics the number of visitors, bounce rate, traffic source, etc.', 'Advertisement', 'Advertisement', 'Advertisement cookies are used to provide visitors with relevant ads and marketing campaigns.', 'These cookies track visitors across websites and collect information to provide customized ads.', 'Others', 'Others', 'Other uncategorized cookies are those that are being analyzed and have not been classified into a category as yet.', 'Search for:', 'Search', 'Hit enter to search or ESC to close', 'Latest  Categories   Archive', 'Artificial Intelligence   Fairness', 'Social Good', 'Bioinformatics', 'Careers', 'Cloud Computing', 'Computer Science', 'Computer Vision', 'Cybersecurity', 'Data Analysis', 'Data Analytics', 'Data Engineering', 'Data Mining', 'Data Science', 'Data Visualization', 'Deep Learning', 'DevOps', 'Editorial', 'Education', 'Engineering', 'Ethics', 'Future', 'Game Theory', 'Machine Intelligence', 'Machine Learning   Automated Machine Learning', 'ML and Art', 'Mathematics', 'Natural Language Processing', 'Neuroscience', 'News', 'Newsletter', 'Opinion', 'Optimization', 'Physics', 'Privacy and Security', 'Product Management', 'Program Management', 'Project Management', 'Probability', 'Programming', 'Quantum Computing', 'Research', 'Robotics', 'Scholarly', 'Self-driving Cars', 'Software Engineering', 'Science', 'Statistics', 'Systems', 'Technology', 'Web Scraping', 'Sponsors', 'News', 'Tutorials', 'Newsletter', 'Company  Subscribe', 'For Authors', 'About', 'Contact', 'Resources   Editorial', 'eBooks', 'AI Community']\n",
            "***************************************\n"
          ]
        }
      ],
      "source": [
        "# Jasmini Rebecca Gomes dos Santos\n",
        "# Obs: Execute todas as células para conseguir rodar o código corretamente\n",
        "'''\n",
        "Sua tarefa será  gerar a matriz termo documento, dos documentos recuperados da internet e \n",
        "imprimir esta matriz na tela. Para tanto: \n",
        "a) Considere que todas as listas de sentenças devem ser transformadas em listas de vetores, \n",
        "onde cada item será uma das palavras da sentença. \n",
        "b) Todos  os  vetores  devem  ser  unidos  em  um  corpus  único  formando  uma  lista  de  vetores, \n",
        "onde cada item será um lexema.  \n",
        "c) Este único corpus será usado para gerar o vocabulário. \n",
        "d) O  resultado  esperado  será  uma  matriz  termo  documento  criada  a  partir  da  aplicação  da \n",
        "técnica bag of Words em todo o corpus.  \n",
        "'''\n",
        "\n",
        "from __future__ import unicode_literals, print_function\n",
        "from spacy.lang.en import English \n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "import re\n",
        "import traceback\n",
        "import sys\n",
        "import unicodedata\n",
        "import spacy\n",
        "from spacy.pipeline import EntityRuler\n",
        "from spacy import displacy\n",
        "from spacy.matcher import Matcher\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def split_in_sentences(text):\n",
        "    doc = nlp(text)\n",
        "    return [str(sent).strip() for sent in doc.sents]\n",
        "\n",
        "def remove_invalid_characters(text):\n",
        "  text = text.strip().split(\"\\n\")\n",
        "  text_array = []\n",
        "  for i in range(0, len(text) - 1):\n",
        "    if text[i] != \"\" and re.search('[a-z0-9]', text[i]):\n",
        "      text[i] = unicodedata.normalize(\"NFKD\", text[i])\n",
        "      text[i] = text[i].strip()\n",
        "      text_array.append(text[i])\n",
        "  text = \"\\n\".join(text_array)\n",
        "  return text\n",
        "\n",
        "def extract_text_from_url(url):\n",
        "    try:\n",
        "      req = Request(url, headers={'User-Agent': 'XYZ/3.0'})\n",
        "      html = urlopen(req, timeout=10).read()\n",
        "      soup = BeautifulSoup(html, 'lxml') \n",
        "      invalid_tags = ['[document]','noscript','meta','head', 'input','script', 'style', 'svg']\n",
        "      for tag in soup.find_all(invalid_tags):\n",
        "          tag.extract()\n",
        "      text = soup.getText()\n",
        "      text = remove_invalid_characters(text)\n",
        "      text_array = split_in_sentences(text)\n",
        "      text = \"\\n\".join(text_array)\n",
        "      text = remove_invalid_characters(text)\n",
        "      return text\n",
        "    except:\n",
        "        print(traceback.format_exc())\n",
        "        sys.exit(-1)\n",
        "\n",
        "def find_number_words(text):\n",
        "  words_array = re.findall(r\"[\\w']+\", text)\n",
        "  return len(words_array)\n",
        "  \n",
        "def description_of_text_from_url(url):\n",
        "  text = extract_text_from_url(url)\n",
        "  sentence_array = text.split(\"\\n\")\n",
        "  print(\"***************************************\")\n",
        "  print(\"Url: \" + url)\n",
        "  print(\"Number of words: \" + str(find_number_words(text)))\n",
        "  print(\"Number of sentences: \" + str(len(sentence_array)))\n",
        "  print(\"List of sentences: \")\n",
        "  print(sentence_array)\n",
        "  print(\"***************************************\")\n",
        "  \n",
        "url1 = \"https://www.ibm.com/cloud/learn/natural-language-processing\"\n",
        "url2 = \"https://www.dominodatalab.com/blog/natural-language-in-python-using-spacy\"\n",
        "url3 = \"https://www.analyticsvidhya.com/blog/2020/12/understanding-text-classification-in-nlp-with-movie-review-example-example/\"\n",
        "url4 = \"https://blogs.commons.georgetown.edu/cctp-607-spring2019/2019/02/27/natural-language-processing-google-translate/\"\n",
        "url5 = \"https://towardsai.net/p/nlp/natural-language-processing-nlp-with-python-tutorial-for-beginners-1f54e610a1a0\"\n",
        "urls = [url1, url2, url3, url4, url5]\n",
        "\n",
        "for url in urls:\n",
        "  description_of_text_from_url(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Técnica Bag of Words"
      ],
      "metadata": {
        "id": "9WwkNJbcB7ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contractions = {\n",
        "\"ain't\" :\"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot have\",\n",
        "\"cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"doesn’t\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",    \n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how does\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"it’s\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"musn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so is\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"they'd\":  \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",    \n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\" u \": \"you\",\n",
        "\" ur \": \"your\",\n",
        "\" n \": \" and \"\n",
        "}\n",
        "\n",
        "def replace_contractions(x):\n",
        "    for word in x.split():\n",
        "        if word in contractions:\n",
        "          x = x.replace(word, contractions[word])\n",
        "    return x\n",
        "\n",
        "def print_matrix(matrix, vocab):\n",
        "  head = \"Doc\"\n",
        "  line = \"\"\n",
        "  table = \"\"\n",
        "  for i in range(len(matrix)):\n",
        "    head = head + \" \" + str(i+1)\n",
        "  head = head + \"\\n\"\n",
        "  head = head + \"-----------------\"\n",
        "  print(head)  \n",
        "  for i in range(len(vocab)):\n",
        "    line = vocab[i] + \" \"\n",
        "    for j in range(len(matrix)):\n",
        "      line = line + str(matrix[j][i]) + \" \"\n",
        "    table = table + line + \"\\n\"\n",
        "  print(table)  \n",
        "\n",
        "def get_vocabulary_from_texts(urls):\n",
        "  vocab = []\n",
        "  for url in urls:\n",
        "    text = extract_text_from_url(url)\n",
        "    text = replace_contractions(text)\n",
        "    words = re.findall(r'\\w+',text)\n",
        "    for word in words:\n",
        "      if word not in vocab:\n",
        "        vocab.append(word)\n",
        "  return vocab\n",
        "\n",
        "def get_all_texts(urls):\n",
        "  documents = []\n",
        "  for url in urls:\n",
        "    text = extract_text_from_url(url)\n",
        "    text = replace_contractions(text)\n",
        "    words = re.findall(r'\\w+',text)\n",
        "    documents.append(words)\n",
        "  return documents\n",
        "\n",
        "print(get_vocabulary_from_texts(urls))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkx9nPPUDRom",
        "outputId": "0c66957c-b869-4213-ee61-f78d2590c72e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Skip', 'to', 'content', 'IBM', 'Cloud', 'Learn', 'Hub', 'What', 'is', 'Natural', 'Language', 'Processing', 'NLP', 'By', 'Education', '2', 'July', '2020', 'Artificial', 'intelligence', 'natural', 'language', 'processing', 'tasks', 'tools', 'and', 'approaches', 'use', 'cases', 'Watson', 'Jump', 'strives', 'build', 'machines', 'that', 'understand', 'respond', 'text', 'or', 'voice', 'data', 'with', 'speech', 'of', 'their', 'own', 'in', 'much', 'the', 'same', 'way', 'humans', 'do', 'refers', 'branch', 'computer', 'science', 'more', 'specifically', 'artificial', 'AI', 'concerned', 'giving', 'computers', 'ability', 'spoken', 'words', 'human', 'beings', 'can', 'combines', 'computational', 'linguistics', 'rule', 'based', 'modeling', 'statistical', 'machine', 'learning', 'deep', 'models', 'Together', 'these', 'technologies', 'enable', 'process', 'form', 'its', 'full', 'meaning', 'complete', 'speaker', 'writer', 's', 'intent', 'sentiment', 'drives', 'programs', 'translate', 'from', 'one', 'another', 'commands', 'summarize', 'large', 'volumes', 'rapidly', 'even', 'real', 'time', 'There', 'a', 'good', 'chance', 'you', 've', 'interacted', 'operated', 'GPS', 'systems', 'digital', 'assistants', 'dictation', 'software', 'customer', 'service', 'chatbots', 'other', 'consumer', 'conveniences', 'But', 'also', 'plays', 'growing', 'role', 'enterprise', 'solutions', 'help', 'streamline', 'business', 'operations', 'increase', 'employee', 'productivity', 'simplify', 'mission', 'critical', 'processes', 'Human', 'filled', 'ambiguities', 'make', 'it', 'incredibly', 'difficult', 'write', 'accurately', 'determines', 'intended', 'Homonyms', 'homophones', 'sarcasm', 'idioms', 'metaphors', 'grammar', 'usage', 'exceptions', 'variations', 'sentence', 'structure', 'just', 'few', 'irregularities', 'take', 'years', 'learn', 'but', 'programmers', 'must', 'teach', 'driven', 'applications', 'recognize', 'start', 'if', 'those', 'are', 'going', 'be', 'useful', 'Several', 'break', 'down', 'ways', 'sense', 'what', 'ingesting', 'Some', 'include', 'following', 'Speech', 'recognition', 'called', 'task', 'reliably', 'converting', 'into', 'required', 'for', 'any', 'application', 'follows', 'answers', 'questions', 'makes', 'especially', 'challenging', 'people', 'talk', 'quickly', 'slurring', 'together', 'varying', 'emphasis', 'intonation', 'different', 'accents', 'often', 'using', 'incorrect', 'Part', 'tagging', 'grammatical', 'determining', 'part', 'particular', 'word', 'piece', 'on', 'context', 'identifies', 'as', 'verb', 'I', 'paper', 'plane', 'noun', 'car', 'Word', 'disambiguation', 'selection', 'multiple', 'meanings', 'through', 'semantic', 'analysis', 'determine', 'most', 'given', 'For', 'example', 'helps', 'distinguish', 'grade', 'achieve', 'vs', 'bet', 'place', 'Named', 'entity', 'NEM', 'phrases', 'entities', 'Kentucky', 'location', 'Fred', 'man', 'name', 'Co', 'reference', 'resolution', 'identifying', 'when', 'two', 'refer', 'The', 'common', 'person', 'object', 'which', 'certain', 'pronoun', 'e', 'g', 'she', 'Mary', 'involve', 'metaphor', 'an', 'idiom', 'instance', 'bear', 'not', 'animal', 'hairy', 'Sentiment', 'attempts', 'extract', 'subjective', 'qualities', 'attitudes', 'emotions', 'confusion', 'suspicion', 'generation', 'sometimes', 'described', 'opposite', 'putting', 'structured', 'information', 'See', 'blog', 'post', 'NLU', 'NLG', 'differences', 'between', 'three', 'concepts', 'deeper', 'look', 'how', 'relate', 'Python', 'Toolkit', 'NLTK', 'programing', 'provides', 'wide', 'range', 'libraries', 'attacking', 'specific', 'Many', 'found', 'open', 'source', 'collection', 'education', 'resources', 'building', 'includes', 'many', 'listed', 'above', 'plus', 'subtasks', 'such', 'parsing', 'segmentation', 'stemming', 'lemmatization', 'methods', 'trimming', 'roots', 'tokenization', 'breaking', 'sentences', 'paragraphs', 'passages', 'tokens', 'better', 'It', 'implementing', 'capabilities', 'reasoning', 'reach', 'logical', 'conclusions', 'facts', 'extracted', 'Statistical', 'earliest', 'were', 'hand', 'coded', 'rules', 'could', 'perform', 'easily', 'scale', 'accommodate', 'seemingly', 'endless', 'stream', 'increasing', 'Enter', 'algorithms', 'automatically', 'classify', 'label', 'elements', 'then', 'assign', 'likelihood', 'each', 'possible', 'Today', 'techniques', 'convolutional', 'neural', 'networks', 'CNNs', 'recurrent', 'RNNs', 'they', 'work', 'ever', 'accurate', 'huge', 'raw', 'unstructured', 'unlabeled', 'sets', 'dive', 'nuances', 'see', 'Machine', 'Learning', 'Deep', 'Neural', 'Networks', 'Difference', 'driving', 'force', 'behind', 'modern', 'world', 'Here', 'examples', 'Spam', 'detection', 'You', 'may', 'think', 'spam', 'solution', 'best', 'classification', 'scan', 'emails', 'indicates', 'phishing', 'These', 'indicators', 'overuse', 'financial', 'terms', 'characteristic', 'bad', 'threatening', 'inappropriate', 'urgency', 'misspelled', 'company', 'names', 'handful', 'problems', 'experts', 'consider', 'mostly', 'solved', 'although', 'argue', 'this', 'does', 'match', 'your', 'email', 'experience', 'translation', 'Google', 'Translate', 'widely', 'available', 'technology', 'at', 'Truly', 'involves', 'than', 'replacing', 'Effective', 'has', 'capture', 'tone', 'input', 'desired', 'impact', 'output', 'making', 'progress', 'accuracy', 'A', 'great', 'test', 'tool', 'back', 'original', 'An', 'oft', 'cited', 'classic', 'Not', 'long', 'ago', 'translating', 'spirit', 'willing', 'flesh', 'weak', 'English', 'Russian', 'yielded', 'vodka', 'meat', 'rotten', 'result', 'desires', 'isn', 't', 'perfect', 'inspires', 'confidence', 'Virtual', 'agents', 'Apple', 'Siri', 'Amazon', 'Alexa', 'patterns', 'appropriate', 'action', 'helpful', 'comments', 'Chatbots', 'magic', 'response', 'typed', 'entries', 'contextual', 'clues', 'about', 'requests', 'them', 'provide', 'responses', 'options', 'over', 'next', 'enhancement', 'question', 'answering', 'our', 'anticipated', 'relevant', 'Social', 'media', 'become', 'essential', 'uncovering', 'hidden', 'insights', 'social', 'channels', 'analyze', 'used', 'posts', 'reviews', 'products', 'promotions', 'events', 'companies', 'product', 'designs', 'advertising', 'campaigns', 'Text', 'summarization', 'uses', 'digest', 'create', 'summaries', 'synopses', 'indexes', 'research', 'databases', 'busy', 'readers', 'who', 'have', 'read', 'add', 'innovated', 'space', 'by', 'pioneering', 'services', 'organizations', 'automate', 'complex', 'while', 'gaining', 'Discovery', 'Surface', 'high', 'quality', 'rich', 'documents', 'tables', 'PDFs', 'big', 'search', 'Enable', 'employees', 'informed', 'decisions', 'save', 'engine', 'mining', 'extraction', 'relationships', 'buried', 'leverages', 'custom', 'users', 'understands', 'unique', 'industry', 'Explore', 'Understanding', 'Analyze', 'formats', 'including', 'HTML', 'webpages', 'Increase', 'understanding', 'leveraging', 'kit', 'identify', 'keywords', 'categories', 'semantics', 'named', 'NER', 'Assistant', 'Improve', 'reducing', 'costs', 'chatbot', 'easy', 'visual', 'builder', 'so', 'deploy', 'virtual', 'across', 'channel', 'minutes', 'Purpose', 'built', 'healthcare', 'life', 'sciences', 'domains', 'Annotator', 'Clinical', 'Data', 'extracts', 'key', 'clinical', 'like', 'conditions', 'medications', 'allergies', 'procedures', 'values', 'attributes', 'develop', 'meaningful', 'Potential', 'sources', 'notes', 'discharge', 'trial', 'protocols', 'literature', 'get', 'started', 'visit', 'page', 'Sign', 'up', 'IBMid', 'account', 'Featured', 'Platform', 'Domino', 'Enterprise', 'MLOps', 'Components', 'System', 'Record', 'Integrated', 'Model', 'Factory', 'Self', 'Service', 'Infrastructure', 'Portal', 'Pricing', 'Nexus', 'Updates', 'Solutions', 'Role', 'Chief', 'Analytics', 'Executives', 'Science', 'Leaders', 'Scientists', 'IT', 'Industry', 'Financial', 'Services', 'Health', 'Life', 'Sciences', 'Insurance', 'More', 'Use', 'Cases', 'Open', 'Risk', 'Management', 'Customers', 'Resources', 'Guides', 'Videos', 'Blog', 'Events', 'Podcast', 'Community', 'Documentation', 'Partners', 'Tools', 'Implementation', 'Consulting', 'Become', 'Partner', 'Azure', 'Accenture', 'Snowflake', 'Company', 'About', 'Careers', 'We', 're', 'Hiring', 'News', 'Press', 'Contact', 'Watch', 'Demo', 'Try', 'Now', 'Executive', 'Media', 'Technology', 'Manufacturing', 'Retail', 'eCommerce', 'Consumer', 'Products', 'Field', 'Comparison', 'spaCy', 'Introduction', 'Paco', 'Nathan', 'September', '10', '2019', '17', 'min', 'This', 'article', 'brief', 'introduction', 'related', 'teams', 'lots', 'top', 'four', 'Usually', 'generated', 'always', 'Think', 'operating', 'system', 'Typically', 'there', 'contracts', 'sales', 'agreements', 'partnerships', 'invoices', 'insurance', 'policies', 'regulations', 'laws', 'All', 'represented', 'run', 'acronyms', 'roughly', 'speaking', 'respectively', 'Increasingly', 'overlap', 'becomes', 'categorize', 'feature', 'Spacy', 'How', 'go', 'Oftentimes', 'turn', 'various', 'manage', 'sPacy', 'library', 'conduct', 'advanced', 'underpin', 'document', 'all', 'forms', 'framework', 'along', 'plug', 'ins', 'integrations', 'features', 'quite', 'community', 'support', 'commercialization', 'advances', 'area', 'continues', 'evolve', 'working', 'analytics', 'Getting', 'Started', 'configured', 'default', 'Compute', 'Environment', 'packages', 'll', 'need', 'tutorial', 'Check', 'out', 'project', 'code', 'If', 'interested', 'Environments', 'check', 'Support', 'Page', 'let', 'us', 'load', 'some', 'import', 'spacy', 'nlp', 'en_core_web_sm', 'That', 'variable', 'now', 'gateway', 'things', 'loaded', 'small', 'model', 'Next', 'parser', 'rain', 'Spain', 'falls', 'mainly', 'plain', 'doc', 'token', 'print', 'lemma_', 'pos_', 'is_stop', 'DET', 'Truerain', 'NOUN', 'Falsein', 'ADP', 'TrueSpain', 'PROPN', 'Falsefalls', 'fall', 'VERB', 'Falsemainly', 'ADV', 'Falseon', 'Truethe', 'Trueplain', 'False', 'PUNCT', 'First', 'we', 'created', 'container', 'annotations', 'Then', 'iterated', 'had', 'parsed', 'Good', 'lot', 'info', 'bit', 'Let', 'reformat', 'parse', 'pandas', 'dataframe', 'pd', 'cols', 'lemma', 'POS', 'explain', 'stopword', 'rows', 'row', 'append', 'df', 'DataFrame', 'columns', 'Much', 'readable', 'In', 'simple', 'case', 'entire', 'merely', 'short', 'accessed', 'fields', 'show', 'root', 'flag', 'whether', 'i', 'filtered', 'displaCy', 'visualize', 'tree', 'displacy', 'render', 'style', 'dep', 'Does', 'bring', 'memories', 'school', 'Frankly', 'coming', 'background', 'diagram', 'sparks', 'joy', 'backup', 'moment', 'handle', 'boundary', 'SBD', 'known', 'builtin', 'sentencizer', 'zoo', 'day', 'was', 'doing', 'acting', 'walking', 'railing', 'gorilla', 'exhibit', 'fell', 'Everyone', 'screamed', 'Tommy', 'jumped', 'after', 'me', 'forgetting', 'he', 'blueberries', 'his', 'front', 'pocket', 'gorillas', 'went', 'wild', 'sent', 'sents', 'When', 'creates', 'principle', 'non', 'destructive', 'etc', 'simply', 'array', 'carve', 'little', 'pieces', 'So', 'span', 'end', 'index', '0', '25', '29', '48', '54', 'pull', 'Or', 'last', '51', 'At', 'point', 'segment', 'Acquiring', 'texts', 'where', 'One', 'quick', 'leverage', 'interwebs', 'Of', 'course', 'download', 'web', 'pages', 'Beautiful', 'Soup', 'popular', 'package', 'housekeeping', 'sys', 'warnings', 'filterwarnings', 'ignore', 'function', 'get_text', 'find', 'p', 'tags', 'bs4', 'BeautifulSoup', 'traceback', 'def', 'url', 'buf', 'try', 'soup', 'html', 'find_all', 'return', 'join', 'except', 'format_exc', 'exit', '1', 'grab', 'online', 'compare', 'licenses', 'hosted', 'Source', 'Initiative', 'site', 'lic', 'mit', 'https', 'opensource', 'org', 'MIT', 'asl', 'Apache', '20', 'bsd', 'BSD', '3', 'Clause', 'SPDX', 'identifier', 'Note', 'license', 'been', 'New', 'License', 'Modified', 'clause', 'similarity', 'metrics', 'among', 'pairs', 'b', '9482039305669306asl', '9391555350757145bsd', '9895838089575453', 'interesting', 'since', 'appear', 'similar', 'fact', 'closely', 'Admittedly', 'extra', 'included', 'due', 'OSI', 'disclaimer', 'footer', 'reasonable', 'approximation', 'comparing', 'Given', 'purely', 'standpoint', 'chunks', 'Steve', 'Jobs', 'Wozniak', 'incorporated', 'Computer', 'January', '1977', 'Cupertino', 'California', 'chunk', 'noun_chunks', 'JobsSteve', 'WozniakApple', 'ComputerJanuaryCupertinoCalifornia', 'generally', 'filter', 'reduce', 'distilled', 'representation', 'approach', 'further', 'within', 'proper', 'nouns', 'ent', 'ents', 'label_', 'PERSONSteve', 'PERSONApple', 'ORGJanuary', 'DATECupertino', 'GPECalifornia', 'GPE', 'excellent', 'knowledge', 'graph', 'linked', 'challenge', 'construct', 'links', 'linking', 'Identifying', 'first', 'step', 'kind', 'might', 'link', 'Wozniaknamed', 'lookup', 'DBpedia', 'general', 'lemmas', 'describe', 'early', 'section', 'able', 'venerable', 'WordNet', 'lexical', 'database', 'computable', 'thesaurus', 'integration', 'wordnet', 'Daniel', 'Vila', 'Suero', 'expert', 'via', 'happen', 'nltk', 'nltk_data', 'Downloading', 'home', 'ceteri', 'Package', 'already', 'date', 'True', 'runs', 'pipeline', 'allows', 'means', 'customizing', 'parts', 'supporting', 'really', 'workflow', 'WordnetAnnotator', 'spacy_wordnet', 'wordnet_annotator', 'before', 'pipe_names', 'add_pipe', 'lang', 'tagger', 'ner', 'Within', 'infamous', 'having', 'click', 'results', 'withdraw', '_', 'synsets', 'Synset', 'v', '01', 'retire', '02', 'disengage', 'recall', '07', 'swallow', '05', 'seclude', 'adjourn', 'bow_out', '09', '08', 'retreat', '04', 'remove', 'Lemma', 'pull_away', 'draw_back', 'recede', 'pull_back', 'wordnet_domains', 'astronomy', 'telegraphy', 'psychology', 'ethnology', 'administration', 'finance', 'economy', 'exchange', 'banking', 'commerce', 'medicine', 'university', 'Again', 'graphs', 'larger', 'sections', 'technique', 'beyond', 'scope', 'currently', 'Going', 'direction', 'know', 'priori', 'domain', 'set', 'topics', 'constrain', 'returned', 'want', 'Finance', 'Banking', '5', '000', 'euros', 'enriched_sent', 'wordnet_synsets_for_domain', 'lemmas_for_synset', 'synset', 'variants', 'enriched', 'extend', 'lemma_names', 'format', 'else', 'require', 'draw_off', 'draw', 'take_out', 'play', 'list', 'combinatorial', 'explosion', 'without', 'constraints', 'Imagine', 'millions', 'd', 'searches', 'avoid', 'every', 'query', 'days', 'weeks', 'months', 'compute', 'Scattertext', 'Sometimes', 'encountered', 'trying', 'yet', 'corpus', 'dataset', 'interactive', 'visualization', 'scattertext', 'genius', 'Jason', 'Kessler', 'party', 'conventions', 'during', '2012', 'US', 'Presidential', 'elections', 'cell', 'number', 'crunching', 'worth', 'wait', 'st', 'merge_entities', 'create_pipe', 'merge_noun_chunks', 'convention_df', 'SampleCorpora', 'ConventionData2012', 'get_data', 'CorpusFromPandas', 'category_col', 'text_col', 'Once', 'ready', 'generate', 'produce_scattertext_explorer', 'category', 'democrat', 'category_name', 'Democratic', 'not_category_name', 'Republican', 'width_in_pixels', '1000', 'metadata', 'give', 'minute', 'IPython', 'display', 'IFrame', 'file_name', 'foo', 'wb', 'f', 'encode', 'utf', '8', 'src', 'width', '1200', 'height', '700', 'past', 'organization', 'Suppose', 'team', 'needed', 'customers', 'talking', 'come', 'handy', 'cluster', 'k', 'NPS', 'scores', 'evaluation', 'metric', 'replace', 'Democrat', 'dimension', 'components', 'clustering', 'Summary', 'Five', 'asked', 'answer', 'would', 'everything', 'kitchen', 'sink', 'relatively', 'academic', 'Another', 'CoreNLP', 'Stanford', 'Also', 'albeit', 'powerful', 'though', 'integrate', 'production', 'corner', 'began', 'change', 'principal', 'authors', 'Matthew', 'Honnibal', 'Ines', 'Montani', 'launched', '2015', 'adoption', 'rapid', 'They', 'focused', 'opinionated', 'well', 'no', 'less', 'provided', 'workflows', 'faster', 'execution', 'alternatives', 'Based', 'priorities', 'became', 'sort', 'Since', 'consistently', 'being', 'depending', 'directions', 'commercial', 'said', 'incorporate', 'SOTA', 'effectively', 'becoming', 'conduit', 'moving', 'important', 'note', 'got', 'boost', 'mid', '2000', 'win', 'international', 'competitions', 'occurred', '2017', '2018', 'successes', 'previous', 'ELMo', 'embedding', 'Allen', 'followed', 'BERT', 'recently', 'ERNIE', 'Baidu', 'giants', 'gifted', 'rest', 'Sesame', 'Street', 'repertoire', 'embedded', 'state', 'art', 'Speaking', 'keep', 'track', 'eye', 'Progress', 'Papers', 'Code', 'shifted', 'dramatically', 'arose', 'fore', 'Circa', '2014', 'shown', 'count', 'keyword', 'target', 'underwhelming', 'analyzing', 'thousands', 'vendor', 'industrial', 'supply', 'chain', 'optimization', 'hundreds', 'policyholders', 'gazillions', 'regarding', 'disclosures', 'contemporary', 'tends', 'construction', 'increasingly', 'numbers', 'summarized', 'Universe', 'dives', 'field', 'evolving', 'selections', 'universe', 'Blackstone', 'legal', 'Kindred', 'extracting', 'biomedical', 'Pharma', 'mordecai', 'geographic', 'Prodigy', 'loop', 'annotation', 'labeling', 'datasets', 'Rasa', 'chat', 'apps', 'couple', 'super', 'new', 'items', 'mention', 'pytorch', 'transformers', 'fine', 'tune', 'transfer', 'characters', 'friends', 'GPT', 'XLNet', 'IRL', 'conference', 'videos', 'talks', 'hopefully', 'wish', 'Practical', 'Techniques', 'Share', 'Subscribe', 'Newsletter', 'Receive', 'tips', 'tutorials', 'leading', 'leaders', 'right', 'inbox', 'Other', 'class', 'v3', 'David', 'Bloch', 'Comparing', 'Functionality', 'Libraries', 'guest', 'Maziyar', 'Panahi', 'Talby', 'cheat', 'sheet', 'choosing', 'Ta', 'around', 'spreadsheets', 'analyse', 'daily', 'basis', 'weather', 'forecast', 'Dr', 'J', 'Rogel', 'Salazar', '22', '135', 'Townsend', 'St', 'Floor', '5San', 'Francisco', 'CA', '94107', '415', '570', '2425', 'Product', 'Dictionary', 'Start', 'Articles', 'Vision', 'Visualization', 'Interview', 'Questions', 'Infographics', 'Podcasts', 'E', 'Books', 'Companies', 'Datahack', 'Summit', 'Glossary', 'Archive', 'Write', 'Article', 'Courses', 'Certified', 'ML', 'BlackBelt', 'Plus', 'Immersive', 'Bootcamp', 'Blogathon', 'Creators', 'Club', 'Join', 'Manage', 'AV', 'Account', 'My', 'Hackathons', 'Bookmarks', 'Applied', 'Out', 'Home', 'Related', 'Free', 'Movie', 'Review', 'Example', 'Facebook', 'Twitter', 'Linkedin', 'Parlad', 'Neupane', 'Published', 'On', 'December', '11', 'Last', '11th', 'Advanced', 'Classification', 'Entertainment', 'Programming', 'Project', 'Supervised', 'Unstructured', 'published', 'improved', 'tremendously', 'needing', 'underlying', 'hardware', 'infrastructure', 'Users', 'program', 'old', 'beneficiary', 'effect', 'unlimited', 'branches', 'gives', 'deliver', 'very', 'successful', 'resource', 'speeches', 'plenty', 'hard', 'mine', 'Written', 'contain', 'because', 'intelligent', 'writing', 'primary', 'communication', 'cognitive', 'assistant', 'filtering', 'fake', 'news', 'will', 'cover', 'Mainly', 'focusing', 'Words', 'Sequence', 'Analysis', 'vector', 'probabilistic', 'sequential', 'reorganization', 'fifty', 'thousand', 'IMDB', 'movie', 'reviewer', 'Our', 'goal', 'review', 'posted', 'user', 'positive', 'negative', 'Topic', 'List', 'Understand', 'Sequences', 'Vector', 'Semantic', 'Probabilistic', 'Models', 'Parsers', 'Semantics', 'Performing', 'cars', 'smartphones', 'speakers', 'websites', 'Translator', 'translator', 'wrote', 'desire', 'google', 'noises', 'CNN', 'native', 'reduces', 'asking', 'needs', 'cans', 'ask', 'problem', 'AMAZON', 'robust', 'asks', 'converts', 'understandable', 'internal', 'call', 'toke', 'goes', 'idea', 'retrieval', 'IR', 'deals', 'storage', 'repositories', 'retrieve', 'only', 'trim', 'unnecessary', 'Application', 'Translation', 'Information', 'Question', 'Answering', 'ChatBot', 'Summarization', 'Mining', 'sign', 'properly', 'symbols', 'sequence', 'clarification', 'categorizing', 'group', 'predefined', 'topic', 'Rule', 'Hybrid', 'separated', 'organized', 'handicraft', 'linguistic', 'Those', 'define', 'characterized', 'groups', 'Donald', 'Trump', 'Boris', 'Johnson', 'categorized', 'politics', 'People', 'LeBron', 'James', 'Ronaldo', 'sports', 'classifier', 'learns', 'observation', 'User', 'prelabeled', 'tarin', 'collects', 'strategy', 'inputs', 'continuously', 'bag', 'extension', 'represents', 'frequency', 'dictionary', 'Nai', 'Bayer', 'SVM', 'third', 'Approach', 'tag', 'train', 'compared', 'something', 'improve', 'manually', 'method', 'implement', 'defines', 'interprets', 'main', 'alike', 'divide', 'multi', 'dimensional', 'Embedding', 'translates', 'spares', 'vectors', 'low', 'preserves', 'type', 'types', 'Word2vec', 'Doc2Vec', 'Word2Vec', 'standalone', 'analyzes', 'calculate', 'probability', 'occurring', '00013131', 'percent', 'Labeling', 'typical', 'assigns', 'someone', 'says', 'tom', 'hanks', 'Play', 'Movies', 'Tom', 'Hanks', 'divides', 'LSTM', 'Parsing', 'phase', 'syntactic', 'constituent', 'ate', 'apple', 'divided', 'determiner', 'discuss', 'classified', 'algorithm', 'discover', 'covers', 'Bags', 'bidirectional', 'labeled', 'TensorFlow', 'steps', 'cleaning', 'balance', 'sampling', 'Parser', 'component', 'separates', 'details', 'heart', 'communicate', 'written', 'conversation', 'lengthy', 'articles', 'books', 'seeks', 'constructing', 'convey', 'feedback', 'Sample', 'Importing', 'necessary', 'defined', 'kaggle', 'python', 'Docker', 'image', 'github', 'com', 'docker', 'here', 'several', 'numpy', 'np', 'linear', 'algebra', 'CSV', 'file', 'O', 'read_csv', 'Input', 'files', 'directory', 'running', 'clicking', 'pressing', 'Shift', 'under', 'os', 'dirname', 'filenames', 'walk', 'filename', 'path', '20GB', 'current', 'gets', 'preserved', 'version', 'Save', 'Run', 'temporary', 'temp', 'won', 'saved', 'outside', 'session', 'matplotlib', 'pyplot', 'plt', 'tkinter', 'seaborn', 'sns', 'scipy', 'tensorflow', 'tf', 'tensorflow_hub', 'hub', 'tensorflow_datasets', 'tfds', 'keras', 'Sequential', 'layers', 'Dense', 'sklearn', 'model_selection', 'train_test_split', 'confusion_matrix', 'classification_report', 'cells', 'takes', 'please', 'once', 'Split', 'training', '60', '40', '15', 'validation', 'testing', 'original_train_data', 'original_validation_data', 'original_test_data', 'imdb_reviews', 'split', 'as_supervised', 'Keras', 'tokanizing', 'word_index', 'imdb', 'get_word_index', 'imdb_word_index', 'json', '16', '14', '9', '6', '18', '4', 'br', '7', '13', 'film', '19', '12', 'Positive', 'Negative', 'Comparision', 'Creating', 'Train', 'Test', 'Splitting', 'fitting', 'Overview', 'Confusion', 'Matrix', 'Correlation', 'Report', 'publicly', 'Tensorflow', 'follow', 'GitHub', 'Repository', 'conclusion', 'opportunities', 'tremendous', 'Knowledge', 'impossible', 'five', 'rise', 'made', 'still', 'blogathondoc2vevparsingtext', 'classificationwor2vec', 'Duration', 'Oct', '21', '27', 'Table', 'contents', 'Author', 'Top', 'Authors', 'view', 'Download', 'Vidhya', 'App', 'Latest', 'Previous', 'Post', 'Kick', 'Journey', 'Pattern', 'Recognition', 'Leave', 'Reply', 'Your', 'address', 'Required', 'marked', 'Cancel', 'reply', 'Notify', 'Submit', 'Tutorial', 'Working', 'Harika', 'Bonthu', 'Aug', '2021', 'Boost', 'Accuracy', 'Imbalanced', 'COVID', 'Mortality', 'Prediction', 'Using', 'GAN', 'Bala', 'Gangadhar', 'Thilak', 'Adiboina', 'Most', 'Comprehensive', 'Guide', 'K', 'Means', 'Clustering', 'Ever', 'Need', 'Pulkit', 'Sharma', 'Joins', 'Pandas', 'Master', 'Different', 'Types', 'Abhishek', 'Feb', 'Us', 'Team', 'Hackathon', 'Discussions', 'Apply', 'Trainings', 'Advertising', 'Visit', 'Copyright', '2013', '2022', 'Privacy', 'Policy', 'Terms', 'Refund', 'cookies', 'traffic', 'agree', 'AcceptPrivacy', 'Cookies', 'Close', 'website', 'navigate', 'stored', 'browser', 'basic', 'functionalities', 'consent', 'option', 'opt', 'opting', 'affect', 'browsing', 'Necessary', 'Always', 'Enabled', 'absolutely', 'ensures', 'security', 'store', 'personal', 'Non', 'Any', 'particularly', 'collect', 'ads', 'termed', 'mandatory', 'procure', 'prior', 'Please', 'signup', 'CCTP', '607', 'Big', 'Ideas', 'Spring', 'Menu', 'Course', 'Syllabus', 'Prof', 'Irvine', 'Weekly', 'Writing', 'Instructions', 'Final', 'replacement', 'As', 'seen', 'readings', 'module', 'Because', 'deconstruct', 'reconstruct', 'depend', 'addresses', 'complexity', 'ambiguity', 'PBS', 'Crash', 'video', 'breaks', 'Deconstructing', 'smaller', 'processed', 'order', 'Development', 'Phrase', 'Structure', 'Rules', 'encapsulate', 'phrase', 'structures', 'trees', 'Image', 'retrieved', 'www', 'youtube', 'watch', 'fOvTtapxa9c', 'Parse', 'Trees', 'likely', 'explains', 'Additionally', 'Looking', 'Network', 'works', 'Emportium', 'describes', 'network', 'solver', 'job', 'solve', 'French', 'learned', 'brains', 'convert', 'Recurrent', 'Step', 'Take', 'Convert', 'AIpXjFwVdIE', 'According', 'Encoder', 'Decoder', 'Architecture', 'pictured', 'medium', 'length', 'Cho', 'et', 'al', 'Emporium', 'tested', 'RNN', 'longer', 'translations', 'did', 'lack', 'present', 'While', 'generating', '10th', 'looks', 'nine', 'looking', 'both', 'Therefore', 'BiDirectional', 'Bidirectional', 'Vs', 'should', 'align', 'additional', 'unit', 'attention', 'mechanism', 'Process', 'Layer', 'Breakdown', 'encoder', 'assigned', 'focus', 'decoder', 'determined', 'Works', 'Cited', 'CrashCourse', 'Structures', 'YouTube', 'DuDz6B4cqVc', 'Intelligence', '34', 'z', 'EtmaFJieY', '36', 'CS', 'Dojo', 'Algorithm', 'Explained', 'Thierry', 'Poibeau', 'Cambridge', 'MA', 'Selections', 'entry', 'Week', 'February', 'Adey', 'Zegeye', 'navigation', 'Deblackboxing', 'Pidgin', 'Search', 'Recent', 'Posts', 'Design', 'Ethical', 'Implications', 'Explainable', 'XAI', 'Music', 'To', 'Ears', 'De', 'Blackboxing', 'Spotify', 'Recommendation', 'Engine', 'Gender', 'Bias', 'Challenges', 'Algorithmic', 'Composition', 'Chatting', 'Way', 'High', 'Brand', 'Equity', 'Class', 'Seminar', 'MembersKevin', 'Ackermann', 'Linda', 'Bardha', 'Annaliese', 'Blank', 'Beiyuan', 'Gu', 'Dominique', 'Haywood', 'Yajing', 'Hu', 'Proma', 'Huq', 'Deborah', 'Oliveros', 'Zachary', 'Omer', 'Shahin', 'Rafikian', 'Beiyue', 'Wang', 'Tianyi', 'Zhao', 'Professor', 'Site', 'AdministratorMartin', 'Uses', 'Martin', 'student', 'licensed', 'Creative', 'Commons', 'Attribution', 'Noncommercial', 'No', 'Derivative', 'United', 'States', 'educational', 'permitted', 'attribution', 'quoted', 'property', 'copyright', 'holders', 'creative', 'students', 'seminar', 'respective', 'writers', 'referenced', 'faculty', 'georgetown', 'edu', 'irvinem', 'NonCommercial', 'NoDerivs', 'Meta', 'Log', 'Entries', 'RSS', 'Comments', 'Trends', 'Shop', 'Categories', 'Fairness', 'Bioinformatics', 'Computing', 'Cybersecurity', 'Engineering', 'DevOps', 'Editorial', 'Ethics', 'Future', 'Game', 'Theory', 'Automated', 'Art', 'Mathematics', 'Neuroscience', 'Opinion', 'Optimization', 'Physics', 'Security', 'Program', 'Probability', 'Quantum', 'Research', 'Robotics', 'Scholarly', 'Cars', 'Software', 'Statistics', 'Systems', 'Web', 'Scraping', 'Sponsors', 'Tutorials', 'eBooks', 'Name', 'Towards', 'Legal', 'Inc', 'Description', 'publication', 'Read', 'thought', 'decision', 'makers', 'Website', 'towardsai', 'net', 'Publisher', 'publisher', 'Diversity', 'Masthead', 'Founder', 'Roberto', 'Iriondo', 'Cover', 'Logo', 'Areas', 'Served', 'Worldwide', 'Alternate', 'towards', 'ai', 'tai', 'toward', 'pub', 'Follow', 'LinkedIn', 'Instagram', 'Youtube', 'Github', 'Business', 'Maps', 'Discord', 'Medium', 'Flipboard', 'Publication', 'Feed', 'Contribute', 'stars', '497', 'Frequently', 'Used', 'Contextual', 'References', 'Remember', 'copy', 'IDs', 'whenever', 'URL', '304b2e42315e', 'enthusiasts', '161', 'likes', 'Updated', 'October', 'Pratik', 'Shukla', 'Pixabay', 'basics', 'sample', 'implementation', 'explore', 'toolkit', 'Afterward', 'coding', 'implementations', 'Colab', 'Contents', 'Applications', 'Current', 'challenges', 'Easy', 'Exploring', 'Features', 'Stemming', 'Lemmatization', 'PoS', 'Chunking', 'Chinking', 'Entity', 'Bag', 'Computers', 'tabular', 'However', 'speak', 'clear', 'interpret', 'subfield', 'depth', 'interactions', 'Chatbot', 'Intelligent', 'Classifications', 'Character', 'Spell', 'Checking', 'Detection', 'Autocomplete', 'Predictive', 'Typing', 'Figure', 'Revealing', 'listening', 'considerably', 'misunderstand', 'thing', 'differently', 'interpretation', 'saw', 'hill', 'telescope', 'interpretations', 'watched', 'him', 'my', 'm', 'Can', 'formation', 'second', 'represent', 'holds', 'food', 'liquid', 'Hence', 'deterministic', 'suitable', 'situations', 'freezing', 'temperature', 'lead', 'death', 'hot', 'coffee', 'burn', 'skin', 'requires', 'manual', 'effort', 'amounts', 'tries', 'derive', 'After', 'trained', 'outcomes', 'deduction', 'Lexical', 'With', 'whole', 'Syntactic', 'arranging', 'manner', 'shows', 'relationship', 'shop', 'house', 'pass', 'c', 'draws', 'exact', 'meaningfulness', 'Sentences', 'ice', 'cream', 'Disclosure', 'Integration', 'considers', 'ends', 'He', 'Pragmatic', 'overall', 'deriving', 'overview', 'beginners', 'Breaking', 'Tagging', 'Building', 'vocabulary', 'Linking', 'Extracting', 'Transforming', 'Ambiguity', 'usually', 'exciting', 'ease', 'Tokenization', 'Packages', 'Pros', 'cons', 'designed', 'fast', 'focuses', 'providing', 'Dependency', 'autocorrect', 'Analyzing', 'Gensim', 'purpose', 'handles', 'Latent', 'matrix', 'factorization', 'Converting', 'Finding', 'straightforward', 'syntax', 'scientific', 'highly', 'valuable', 'Spelling', 'correction', 'TextBlob', 'textual', 'Noun', 'Wordnet', 'Correction', 'dig', 'Small', 'snippet', 'string', 'notice', 'String', '675', 'Import', 'Sentence', 'tokenizing', 'sent_tokenize', 'tokenize', 'total', 'word_tokenize', '144', 'Find', 'distribution', 'FreqDist', 'Printing', 'ten', 'Notice', 'punctuation', 'marks', 'stopwords', 'actual', 'Plot', 'plot', 'Plotting', 'period', 'times', 'Analytically', 'removing', 'Remove', 'isalpha', 'separate', 'words_no_punc', 'lower', 'exclude', 'creating', 'excluded', 'cross', 'h', '23', '24', 'others', 'coordinating', 'conjunctions', '26', 'j', 'Removing', 'Cleaning', '28', 'Cleaned', 'Displaying', 'final', '30', 'showing', 'chart', 'frequent', 'bolder', 'font', 'thinner', 'fonts', 'beneficial', 'glance', 'analyzed', 'Properties', 'font_path', 'specifies', 'canvas', 'min_font_size', 'smallest', 'size', 'max_font_size', 'largest', 'font_step', 'max_words', 'maximum', 'cloud', 'eliminate', 'background_color', 'color', 'normalize_plurals', 'removes', 'trailing', 'documentation', 'WordCloud', '31', '32', 'displayed', 'shape', 'circle', '33', 'Circle', '35', 'mentioned', 'CloudAdvantages', 'engaging', 'casual', 'visually', 'appealing', 'Disadvantages', 'clean', 'normalize', 'languages', 'single', 'upon', 'study', 'studies', 'studying', 'studied', 'interpreter', 'Moreover', 'resolve', 'normalizes', 'truncating', 'stem', 'reduced', 'studi', 'Porter', 'Stemmer', 'below', 'truncate', 'stemmed', 'recognizable', '37', 'SnowballStemmer', 'generates', 'porter', 'stemmer', 'supports', '38', 'Languages', 'supported', 'snowball', '39', 'Various', 'Algorithms', 'pros', 'Lovin', '41', 'Dawson', '42', 'Krovetz', '43', 'Xerox', '44', 'Snowball', '45', 'scratch', 'math', 'detail', 'base', 'finds', 'instead', 'why', 'higher', 'crucial', 'tight', 'deadline', 'amortization', 'speed', 'outputs', 'choices', '46', 'Lemmatizer', 'truncated', '47', 'Lemmatizing', 'During', 'displays', 'demonstrating', 'lemmatizer', 'taking', 'apply', '49', 'Simple', 'value', 'n', '50', 'power', 'am', '52', 'Why', '53', 'Parts', 'Giving', 'correctly', 'Below', 'CC', 'Coordinating', 'Conjunction', 'conjunction', 'CD', 'Cardinal', 'Digit', '55', 'digit', 'DT', 'Determiner', '56', 'EX', 'Existential', '57', 'FW', 'Foreign', '58', 'IN', 'Preposition', 'Subordinating', '59', 'JJ', 'Adjective', 'JJR', 'Comparative', '61', 'comparative', 'JJS', 'Superlative', '62', 'LS', 'Marker', '63', 'marker', 'MD', 'Modal', '64', 'NN', 'Singular', '65', 'singular', 'NNS', 'Plural', '66', 'plural', 'NNP', 'Proper', '67', '68', 'PDT', 'Predeterminer', '69', 'Possessive', 'Endings', '70', 'endings', 'PRP', 'Personal', 'Pronoun', '71', '72', 'RB', 'Adverb', '73', 'RBR', '74', 'RBS', '75', 'superlative', 'RP', 'Particle', '76', 'TO', '77', 'UH', 'Interjection', '78', 'VB', 'Verb', 'Base', 'Form', '79', 'VBD', 'Past', 'Tense', '80', 'tense', 'VBG', 'Present', 'Participle', '81', 'participle', 'VBN', '82', 'VBP', 'Third', 'Person', '83', 'VBZ', '84', 'Wh', '85', 'WP', '86', '87', '88', '89', '90', 'Full', 'book', 'infer', 'literally', 'individual', '91', 'chunking', 'Before', 'Meaningful', 'significant', 'Phrases', 'NP', 'VP', 'ADJP', 'ADVP', 'Prepositional', 'PP', 'S', 'V', 'AP', '92', 'optional', 'adjectives', 'RegexpParser', '93', 'successfully', '94', 'Successful', 'excludes', 'extractions', 'unuseful', 'scenarios', 'chinking', 'chunked', 'apart', 'inverted', 'curly', 'braces', '95', 'From', '96', 'fundamental', 'places', 'money', 'discussed', 'Content', 'Summarizing', 'resumes', 'Optimizing', 'Customer', 'Commonly', '97', 'commonly', 'binary', '98', '99', 'Graph', 'equals', '100', '101', 'false', 'synonyms', 'antonyms', 'definitions', '102', '103', 'Gathering', '104', '105', 'Hypernyms', 'abstract', 'term', '106', 'hypernym', 'Hyponyms', '107', 'hyponym', 'Get', '108', 'Synonyms', '109', 'Antonyms', '110', '111', '112', 'ratio', '113', '114', 'discard', 'occurrences', 'counts', 'summary', '115', 'Raw', 'Clean', 'contains', 'Tokenize', 'Vocab', 'Generate', 'frequencies', 'Jim', 'Pam', 'traveled', 'bus', 'late', 'flight', 'Traveling', 'expensive', '116', '117', 'Combining', '118', 'Combination', '119', '120', '121', 'Output', '122', 'Limitations', 'ignores', 'Preprocessing', 'preprocessing', 'cleansing', 'TF', 'IDF', 'stands', 'Term', 'Frequency', 'Inverse', 'Document', 'scoring', 'measure', 'score', 'intuition', 'appears', 'importance', 'fewer', 'maybe', 'cannot', 'dog', 'descriptions', 'wants', 'cute', 'closest', 'possibly', 'suppose', 'furry', 'doggo', 'lovely', 'description', 'forth', 'calculates', 'none', 'repeat', 'instrumental', 'Eventually', 'increases', 'discriminative', 'Simply', 'put', 'rarer', 'vice', 'versa', '123', 'Calculation', 'Represent', 'table', '124', '125', 'Calculating', 'formula', '126', '127', 'Resulting', '128', '129', 'multiplication', '130', 'resulting', 'discriminate', 'smoothing', 'variation', 'log', 'again', '131', '132', '133', 'calculations', 'formulas', 'Actual', 'Calculations', '134', 'calculation', 'Formula', '136', 'Applying', '137', '138', '139', '140', 'Conclusion', 'hope', 'enjoyed', 'reading', 'suggestions', 'continue', 'views', 'expressed', 'author', 'Carnegie', 'Mellon', 'University', 'nor', 'directly', 'indirectly', 'associated', 'writings', 'intend', 'rather', 'reflection', 'thinking', 'catalyst', 'discussion', 'improvement', 'Citation', 'contexts', 'cite', 'BibTex', 'citation', 'pratik_iriondo_2020', 'title', 'journal', 'year', 'month', 'Jul', 'gathered', 'American', 'Literature', 'americanliterature', 'KDnuggets', 'kdnuggets', 'wtf', 'idf', 'Multidisciplinary', 'Journal', 'head', 'Snorkel', 'Silicon', 'Valley', 'unicorn', 'empowers', 'scientists', 'developers', 'scalable', 'Flow', 'centric', 'platform', 'co', 'founder', 'advisor', 'CEO', 'internationally', 'servicing', 'startups', 'enterprises', 'clients', 'Asia', 'Europe', 'Previously', 'acquired', 'engineer', 'marketing', 'Department', 'strategist', 'helped', 'goals', 'significantly', 'ROIs', 'MBZUAI', 'Gather', 'Determined', 'Lambda', 'Labs', 'Udacity', 'Setting', 'Alpaca', 'API', 'algorithmic', 'trading', 'Time', 'Series', 'prediction', 'Adaptive', 'Gradient', 'Descent', 'Variants', 'Mathematical', 'Intuition', 'Which', 'Task', 'NOT', 'Benefit', 'Pre', 'Avremee', 'August', 'explanations', 'thank', 'Shivakumar', 'Kokkula', 'June', 'Superb', 'Hemalatha', 'Abothula', 'javed', 'khan', 'explanation', 'Feedback', 'Select', 'Category', 'Cryptography', 'Centric', 'Futuristic', 'Innovation', 'Internet', 'Things', 'Logic', 'Scientific', 'Weak', 'Supervision', 'Popular', 'Best', 'Workstations', 'Descriptive', 'Decision', 'Making', 'Paid', 'Recommendations', 'Laptops', 'newsletter', 'Seal', 'Containerized', 'Deal', 'Podman', 'March', 'Gaussian', 'Naive', 'Bayes', 'Hands', 'Scikit', 'Manipulating', 'Logistic', 'Regression', 'Binary', 'Multiclass', 'SciKit', 'vision', 'Convolutional', 'finder', 'demystifying', 'series', 'inventory', 'management', 'Japan', 'Linear', 'Algebra', 'Reinforcement', 'Yolo', 'V5', 'World', 'Leading', 'CompanyHome', '228', 'Park', 'Avenue', 'South', 'PMB', '99625', 'York', 'NY', '10003', 'protected', 'offerings', '650', '246', '9381', 'mw', 'YUqeeOatA2', 'programming', 'Interesting', 'Facts', 'KLLGedpIOm', 'latest', 'Three', 'Theories', 'Help', 'Overfitting', 'Underfitting', 'LiCbkynzII', 'Quintic', 'Polynomial', 'Roots', 'OSOS', 'Quads', 'Tricky', 'Cubic', 'zHy0WqLF5t', 'Make', 'College', 'Essay', 'Harvard', 'Ready', 'hraSbxCaLP', 'recognized', 'tech', 'o', 'VaUlz7qF5i', 'Rights', 'Reserved', 'remembering', 'preferences', 'visits', 'Accept', 'ALL', 'Do', 'sell', 'Cookie', 'SettingsAcceptManage', 'ensure', 'anonymously', 'CookieDurationDescriptioncookielawinfo', 'checkbox', 'analytics11', 'monthsThis', 'cookie', 'GDPR', 'Consent', 'plugin', 'cookielawinfo', 'functional11', 'monthsThe', 'record', 'Functional', 'necessary11', 'others11', 'performance11', 'Performance', 'viewed_cookie_policy11', 'consented', 'sharing', 'platforms', 'feedbacks', 'performance', 'delivering', 'visitors', 'Analytical', 'interact', 'bounce', 'rate', 'Advertisement', 'customized', 'Others', 'uncategorized', 'Hit', 'enter', 'ESC', 'close']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_bag_of_words(urls):\n",
        "  documents = get_all_texts(urls)\n",
        "  vocab = get_vocabulary_from_texts(urls)\n",
        "  matrix = []\n",
        "  for document in documents:\n",
        "    array = []\n",
        "    for word in vocab:\n",
        "      if word not in document:\n",
        "        array.append(0)\n",
        "      else:\n",
        "        array.append(document.count(word))\n",
        "    matrix.append(array)\n",
        "  print_matrix(matrix, vocab)\n",
        "generate_bag_of_words(urls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INcTV1fDPSjv",
        "outputId": "ca25448a-851e-416f-84f0-2259ffd487d8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doc 1 2 3 4 5\n",
            "-----------------\n",
            "Skip 1 1 0 1 0 \n",
            "to 54 65 54 24 128 \n",
            "content 1 2 0 2 4 \n",
            "IBM 11 0 0 0 0 \n",
            "Cloud 3 2 0 1 10 \n",
            "Learn 1 0 0 0 4 \n",
            "Hub 1 0 0 0 0 \n",
            "What 7 1 2 0 3 \n",
            "is 28 26 49 13 104 \n",
            "Natural 15 2 1 3 28 \n",
            "Language 8 2 3 4 30 \n",
            "Processing 4 1 1 3 25 \n",
            "NLP 34 5 26 1 52 \n",
            "By 1 2 2 0 4 \n",
            "Education 1 0 0 0 3 \n",
            "2 1 3 1 2 9 \n",
            "July 1 0 0 0 1 \n",
            "2020 1 0 4 0 4 \n",
            "Artificial 1 0 2 2 3 \n",
            "intelligence 4 0 2 0 3 \n",
            "natural 10 21 1 0 12 \n",
            "language 33 25 8 9 20 \n",
            "processing 11 3 2 0 18 \n",
            "tasks 8 3 0 0 5 \n",
            "tools 7 0 0 0 0 \n",
            "and 79 45 65 19 128 \n",
            "approaches 4 1 0 0 1 \n",
            "use 10 11 6 3 30 \n",
            "cases 3 4 0 0 6 \n",
            "Watson 15 0 0 0 0 \n",
            "Jump 1 0 0 0 0 \n",
            "strives 1 0 0 0 0 \n",
            "build 1 2 1 0 2 \n",
            "machines 1 0 1 0 1 \n",
            "that 20 27 15 9 49 \n",
            "understand 5 4 8 1 11 \n",
            "respond 5 0 0 0 0 \n",
            "text 32 37 22 1 55 \n",
            "or 21 8 7 2 25 \n",
            "voice 11 0 2 0 0 \n",
            "data 16 11 14 1 28 \n",
            "with 18 23 9 5 61 \n",
            "speech 10 1 4 2 3 \n",
            "of 50 56 61 12 175 \n",
            "their 6 2 1 2 5 \n",
            "own 3 0 0 0 0 \n",
            "in 31 68 30 13 84 \n",
            "much 3 2 0 0 2 \n",
            "the 60 120 83 47 339 \n",
            "same 5 0 0 1 8 \n",
            "way 4 1 1 3 0 \n",
            "humans 2 0 1 0 4 \n",
            "do 3 7 2 4 5 \n",
            "refers 2 0 0 0 0 \n",
            "branch 2 0 0 0 0 \n",
            "computer 5 0 1 2 1 \n",
            "science 1 6 0 0 1 \n",
            "more 12 8 3 3 9 \n",
            "specifically 1 0 1 1 0 \n",
            "artificial 2 0 0 0 3 \n",
            "AI 5 2 2 2 55 \n",
            "concerned 1 0 0 0 0 \n",
            "giving 1 0 0 0 0 \n",
            "computers 2 0 1 6 3 \n",
            "ability 3 0 1 0 0 \n",
            "spoken 3 0 2 0 0 \n",
            "words 8 4 17 12 89 \n",
            "human 8 3 2 0 1 \n",
            "beings 1 0 1 0 1 \n",
            "can 7 15 8 3 47 \n",
            "combines 2 0 1 0 0 \n",
            "computational 1 1 0 0 1 \n",
            "linguistics 1 1 0 0 0 \n",
            "rule 1 0 7 0 0 \n",
            "based 5 2 14 0 4 \n",
            "modeling 1 0 0 0 2 \n",
            "statistical 3 0 1 0 0 \n",
            "machine 7 4 6 0 8 \n",
            "learning 10 8 5 0 9 \n",
            "deep 4 4 1 1 0 \n",
            "models 4 4 1 0 2 \n",
            "Together 1 0 0 0 0 \n",
            "these 8 3 5 0 6 \n",
            "technologies 4 0 0 0 0 \n",
            "enable 3 0 0 0 0 \n",
            "process 3 0 2 5 2 \n",
            "form 2 1 2 0 3 \n",
            "its 2 2 5 0 7 \n",
            "full 2 0 1 0 3 \n",
            "meaning 8 2 5 0 10 \n",
            "complete 1 0 1 0 0 \n",
            "speaker 1 1 1 0 0 \n",
            "writer 1 0 0 0 0 \n",
            "s 8 15 5 4 27 \n",
            "intent 1 0 0 0 0 \n",
            "sentiment 3 1 6 0 0 \n",
            "drives 1 0 0 0 0 \n",
            "programs 3 0 0 0 1 \n",
            "translate 3 0 1 1 0 \n",
            "from 9 24 15 6 35 \n",
            "one 5 8 1 1 3 \n",
            "another 2 0 2 3 2 \n",
            "commands 3 0 0 1 0 \n",
            "summarize 1 0 0 0 0 \n",
            "large 2 2 4 0 4 \n",
            "volumes 4 0 0 0 0 \n",
            "rapidly 1 1 0 0 0 \n",
            "even 2 0 1 0 3 \n",
            "real 3 0 1 0 0 \n",
            "time 5 0 3 1 4 \n",
            "There 1 3 4 0 7 \n",
            "a 22 77 49 24 138 \n",
            "good 3 1 0 0 1 \n",
            "chance 1 0 0 0 0 \n",
            "you 4 16 4 0 12 \n",
            "ve 1 1 1 0 0 \n",
            "interacted 1 0 0 0 0 \n",
            "operated 1 0 0 0 0 \n",
            "GPS 1 0 0 0 0 \n",
            "systems 3 0 0 0 3 \n",
            "digital 2 0 0 0 0 \n",
            "assistants 1 0 0 0 0 \n",
            "dictation 1 0 0 0 0 \n",
            "software 2 2 1 0 1 \n",
            "customer 2 2 2 0 0 \n",
            "service 1 0 0 0 0 \n",
            "chatbots 2 0 1 0 2 \n",
            "other 1 10 3 0 15 \n",
            "consumer 1 0 0 0 0 \n",
            "conveniences 1 0 0 0 0 \n",
            "But 1 1 1 0 1 \n",
            "also 6 5 7 2 13 \n",
            "plays 1 0 0 0 0 \n",
            "growing 1 1 0 0 0 \n",
            "role 1 0 0 0 0 \n",
            "enterprise 2 0 0 0 1 \n",
            "solutions 1 0 0 0 0 \n",
            "help 3 1 2 0 8 \n",
            "streamline 1 0 0 0 0 \n",
            "business 6 1 0 0 1 \n",
            "operations 1 0 0 0 1 \n",
            "increase 1 0 0 0 2 \n",
            "employee 1 0 0 0 0 \n",
            "productivity 1 0 0 0 0 \n",
            "simplify 1 0 0 0 0 \n",
            "mission 1 0 0 0 0 \n",
            "critical 1 0 0 0 0 \n",
            "processes 2 0 0 0 0 \n",
            "Human 1 0 1 0 0 \n",
            "filled 1 0 0 0 0 \n",
            "ambiguities 1 0 0 0 0 \n",
            "make 9 0 1 1 2 \n",
            "it 5 10 8 6 44 \n",
            "incredibly 1 0 0 0 0 \n",
            "difficult 1 2 2 0 0 \n",
            "write 1 2 2 0 3 \n",
            "accurately 3 0 0 1 0 \n",
            "determines 1 0 3 0 0 \n",
            "intended 1 0 0 0 0 \n",
            "Homonyms 1 0 0 0 0 \n",
            "homophones 1 0 0 0 0 \n",
            "sarcasm 2 0 0 0 0 \n",
            "idioms 1 0 0 0 0 \n",
            "metaphors 1 0 0 0 0 \n",
            "grammar 3 0 2 4 5 \n",
            "usage 1 0 4 0 1 \n",
            "exceptions 2 0 0 0 0 \n",
            "variations 1 0 0 0 1 \n",
            "sentence 2 15 1 9 18 \n",
            "structure 1 0 2 1 5 \n",
            "just 1 5 0 1 0 \n",
            "few 2 3 0 0 0 \n",
            "irregularities 1 0 0 0 0 \n",
            "take 1 3 1 1 6 \n",
            "years 1 5 1 0 0 \n",
            "learn 3 0 0 1 0 \n",
            "but 9 7 4 0 9 \n",
            "programmers 1 0 0 0 0 \n",
            "must 1 1 0 0 1 \n",
            "teach 1 0 0 0 0 \n",
            "driven 2 0 0 0 1 \n",
            "applications 6 1 0 0 2 \n",
            "recognize 3 0 0 0 0 \n",
            "start 1 3 0 0 0 \n",
            "if 2 9 2 0 3 \n",
            "those 2 6 0 0 5 \n",
            "are 4 13 19 5 53 \n",
            "going 1 0 1 0 14 \n",
            "be 1 8 10 6 19 \n",
            "useful 4 0 3 0 4 \n",
            "Several 1 0 0 0 0 \n",
            "break 1 0 0 0 0 \n",
            "down 2 0 0 1 0 \n",
            "ways 1 0 0 0 1 \n",
            "sense 4 1 0 2 3 \n",
            "what 1 2 5 0 8 \n",
            "ingesting 1 0 0 0 0 \n",
            "Some 1 1 0 0 0 \n",
            "include 4 2 0 0 0 \n",
            "following 1 3 1 0 6 \n",
            "Speech 2 0 0 0 14 \n",
            "recognition 7 0 0 1 3 \n",
            "called 2 6 0 1 2 \n",
            "task 3 0 2 1 0 \n",
            "reliably 1 0 0 0 0 \n",
            "converting 1 0 0 0 0 \n",
            "into 5 11 11 5 11 \n",
            "required 1 0 0 0 2 \n",
            "for 16 55 19 6 78 \n",
            "any 3 1 1 0 9 \n",
            "application 1 1 0 0 0 \n",
            "follows 1 0 0 0 0 \n",
            "answers 3 0 0 0 0 \n",
            "questions 2 0 2 0 0 \n",
            "makes 2 0 0 1 1 \n",
            "especially 1 0 0 0 0 \n",
            "challenging 1 1 0 0 0 \n",
            "people 1 1 0 0 2 \n",
            "talk 1 0 0 0 0 \n",
            "quickly 1 0 0 0 0 \n",
            "slurring 1 0 0 0 0 \n",
            "together 1 0 0 0 0 \n",
            "varying 1 0 0 0 0 \n",
            "emphasis 1 0 0 0 0 \n",
            "intonation 1 0 0 0 0 \n",
            "different 1 0 0 1 14 \n",
            "accents 1 0 0 0 0 \n",
            "often 2 1 0 0 2 \n",
            "using 1 3 6 3 17 \n",
            "incorrect 1 0 0 0 0 \n",
            "Part 2 0 0 0 13 \n",
            "tagging 2 0 0 0 12 \n",
            "grammatical 1 1 0 0 1 \n",
            "determining 2 0 0 0 0 \n",
            "part 1 1 1 1 3 \n",
            "particular 1 5 0 1 5 \n",
            "word 5 9 16 9 52 \n",
            "piece 1 0 0 0 0 \n",
            "on 4 17 14 8 37 \n",
            "context 3 0 4 2 7 \n",
            "identifies 3 0 0 0 0 \n",
            "as 11 13 18 4 27 \n",
            "verb 2 2 1 0 3 \n",
            "I 1 6 1 0 7 \n",
            "paper 1 0 0 1 0 \n",
            "plane 1 0 0 0 0 \n",
            "noun 1 3 2 0 11 \n",
            "car 1 0 0 0 0 \n",
            "Word 1 0 4 0 13 \n",
            "disambiguation 2 0 0 0 0 \n",
            "selection 1 0 0 0 0 \n",
            "multiple 1 1 1 0 2 \n",
            "meanings 1 5 0 1 5 \n",
            "through 1 4 2 0 1 \n",
            "semantic 3 0 9 0 6 \n",
            "analysis 5 3 11 0 16 \n",
            "determine 1 0 0 1 0 \n",
            "most 2 2 1 0 9 \n",
            "given 1 2 2 1 2 \n",
            "For 3 6 9 1 16 \n",
            "example 4 7 7 1 60 \n",
            "helps 1 0 1 1 1 \n",
            "distinguish 1 0 0 0 1 \n",
            "grade 1 2 0 0 0 \n",
            "achieve 1 0 0 0 2 \n",
            "vs 6 0 0 0 3 \n",
            "bet 1 0 0 0 0 \n",
            "place 1 1 0 0 0 \n",
            "Named 1 0 0 0 9 \n",
            "entity 4 2 1 0 9 \n",
            "NEM 2 0 0 0 0 \n",
            "phrases 2 2 1 0 7 \n",
            "entities 1 6 0 0 3 \n",
            "Kentucky 1 0 0 0 0 \n",
            "location 1 0 0 0 0 \n",
            "Fred 1 0 0 0 0 \n",
            "man 1 0 0 0 6 \n",
            "name 1 0 1 0 3 \n",
            "Co 1 0 0 0 3 \n",
            "reference 1 0 0 0 0 \n",
            "resolution 1 0 0 0 0 \n",
            "identifying 2 0 1 0 1 \n",
            "when 1 5 1 1 7 \n",
            "two 1 4 3 0 5 \n",
            "refer 1 0 0 0 1 \n",
            "The 10 12 8 6 31 \n",
            "common 1 2 1 0 7 \n",
            "person 2 0 0 0 3 \n",
            "object 1 0 0 0 0 \n",
            "which 4 7 1 6 10 \n",
            "certain 2 0 0 0 2 \n",
            "pronoun 1 0 0 0 3 \n",
            "e 2 6 3 0 8 \n",
            "g 2 1 1 0 3 \n",
            "she 1 0 0 0 0 \n",
            "Mary 1 0 0 0 0 \n",
            "involve 1 0 0 0 0 \n",
            "metaphor 1 0 0 0 0 \n",
            "an 10 14 10 4 16 \n",
            "idiom 1 0 0 0 0 \n",
            "instance 1 0 1 0 7 \n",
            "bear 1 0 0 0 0 \n",
            "not 6 6 5 4 32 \n",
            "animal 1 0 0 0 0 \n",
            "hairy 1 0 0 0 0 \n",
            "Sentiment 2 0 1 0 8 \n",
            "attempts 1 0 0 0 0 \n",
            "extract 4 2 1 0 4 \n",
            "subjective 1 0 0 0 0 \n",
            "qualities 1 0 0 0 0 \n",
            "attitudes 2 0 0 0 0 \n",
            "emotions 3 0 0 0 0 \n",
            "confusion 1 0 0 0 0 \n",
            "suspicion 1 0 0 0 0 \n",
            "generation 3 1 0 0 0 \n",
            "sometimes 1 1 0 0 1 \n",
            "described 1 0 0 0 0 \n",
            "opposite 1 1 1 0 0 \n",
            "putting 1 0 0 0 0 \n",
            "structured 1 0 0 0 1 \n",
            "information 3 3 7 5 7 \n",
            "See 1 2 0 0 0 \n",
            "blog 1 0 1 0 0 \n",
            "post 1 1 0 0 2 \n",
            "NLU 2 5 0 0 0 \n",
            "NLG 2 2 0 0 0 \n",
            "differences 1 0 0 0 0 \n",
            "between 2 1 0 0 6 \n",
            "three 1 1 1 0 0 \n",
            "concepts 4 0 0 0 0 \n",
            "deeper 2 0 0 0 1 \n",
            "look 1 2 2 0 0 \n",
            "how 2 4 7 3 11 \n",
            "relate 1 0 0 0 0 \n",
            "Python 2 10 2 0 39 \n",
            "Toolkit 2 0 0 0 2 \n",
            "NLTK 3 3 0 0 14 \n",
            "programing 1 0 0 0 0 \n",
            "provides 1 8 0 0 1 \n",
            "wide 1 2 0 0 0 \n",
            "range 1 2 0 0 0 \n",
            "libraries 5 5 0 0 5 \n",
            "attacking 1 0 0 0 0 \n",
            "specific 1 1 0 0 4 \n",
            "Many 1 0 1 0 0 \n",
            "found 1 0 0 1 2 \n",
            "open 1 8 0 0 3 \n",
            "source 1 8 0 1 2 \n",
            "collection 1 0 0 0 1 \n",
            "education 1 0 0 0 3 \n",
            "resources 1 1 0 0 0 \n",
            "building 1 0 0 0 0 \n",
            "includes 2 1 2 0 0 \n",
            "many 2 4 1 0 15 \n",
            "listed 1 0 0 0 0 \n",
            "above 1 1 0 1 12 \n",
            "plus 1 0 0 0 0 \n",
            "subtasks 1 0 0 0 0 \n",
            "such 3 1 1 0 7 \n",
            "parsing 1 2 1 0 1 \n",
            "segmentation 1 1 0 0 0 \n",
            "stemming 1 0 0 0 11 \n",
            "lemmatization 1 0 0 0 9 \n",
            "methods 2 0 1 1 1 \n",
            "trimming 1 0 0 0 0 \n",
            "roots 1 0 0 0 0 \n",
            "tokenization 1 1 0 0 0 \n",
            "breaking 1 0 0 0 0 \n",
            "sentences 1 3 0 6 15 \n",
            "paragraphs 1 0 0 0 1 \n",
            "passages 1 0 0 0 0 \n",
            "tokens 1 3 1 0 2 \n",
            "better 2 2 2 0 1 \n",
            "It 1 3 12 0 28 \n",
            "implementing 1 0 0 0 0 \n",
            "capabilities 3 2 0 0 0 \n",
            "reasoning 2 0 0 0 2 \n",
            "reach 1 0 0 0 0 \n",
            "logical 1 0 0 0 0 \n",
            "conclusions 2 0 0 0 1 \n",
            "facts 1 0 0 0 0 \n",
            "extracted 1 0 0 0 1 \n",
            "Statistical 1 0 0 0 5 \n",
            "earliest 1 0 0 0 0 \n",
            "were 1 4 0 0 1 \n",
            "hand 1 0 1 0 0 \n",
            "coded 1 0 0 0 0 \n",
            "rules 1 0 2 0 3 \n",
            "could 2 2 0 1 0 \n",
            "perform 4 2 1 0 6 \n",
            "easily 1 0 0 2 1 \n",
            "scale 1 1 0 1 0 \n",
            "accommodate 1 0 0 0 0 \n",
            "seemingly 1 0 0 0 0 \n",
            "endless 1 0 0 0 0 \n",
            "stream 1 1 0 0 0 \n",
            "increasing 1 0 0 0 0 \n",
            "Enter 1 0 1 0 0 \n",
            "algorithms 1 1 1 0 3 \n",
            "automatically 1 1 2 0 1 \n",
            "classify 1 0 1 0 0 \n",
            "label 1 0 1 0 0 \n",
            "elements 2 1 0 0 0 \n",
            "then 2 6 1 2 10 \n",
            "assign 1 0 1 0 1 \n",
            "likelihood 1 0 0 0 0 \n",
            "each 1 6 1 1 0 \n",
            "possible 1 2 1 0 1 \n",
            "Today 2 0 0 0 0 \n",
            "techniques 2 1 1 0 0 \n",
            "convolutional 1 0 0 0 0 \n",
            "neural 2 0 0 7 1 \n",
            "networks 2 0 0 3 1 \n",
            "CNNs 1 0 0 0 0 \n",
            "recurrent 1 0 0 2 0 \n",
            "RNNs 1 0 0 0 0 \n",
            "they 1 2 3 2 2 \n",
            "work 2 11 1 1 2 \n",
            "ever 1 0 0 0 0 \n",
            "accurate 1 0 0 0 1 \n",
            "huge 2 0 0 0 0 \n",
            "raw 1 1 0 0 2 \n",
            "unstructured 3 2 1 0 4 \n",
            "unlabeled 1 0 0 0 0 \n",
            "sets 1 0 1 0 0 \n",
            "dive 1 1 0 0 1 \n",
            "nuances 1 0 0 0 0 \n",
            "see 1 3 0 0 9 \n",
            "Machine 3 1 7 6 18 \n",
            "Learning 2 4 6 4 18 \n",
            "Deep 2 1 3 1 5 \n",
            "Neural 1 0 0 7 1 \n",
            "Networks 1 0 0 1 1 \n",
            "Difference 1 0 0 0 1 \n",
            "driving 1 0 0 0 3 \n",
            "force 1 0 0 0 0 \n",
            "behind 1 0 1 0 2 \n",
            "modern 1 0 0 0 0 \n",
            "world 1 2 0 0 4 \n",
            "Here 1 2 2 0 1 \n",
            "examples 1 0 3 0 6 \n",
            "Spam 2 0 0 0 1 \n",
            "detection 4 2 4 0 1 \n",
            "You 1 2 4 0 1 \n",
            "may 2 4 2 1 4 \n",
            "think 1 0 0 0 0 \n",
            "spam 3 0 0 0 0 \n",
            "solution 1 0 1 0 0 \n",
            "best 3 1 2 2 1 \n",
            "classification 2 0 11 0 1 \n",
            "scan 1 0 0 0 1 \n",
            "emails 1 0 0 0 0 \n",
            "indicates 1 0 0 0 0 \n",
            "phishing 1 0 0 0 0 \n",
            "These 2 0 2 0 9 \n",
            "indicators 1 0 0 0 0 \n",
            "overuse 1 0 0 0 0 \n",
            "financial 1 1 0 0 0 \n",
            "terms 2 1 0 0 1 \n",
            "characteristic 1 0 0 0 0 \n",
            "bad 1 1 0 0 0 \n",
            "threatening 1 0 0 0 0 \n",
            "inappropriate 1 0 0 0 0 \n",
            "urgency 1 0 0 0 0 \n",
            "misspelled 1 0 0 0 0 \n",
            "company 1 1 0 0 1 \n",
            "names 1 0 0 0 0 \n",
            "handful 1 0 0 0 0 \n",
            "problems 1 1 0 0 0 \n",
            "experts 1 0 0 0 1 \n",
            "consider 1 1 0 0 3 \n",
            "mostly 1 0 0 0 0 \n",
            "solved 1 0 0 0 0 \n",
            "although 1 0 0 0 0 \n",
            "argue 1 0 0 0 0 \n",
            "this 2 15 8 7 22 \n",
            "does 1 1 2 0 5 \n",
            "match 1 0 1 0 2 \n",
            "your 5 6 10 0 7 \n",
            "email 1 0 3 0 3 \n",
            "experience 2 0 3 0 4 \n",
            "translation 6 1 1 6 1 \n",
            "Google 1 2 4 7 9 \n",
            "Translate 1 0 0 7 0 \n",
            "widely 1 1 1 0 0 \n",
            "available 1 0 2 0 2 \n",
            "technology 1 0 0 0 4 \n",
            "at 1 4 3 5 9 \n",
            "Truly 1 0 0 0 0 \n",
            "involves 1 0 0 0 3 \n",
            "than 1 1 0 2 5 \n",
            "replacing 1 0 0 0 0 \n",
            "Effective 1 0 0 0 0 \n",
            "has 3 6 5 0 10 \n",
            "capture 1 0 0 0 0 \n",
            "tone 1 0 0 0 0 \n",
            "input 1 0 5 2 4 \n",
            "desired 1 1 0 0 0 \n",
            "impact 1 0 0 0 0 \n",
            "output 1 0 1 2 5 \n",
            "making 1 0 0 0 2 \n",
            "progress 1 0 0 0 0 \n",
            "accuracy 1 1 0 1 2 \n",
            "A 1 0 0 0 10 \n",
            "great 1 1 0 0 2 \n",
            "test 1 0 3 0 0 \n",
            "tool 3 0 0 0 2 \n",
            "back 2 1 0 0 0 \n",
            "original 1 0 0 1 3 \n",
            "An 1 1 0 0 1 \n",
            "oft 1 0 0 0 0 \n",
            "cited 1 0 0 0 0 \n",
            "classic 1 0 0 0 0 \n",
            "Not 1 1 0 0 1 \n",
            "long 1 2 1 0 0 \n",
            "ago 1 2 1 0 0 \n",
            "translating 1 0 0 1 0 \n",
            "spirit 2 0 0 0 0 \n",
            "willing 1 0 0 0 0 \n",
            "flesh 2 0 0 0 0 \n",
            "weak 2 0 0 0 0 \n",
            "English 2 3 0 7 2 \n",
            "Russian 2 0 0 0 0 \n",
            "yielded 1 0 0 0 0 \n",
            "vodka 1 0 0 0 0 \n",
            "meat 1 0 0 0 0 \n",
            "rotten 1 0 0 0 0 \n",
            "result 1 0 0 0 3 \n",
            "desires 1 0 0 0 0 \n",
            "isn 1 0 0 0 0 \n",
            "t 1 6 1 0 6 \n",
            "perfect 1 0 0 0 2 \n",
            "inspires 1 0 0 0 0 \n",
            "confidence 1 0 0 0 0 \n",
            "Virtual 2 0 0 0 0 \n",
            "agents 3 0 0 0 0 \n",
            "Apple 1 1 1 0 0 \n",
            "Siri 1 0 0 1 0 \n",
            "Amazon 1 0 1 0 1 \n",
            "Alexa 1 0 1 0 0 \n",
            "patterns 2 0 0 0 0 \n",
            "appropriate 1 0 0 1 2 \n",
            "action 1 0 2 0 0 \n",
            "helpful 2 0 1 0 0 \n",
            "comments 1 0 1 0 1 \n",
            "Chatbots 1 0 1 0 0 \n",
            "magic 1 0 0 0 0 \n",
            "response 2 0 0 0 2 \n",
            "typed 1 0 0 0 0 \n",
            "entries 1 0 0 0 0 \n",
            "contextual 2 0 0 0 0 \n",
            "clues 1 0 0 0 0 \n",
            "about 1 8 1 0 5 \n",
            "requests 1 2 0 0 0 \n",
            "them 1 2 1 1 2 \n",
            "provide 2 2 0 1 3 \n",
            "responses 2 0 0 0 0 \n",
            "options 1 0 0 0 1 \n",
            "over 1 1 0 0 0 \n",
            "next 1 0 0 1 1 \n",
            "enhancement 1 0 0 0 0 \n",
            "question 1 0 0 0 2 \n",
            "answering 1 0 0 0 0 \n",
            "our 1 0 2 1 30 \n",
            "anticipated 1 0 0 0 0 \n",
            "relevant 1 0 1 0 4 \n",
            "Social 1 0 1 0 3 \n",
            "media 4 0 1 0 2 \n",
            "become 1 2 0 0 0 \n",
            "essential 2 0 2 0 8 \n",
            "uncovering 1 0 0 0 0 \n",
            "hidden 1 0 0 0 0 \n",
            "insights 4 0 0 0 0 \n",
            "social 3 0 0 0 1 \n",
            "channels 1 0 0 0 1 \n",
            "analyze 2 1 6 0 5 \n",
            "used 1 5 7 3 26 \n",
            "posts 1 1 1 1 2 \n",
            "reviews 1 0 0 0 2 \n",
            "products 2 0 0 0 1 \n",
            "promotions 1 0 0 0 0 \n",
            "events 1 0 0 0 0 \n",
            "companies 1 0 0 0 2 \n",
            "product 1 3 0 0 0 \n",
            "designs 1 0 0 0 0 \n",
            "advertising 1 0 0 0 0 \n",
            "campaigns 1 0 0 0 1 \n",
            "Text 2 2 5 0 10 \n",
            "summarization 4 1 0 0 2 \n",
            "uses 1 1 3 2 4 \n",
            "digest 1 0 0 0 0 \n",
            "create 2 0 3 0 1 \n",
            "summaries 3 1 0 0 0 \n",
            "synopses 1 0 0 0 0 \n",
            "indexes 1 1 0 0 1 \n",
            "research 1 3 0 1 2 \n",
            "databases 1 0 0 0 0 \n",
            "busy 1 0 0 0 0 \n",
            "readers 1 0 0 0 0 \n",
            "who 1 0 0 0 1 \n",
            "have 1 8 4 1 16 \n",
            "read 1 6 2 0 3 \n",
            "add 1 2 0 0 0 \n",
            "innovated 1 0 0 0 0 \n",
            "space 1 0 2 0 1 \n",
            "by 2 8 13 6 24 \n",
            "pioneering 1 0 0 0 0 \n",
            "services 1 0 1 0 0 \n",
            "organizations 1 0 0 0 2 \n",
            "automate 1 0 0 0 0 \n",
            "complex 2 2 0 1 1 \n",
            "while 2 0 1 0 2 \n",
            "gaining 1 0 0 0 0 \n",
            "Discovery 3 0 0 0 0 \n",
            "Surface 1 0 0 0 0 \n",
            "high 1 0 0 0 0 \n",
            "quality 1 0 0 0 0 \n",
            "rich 1 0 1 0 0 \n",
            "documents 1 5 1 0 7 \n",
            "tables 1 0 0 0 1 \n",
            "PDFs 1 0 0 0 0 \n",
            "big 1 2 0 0 1 \n",
            "search 2 3 2 0 7 \n",
            "Enable 1 0 0 0 0 \n",
            "employees 1 0 0 0 0 \n",
            "informed 1 0 0 0 0 \n",
            "decisions 1 0 0 0 0 \n",
            "save 1 0 0 0 0 \n",
            "engine 1 1 0 0 6 \n",
            "mining 1 0 1 0 0 \n",
            "extraction 2 0 1 0 2 \n",
            "relationships 1 0 1 0 0 \n",
            "buried 1 0 0 0 0 \n",
            "leverages 1 0 0 0 0 \n",
            "custom 1 0 0 0 0 \n",
            "users 1 0 3 0 0 \n",
            "understands 1 0 1 0 0 \n",
            "unique 1 0 0 0 2 \n",
            "industry 1 6 0 0 0 \n",
            "Explore 3 1 0 0 0 \n",
            "Understanding 2 1 2 0 3 \n",
            "Analyze 1 0 0 0 0 \n",
            "formats 1 0 0 0 0 \n",
            "including 1 0 0 0 0 \n",
            "HTML 1 4 0 0 0 \n",
            "webpages 1 0 0 0 0 \n",
            "Increase 1 0 0 0 0 \n",
            "understanding 1 2 1 0 0 \n",
            "leveraging 1 0 0 0 0 \n",
            "kit 1 0 0 0 0 \n",
            "identify 1 2 2 0 0 \n",
            "keywords 1 0 0 0 0 \n",
            "categories 1 1 1 0 1 \n",
            "semantics 1 0 0 0 0 \n",
            "named 1 4 0 0 8 \n",
            "NER 1 0 0 0 6 \n",
            "Assistant 4 0 0 0 0 \n",
            "Improve 1 0 0 0 0 \n",
            "reducing 1 0 0 0 0 \n",
            "costs 1 0 0 0 0 \n",
            "chatbot 1 1 3 0 0 \n",
            "easy 1 0 1 0 0 \n",
            "visual 1 0 0 0 0 \n",
            "builder 1 0 0 0 1 \n",
            "so 1 3 2 0 5 \n",
            "deploy 1 0 0 0 1 \n",
            "virtual 1 0 0 0 0 \n",
            "across 1 1 0 0 1 \n",
            "channel 1 0 0 0 0 \n",
            "minutes 1 1 0 0 0 \n",
            "Purpose 1 0 0 0 0 \n",
            "built 1 0 0 0 0 \n",
            "healthcare 1 0 1 0 0 \n",
            "life 1 0 0 0 0 \n",
            "sciences 1 0 0 0 0 \n",
            "domains 1 4 0 0 0 \n",
            "Annotator 1 0 0 0 0 \n",
            "Clinical 1 0 0 0 0 \n",
            "Data 1 28 11 1 25 \n",
            "extracts 1 0 0 0 0 \n",
            "key 2 0 0 0 1 \n",
            "clinical 4 0 0 0 0 \n",
            "like 1 0 7 0 9 \n",
            "conditions 1 0 0 0 0 \n",
            "medications 1 0 0 0 0 \n",
            "allergies 1 0 0 0 0 \n",
            "procedures 1 0 0 0 0 \n",
            "values 1 0 0 0 16 \n",
            "attributes 1 0 0 0 0 \n",
            "develop 1 1 0 0 0 \n",
            "meaningful 1 0 0 0 5 \n",
            "Potential 1 0 0 0 0 \n",
            "sources 1 1 0 1 0 \n",
            "notes 1 0 0 0 0 \n",
            "discharge 1 0 0 0 0 \n",
            "trial 1 0 0 0 0 \n",
            "protocols 1 0 0 0 0 \n",
            "literature 1 0 0 0 0 \n",
            "get 1 5 1 0 3 \n",
            "started 1 0 0 0 0 \n",
            "visit 1 0 0 0 0 \n",
            "page 1 0 0 2 0 \n",
            "Sign 1 0 5 0 0 \n",
            "up 1 1 3 0 4 \n",
            "IBMid 1 0 0 0 0 \n",
            "account 1 0 0 0 2 \n",
            "Featured 1 2 0 0 1 \n",
            "Platform 0 10 0 0 0 \n",
            "Domino 0 8 0 0 0 \n",
            "Enterprise 0 6 0 0 0 \n",
            "MLOps 0 3 0 0 1 \n",
            "Components 0 1 0 0 3 \n",
            "System 0 3 3 0 0 \n",
            "Record 0 3 0 0 0 \n",
            "Integrated 0 3 0 0 0 \n",
            "Model 0 5 5 0 0 \n",
            "Factory 0 3 0 0 0 \n",
            "Self 0 5 0 0 3 \n",
            "Service 0 5 0 0 1 \n",
            "Infrastructure 0 5 0 0 0 \n",
            "Portal 0 3 0 0 0 \n",
            "Pricing 0 3 0 0 0 \n",
            "Nexus 0 2 0 0 0 \n",
            "Updates 0 2 0 0 1 \n",
            "Solutions 0 6 0 0 0 \n",
            "Role 0 1 0 0 0 \n",
            "Chief 0 2 0 0 0 \n",
            "Analytics 0 2 6 0 7 \n",
            "Executives 0 2 0 0 0 \n",
            "Science 0 19 5 3 16 \n",
            "Leaders 0 5 0 0 0 \n",
            "Scientists 0 3 1 0 0 \n",
            "IT 0 3 0 0 0 \n",
            "Industry 0 1 0 0 0 \n",
            "Financial 0 2 0 0 0 \n",
            "Services 0 2 0 0 0 \n",
            "Health 0 2 0 0 0 \n",
            "Life 0 2 0 0 0 \n",
            "Sciences 0 2 0 0 0 \n",
            "Insurance 0 2 0 0 0 \n",
            "More 0 4 1 0 0 \n",
            "Use 0 1 2 0 7 \n",
            "Cases 0 1 0 0 1 \n",
            "Open 0 4 0 0 1 \n",
            "Risk 0 2 0 0 0 \n",
            "Management 0 2 0 0 9 \n",
            "Customers 0 3 0 0 0 \n",
            "Resources 0 3 1 0 5 \n",
            "Guides 0 6 1 0 0 \n",
            "Videos 0 2 1 0 0 \n",
            "Blog 0 3 1 1 1 \n",
            "Events 0 3 0 0 1 \n",
            "Podcast 0 3 0 0 0 \n",
            "Community 0 2 0 1 5 \n",
            "Documentation 0 3 0 0 0 \n",
            "Partners 0 5 0 0 0 \n",
            "Tools 0 2 0 0 0 \n",
            "Implementation 0 2 0 0 11 \n",
            "Consulting 0 2 0 0 0 \n",
            "Become 0 2 0 0 0 \n",
            "Partner 0 2 0 0 0 \n",
            "Azure 0 1 0 0 0 \n",
            "Accenture 0 1 0 0 0 \n",
            "Snowflake 0 1 0 0 0 \n",
            "Company 0 3 0 0 2 \n",
            "About 0 3 2 1 5 \n",
            "Careers 0 3 1 0 4 \n",
            "We 0 10 7 0 16 \n",
            "re 0 6 0 0 0 \n",
            "Hiring 0 3 1 0 0 \n",
            "News 0 3 0 0 7 \n",
            "Press 0 3 0 1 1 \n",
            "Contact 0 3 1 0 2 \n",
            "Watch 0 2 0 0 0 \n",
            "Demo 0 2 0 0 0 \n",
            "Try 0 2 0 0 0 \n",
            "Now 0 8 2 1 3 \n",
            "Executive 0 1 0 0 0 \n",
            "Media 0 1 1 0 0 \n",
            "Technology 0 1 0 0 5 \n",
            "Manufacturing 0 1 0 0 0 \n",
            "Retail 0 1 0 0 0 \n",
            "eCommerce 0 1 0 0 0 \n",
            "Consumer 0 1 0 0 0 \n",
            "Products 0 1 0 0 0 \n",
            "Field 0 2 0 0 0 \n",
            "Comparison 0 3 0 0 1 \n",
            "spaCy 0 24 0 0 4 \n",
            "Introduction 0 2 1 0 0 \n",
            "Paco 0 2 0 0 0 \n",
            "Nathan 0 2 0 0 0 \n",
            "September 0 1 0 0 0 \n",
            "10 0 1 2 5 2 \n",
            "2019 0 3 1 2 2 \n",
            "17 0 1 1 0 3 \n",
            "min 0 4 0 0 0 \n",
            "This 0 6 6 7 7 \n",
            "article 0 2 4 0 4 \n",
            "brief 0 2 0 0 0 \n",
            "introduction 0 3 1 0 0 \n",
            "related 0 6 0 0 1 \n",
            "teams 0 3 0 0 0 \n",
            "lots 0 1 0 0 0 \n",
            "top 0 2 0 0 1 \n",
            "four 0 1 0 0 2 \n",
            "Usually 0 1 0 0 0 \n",
            "generated 0 1 0 2 0 \n",
            "always 0 1 0 1 0 \n",
            "Think 0 1 0 0 0 \n",
            "operating 0 1 0 0 0 \n",
            "system 0 1 8 0 1 \n",
            "Typically 0 1 0 0 0 \n",
            "there 0 5 0 0 6 \n",
            "contracts 0 3 0 0 0 \n",
            "sales 0 1 0 0 0 \n",
            "agreements 0 1 0 0 0 \n",
            "partnerships 0 1 0 0 0 \n",
            "invoices 0 1 0 0 0 \n",
            "insurance 0 2 0 0 0 \n",
            "policies 0 1 0 0 0 \n",
            "regulations 0 1 0 0 0 \n",
            "laws 0 1 0 0 0 \n",
            "All 0 1 2 1 4 \n",
            "represented 0 1 0 0 2 \n",
            "run 0 6 3 0 0 \n",
            "acronyms 0 1 0 0 0 \n",
            "roughly 0 1 0 0 0 \n",
            "speaking 0 1 1 0 1 \n",
            "respectively 0 1 0 0 0 \n",
            "Increasingly 0 1 0 0 0 \n",
            "overlap 0 1 0 0 0 \n",
            "becomes 0 1 0 0 0 \n",
            "categorize 0 1 1 0 0 \n",
            "feature 0 1 1 0 0 \n",
            "Spacy 0 1 0 0 0 \n",
            "How 0 2 0 1 2 \n",
            "go 0 3 0 0 0 \n",
            "Oftentimes 0 1 0 0 0 \n",
            "turn 0 1 0 1 0 \n",
            "various 0 1 0 0 4 \n",
            "manage 0 1 0 0 0 \n",
            "sPacy 0 1 0 0 0 \n",
            "library 0 4 1 0 7 \n",
            "conduct 0 1 0 0 0 \n",
            "advanced 0 1 0 0 0 \n",
            "underpin 0 1 0 0 0 \n",
            "document 0 16 0 0 6 \n",
            "all 0 10 1 0 14 \n",
            "forms 0 1 1 0 3 \n",
            "framework 0 1 0 0 11 \n",
            "along 0 2 0 0 6 \n",
            "plug 0 1 0 0 0 \n",
            "ins 0 1 0 0 0 \n",
            "integrations 0 3 0 0 0 \n",
            "features 0 3 2 0 3 \n",
            "quite 0 3 0 0 0 \n",
            "community 0 2 0 0 1 \n",
            "support 0 3 0 0 1 \n",
            "commercialization 0 1 0 0 0 \n",
            "advances 0 2 0 0 0 \n",
            "area 0 1 0 0 0 \n",
            "continues 0 1 0 0 0 \n",
            "evolve 0 1 0 0 0 \n",
            "working 0 4 2 0 3 \n",
            "analytics 0 1 1 0 0 \n",
            "Getting 0 2 1 0 0 \n",
            "Started 0 1 0 0 0 \n",
            "configured 0 1 0 0 0 \n",
            "default 0 3 0 0 3 \n",
            "Compute 0 2 0 0 0 \n",
            "Environment 0 1 0 0 0 \n",
            "packages 0 1 1 0 0 \n",
            "ll 0 7 2 0 0 \n",
            "need 0 4 0 1 11 \n",
            "tutorial 0 4 0 0 3 \n",
            "Check 0 1 0 0 3 \n",
            "out 0 8 2 1 9 \n",
            "project 0 7 4 0 2 \n",
            "code 0 2 2 0 12 \n",
            "If 0 2 3 0 4 \n",
            "interested 0 2 0 0 0 \n",
            "Environments 0 1 0 0 0 \n",
            "check 0 3 1 0 4 \n",
            "Support 0 1 0 0 1 \n",
            "Page 0 1 0 1 0 \n",
            "let 0 7 0 0 2 \n",
            "us 0 10 4 0 11 \n",
            "load 0 4 2 0 0 \n",
            "some 0 7 2 0 10 \n",
            "import 0 12 20 0 2 \n",
            "spacy 0 7 0 0 0 \n",
            "nlp 0 23 0 0 1 \n",
            "en_core_web_sm 0 2 0 0 0 \n",
            "That 0 6 0 0 1 \n",
            "variable 0 1 0 0 0 \n",
            "now 0 2 0 0 1 \n",
            "gateway 0 1 0 0 0 \n",
            "things 0 2 1 0 0 \n",
            "loaded 0 1 0 0 0 \n",
            "small 0 2 0 0 0 \n",
            "model 0 1 8 2 4 \n",
            "Next 0 2 1 0 8 \n",
            "parser 0 4 1 0 0 \n",
            "rain 0 2 0 0 0 \n",
            "Spain 0 2 0 0 0 \n",
            "falls 0 1 0 0 0 \n",
            "mainly 0 2 1 0 0 \n",
            "plain 0 2 0 0 0 \n",
            "doc 0 14 0 0 0 \n",
            "token 0 19 3 0 1 \n",
            "print 0 13 1 0 0 \n",
            "lemma_ 0 3 0 0 0 \n",
            "pos_ 0 4 0 0 0 \n",
            "is_stop 0 2 0 0 0 \n",
            "DET 0 2 0 0 0 \n",
            "Truerain 0 1 0 0 0 \n",
            "NOUN 0 2 0 0 0 \n",
            "Falsein 0 1 0 0 0 \n",
            "ADP 0 2 0 0 0 \n",
            "TrueSpain 0 1 0 0 0 \n",
            "PROPN 0 1 0 0 0 \n",
            "Falsefalls 0 1 0 0 0 \n",
            "fall 0 1 0 0 0 \n",
            "VERB 0 2 0 0 0 \n",
            "Falsemainly 0 1 0 0 0 \n",
            "ADV 0 1 0 0 0 \n",
            "Falseon 0 1 0 0 0 \n",
            "Truethe 0 1 0 0 0 \n",
            "Trueplain 0 1 0 0 0 \n",
            "False 0 2 0 0 3 \n",
            "PUNCT 0 1 0 0 0 \n",
            "First 0 2 0 0 2 \n",
            "we 0 21 5 1 72 \n",
            "created 0 2 0 0 1 \n",
            "container 0 1 0 0 2 \n",
            "annotations 0 2 0 0 0 \n",
            "Then 0 3 2 0 3 \n",
            "iterated 0 1 0 0 0 \n",
            "had 0 4 0 0 0 \n",
            "parsed 0 2 0 0 0 \n",
            "Good 0 1 0 0 3 \n",
            "lot 0 1 0 0 1 \n",
            "info 0 1 0 0 0 \n",
            "bit 0 1 0 0 0 \n",
            "Let 0 2 0 0 5 \n",
            "reformat 0 1 0 0 0 \n",
            "parse 0 7 0 1 1 \n",
            "pandas 0 2 1 0 0 \n",
            "dataframe 0 1 0 0 0 \n",
            "pd 0 2 2 0 0 \n",
            "cols 0 2 0 0 0 \n",
            "lemma 0 3 0 0 0 \n",
            "POS 0 1 0 0 9 \n",
            "explain 0 2 1 0 0 \n",
            "stopword 0 2 0 0 0 \n",
            "rows 0 3 0 0 0 \n",
            "row 0 2 0 0 1 \n",
            "append 0 4 0 0 0 \n",
            "df 0 2 0 0 0 \n",
            "DataFrame 0 1 0 0 0 \n",
            "columns 0 1 0 0 0 \n",
            "Much 0 1 0 0 1 \n",
            "readable 0 1 0 0 0 \n",
            "In 0 7 6 4 27 \n",
            "simple 0 4 0 2 3 \n",
            "case 0 2 0 1 8 \n",
            "entire 0 1 0 0 3 \n",
            "merely 0 1 0 0 0 \n",
            "short 0 2 0 0 0 \n",
            "accessed 0 1 1 0 0 \n",
            "fields 0 1 1 0 0 \n",
            "show 0 2 0 1 4 \n",
            "root 0 1 0 0 0 \n",
            "flag 0 1 0 0 0 \n",
            "whether 0 1 2 0 3 \n",
            "i 0 5 3 0 4 \n",
            "filtered 0 1 0 0 0 \n",
            "displaCy 0 2 0 0 0 \n",
            "visualize 0 3 0 0 3 \n",
            "tree 0 1 0 0 0 \n",
            "displacy 0 3 0 0 0 \n",
            "render 0 3 0 0 0 \n",
            "style 0 2 0 0 0 \n",
            "dep 0 1 0 0 0 \n",
            "Does 0 1 0 0 1 \n",
            "bring 0 1 0 0 0 \n",
            "memories 0 1 0 0 0 \n",
            "school 0 3 0 0 0 \n",
            "Frankly 0 1 0 0 0 \n",
            "coming 0 1 0 0 0 \n",
            "background 0 1 0 0 1 \n",
            "diagram 0 1 0 0 0 \n",
            "sparks 0 1 0 0 0 \n",
            "joy 0 1 0 0 0 \n",
            "backup 0 1 0 0 0 \n",
            "moment 0 1 0 0 0 \n",
            "handle 0 1 0 0 1 \n",
            "boundary 0 1 0 0 0 \n",
            "SBD 0 1 0 0 0 \n",
            "known 0 1 0 1 1 \n",
            "builtin 0 1 0 0 0 \n",
            "sentencizer 0 1 0 0 0 \n",
            "zoo 0 2 0 0 0 \n",
            "day 0 2 1 0 0 \n",
            "was 0 5 3 1 6 \n",
            "doing 0 2 1 0 0 \n",
            "acting 0 2 0 0 0 \n",
            "walking 0 2 0 0 0 \n",
            "railing 0 2 0 0 0 \n",
            "gorilla 0 2 0 0 0 \n",
            "exhibit 0 2 0 0 0 \n",
            "fell 0 2 0 0 0 \n",
            "Everyone 0 2 0 0 0 \n",
            "screamed 0 2 0 0 0 \n",
            "Tommy 0 2 0 0 0 \n",
            "jumped 0 2 0 0 0 \n",
            "after 0 6 0 3 4 \n",
            "me 0 2 2 0 2 \n",
            "forgetting 0 2 0 0 0 \n",
            "he 0 2 0 0 3 \n",
            "blueberries 0 2 0 0 0 \n",
            "his 0 2 0 0 1 \n",
            "front 0 2 0 0 1 \n",
            "pocket 0 2 0 0 0 \n",
            "gorillas 0 4 0 0 0 \n",
            "went 0 7 0 0 0 \n",
            "wild 0 4 0 0 0 \n",
            "sent 0 7 0 0 0 \n",
            "sents 0 3 0 0 0 \n",
            "When 0 1 1 0 3 \n",
            "creates 0 1 0 0 0 \n",
            "principle 0 1 1 0 0 \n",
            "non 0 1 1 0 4 \n",
            "destructive 0 1 0 0 0 \n",
            "etc 0 3 2 0 1 \n",
            "simply 0 2 0 0 0 \n",
            "array 0 3 0 0 0 \n",
            "carve 0 1 0 0 0 \n",
            "little 0 2 0 0 0 \n",
            "pieces 0 1 0 1 1 \n",
            "So 0 1 0 1 3 \n",
            "span 0 1 2 0 0 \n",
            "end 0 2 1 0 7 \n",
            "index 0 3 1 0 0 \n",
            "0 0 5 1 3 0 \n",
            "25 0 2 1 0 2 \n",
            "29 0 2 0 0 5 \n",
            "48 0 3 0 0 1 \n",
            "54 0 2 0 0 1 \n",
            "pull 0 2 0 0 1 \n",
            "Or 0 1 0 0 0 \n",
            "last 0 1 0 0 0 \n",
            "51 0 1 0 0 1 \n",
            "At 0 2 0 0 1 \n",
            "point 0 2 0 0 0 \n",
            "segment 0 1 0 0 0 \n",
            "Acquiring 0 1 0 0 0 \n",
            "texts 0 7 3 0 0 \n",
            "where 0 3 2 0 2 \n",
            "One 0 2 0 0 0 \n",
            "quick 0 2 0 0 0 \n",
            "leverage 0 1 0 0 0 \n",
            "interwebs 0 1 0 0 0 \n",
            "Of 0 1 0 0 3 \n",
            "course 0 1 0 0 0 \n",
            "download 0 3 0 0 0 \n",
            "web 0 1 1 0 0 \n",
            "pages 0 1 1 1 0 \n",
            "Beautiful 0 2 0 0 0 \n",
            "Soup 0 2 0 0 0 \n",
            "popular 0 2 1 0 0 \n",
            "package 0 2 0 0 0 \n",
            "housekeeping 0 1 0 0 0 \n",
            "sys 0 2 0 0 0 \n",
            "warnings 0 2 0 0 0 \n",
            "filterwarnings 0 1 0 0 0 \n",
            "ignore 0 1 0 0 0 \n",
            "function 0 1 2 0 2 \n",
            "get_text 0 6 0 0 0 \n",
            "find 0 3 1 0 7 \n",
            "p 0 4 0 0 0 \n",
            "tags 0 1 2 0 2 \n",
            "bs4 0 1 0 0 0 \n",
            "BeautifulSoup 0 2 0 0 0 \n",
            "traceback 0 2 0 0 0 \n",
            "def 0 1 0 0 0 \n",
            "url 0 2 0 0 1 \n",
            "buf 0 3 0 0 0 \n",
            "try 0 1 0 0 1 \n",
            "soup 0 2 0 0 0 \n",
            "html 0 4 0 0 1 \n",
            "find_all 0 1 0 0 0 \n",
            "return 0 1 0 0 0 \n",
            "join 0 3 1 0 0 \n",
            "except 0 1 0 0 0 \n",
            "format_exc 0 1 0 0 0 \n",
            "exit 0 1 0 0 0 \n",
            "1 0 1 1 1 15 \n",
            "grab 0 1 0 0 0 \n",
            "online 0 2 0 0 0 \n",
            "compare 0 3 0 0 0 \n",
            "licenses 0 7 0 0 0 \n",
            "hosted 0 1 0 0 0 \n",
            "Source 0 2 8 0 1 \n",
            "Initiative 0 1 0 0 0 \n",
            "site 0 1 3 0 0 \n",
            "lic 0 7 0 0 0 \n",
            "mit 0 5 0 0 0 \n",
            "https 0 3 1 8 15 \n",
            "opensource 0 3 0 0 0 \n",
            "org 0 3 0 0 1 \n",
            "MIT 0 2 0 1 0 \n",
            "asl 0 4 0 0 0 \n",
            "Apache 0 1 0 0 0 \n",
            "20 0 1 1 1 2 \n",
            "bsd 0 5 0 0 0 \n",
            "BSD 0 6 0 0 0 \n",
            "3 0 4 1 5 4 \n",
            "Clause 0 2 0 0 0 \n",
            "SPDX 0 1 0 0 0 \n",
            "identifier 0 1 0 0 0 \n",
            "Note 0 3 1 0 0 \n",
            "license 0 1 0 1 0 \n",
            "been 0 4 3 0 1 \n",
            "New 0 1 0 0 1 \n",
            "License 0 3 0 2 0 \n",
            "Modified 0 1 1 0 0 \n",
            "clause 0 1 0 0 0 \n",
            "similarity 0 2 0 0 5 \n",
            "metrics 0 1 2 0 1 \n",
            "among 0 1 0 0 1 \n",
            "pairs 0 2 0 0 0 \n",
            "b 0 3 0 0 12 \n",
            "9482039305669306asl 0 1 0 0 0 \n",
            "9391555350757145bsd 0 1 0 0 0 \n",
            "9895838089575453 0 1 0 0 0 \n",
            "interesting 0 3 0 0 0 \n",
            "since 0 1 0 0 1 \n",
            "appear 0 1 0 0 2 \n",
            "similar 0 2 5 0 1 \n",
            "fact 0 1 0 0 0 \n",
            "closely 0 1 0 0 0 \n",
            "Admittedly 0 1 0 0 0 \n",
            "extra 0 1 1 0 0 \n",
            "included 0 1 0 0 0 \n",
            "due 0 1 0 1 1 \n",
            "OSI 0 1 0 0 0 \n",
            "disclaimer 0 1 0 0 0 \n",
            "footer 0 1 0 0 0 \n",
            "reasonable 0 2 0 0 0 \n",
            "approximation 0 1 0 0 0 \n",
            "comparing 0 1 0 0 0 \n",
            "Given 0 1 0 0 0 \n",
            "purely 0 1 0 0 0 \n",
            "standpoint 0 1 0 0 0 \n",
            "chunks 0 1 0 0 1 \n",
            "Steve 0 5 0 0 0 \n",
            "Jobs 0 2 5 0 1 \n",
            "Wozniak 0 2 0 0 0 \n",
            "incorporated 0 1 0 0 0 \n",
            "Computer 0 2 1 3 6 \n",
            "January 0 1 0 0 0 \n",
            "1977 0 2 0 0 0 \n",
            "Cupertino 0 1 0 0 0 \n",
            "California 0 1 0 0 0 \n",
            "chunk 0 2 0 0 4 \n",
            "noun_chunks 0 1 0 0 0 \n",
            "JobsSteve 0 1 0 0 0 \n",
            "WozniakApple 0 1 0 0 0 \n",
            "ComputerJanuaryCupertinoCalifornia 0 1 0 0 0 \n",
            "generally 0 1 0 0 6 \n",
            "filter 0 1 0 0 0 \n",
            "reduce 0 1 0 0 0 \n",
            "distilled 0 1 0 0 0 \n",
            "representation 0 1 2 0 2 \n",
            "approach 0 2 7 0 2 \n",
            "further 0 1 0 0 1 \n",
            "within 0 3 0 1 0 \n",
            "proper 0 1 1 0 0 \n",
            "nouns 0 1 0 0 1 \n",
            "ent 0 4 0 0 0 \n",
            "ents 0 1 0 0 0 \n",
            "label_ 0 1 0 0 0 \n",
            "PERSONSteve 0 1 0 0 0 \n",
            "PERSONApple 0 1 0 0 0 \n",
            "ORGJanuary 0 1 0 0 0 \n",
            "DATECupertino 0 1 0 0 0 \n",
            "GPECalifornia 0 1 0 0 0 \n",
            "GPE 0 1 0 0 1 \n",
            "excellent 0 2 0 0 0 \n",
            "knowledge 0 5 0 0 0 \n",
            "graph 0 4 0 0 10 \n",
            "linked 0 1 0 0 0 \n",
            "challenge 0 1 0 0 0 \n",
            "construct 0 1 0 1 0 \n",
            "links 0 2 0 0 0 \n",
            "linking 0 1 0 0 0 \n",
            "Identifying 0 1 0 0 0 \n",
            "first 0 2 0 2 8 \n",
            "step 0 1 0 1 2 \n",
            "kind 0 2 0 0 1 \n",
            "might 0 4 0 0 2 \n",
            "link 0 2 0 3 0 \n",
            "Wozniaknamed 0 1 0 0 0 \n",
            "lookup 0 2 0 0 0 \n",
            "DBpedia 0 1 0 0 0 \n",
            "general 0 1 0 0 1 \n",
            "lemmas 0 2 0 0 0 \n",
            "describe 0 1 0 0 0 \n",
            "early 0 1 0 0 0 \n",
            "section 0 1 0 0 0 \n",
            "able 0 1 0 3 0 \n",
            "venerable 0 1 0 0 0 \n",
            "WordNet 0 6 0 0 3 \n",
            "lexical 0 1 0 0 2 \n",
            "database 0 1 0 0 4 \n",
            "computable 0 1 0 0 0 \n",
            "thesaurus 0 1 0 0 0 \n",
            "integration 0 3 0 0 2 \n",
            "wordnet 0 9 0 0 0 \n",
            "Daniel 0 1 0 0 0 \n",
            "Vila 0 1 0 0 0 \n",
            "Suero 0 1 0 0 0 \n",
            "expert 0 1 0 0 0 \n",
            "via 0 1 1 0 1 \n",
            "happen 0 1 0 0 0 \n",
            "nltk 0 2 1 0 1 \n",
            "nltk_data 0 3 0 0 0 \n",
            "Downloading 0 1 1 0 0 \n",
            "home 0 1 0 0 0 \n",
            "ceteri 0 1 0 0 0 \n",
            "Package 0 1 0 0 0 \n",
            "already 0 1 0 0 0 \n",
            "date 0 1 0 0 1 \n",
            "True 0 1 1 0 4 \n",
            "runs 0 1 0 0 0 \n",
            "pipeline 0 2 0 0 0 \n",
            "allows 0 1 1 0 1 \n",
            "means 0 1 0 0 2 \n",
            "customizing 0 1 0 0 0 \n",
            "parts 0 1 2 0 2 \n",
            "supporting 0 1 0 0 0 \n",
            "really 0 1 1 0 0 \n",
            "workflow 0 1 0 0 0 \n",
            "WordnetAnnotator 0 5 0 0 0 \n",
            "spacy_wordnet 0 1 0 0 0 \n",
            "wordnet_annotator 0 1 0 0 0 \n",
            "before 0 2 0 3 4 \n",
            "pipe_names 0 5 0 0 0 \n",
            "add_pipe 0 3 0 0 0 \n",
            "lang 0 1 0 0 0 \n",
            "tagger 0 3 0 0 0 \n",
            "ner 0 2 0 0 0 \n",
            "Within 0 1 0 0 0 \n",
            "infamous 0 1 0 0 0 \n",
            "having 0 3 0 0 0 \n",
            "click 0 1 0 0 0 \n",
            "results 0 4 0 0 1 \n",
            "withdraw 0 14 0 0 0 \n",
            "_ 0 4 1 0 0 \n",
            "synsets 0 5 0 0 0 \n",
            "Synset 0 12 0 0 0 \n",
            "v 0 19 3 8 0 \n",
            "01 0 11 0 0 0 \n",
            "retire 0 3 0 0 0 \n",
            "02 0 3 0 0 0 \n",
            "disengage 0 1 0 0 0 \n",
            "recall 0 1 0 0 0 \n",
            "07 0 1 1 0 0 \n",
            "swallow 0 1 0 0 0 \n",
            "05 0 1 0 0 0 \n",
            "seclude 0 1 0 0 0 \n",
            "adjourn 0 1 0 0 0 \n",
            "bow_out 0 1 0 0 0 \n",
            "09 0 1 0 0 0 \n",
            "08 0 1 0 0 1 \n",
            "retreat 0 2 0 0 0 \n",
            "04 0 1 0 0 0 \n",
            "remove 0 1 1 0 3 \n",
            "Lemma 0 7 0 0 0 \n",
            "pull_away 0 1 0 0 0 \n",
            "draw_back 0 1 0 0 0 \n",
            "recede 0 1 0 0 0 \n",
            "pull_back 0 1 0 0 0 \n",
            "wordnet_domains 0 1 0 0 0 \n",
            "astronomy 0 1 0 0 0 \n",
            "telegraphy 0 1 0 0 0 \n",
            "psychology 0 1 0 0 0 \n",
            "ethnology 0 3 0 0 0 \n",
            "administration 0 1 0 0 0 \n",
            "finance 0 2 1 0 0 \n",
            "economy 0 1 0 0 0 \n",
            "exchange 0 1 0 0 0 \n",
            "banking 0 2 0 0 0 \n",
            "commerce 0 1 0 0 0 \n",
            "medicine 0 1 0 0 0 \n",
            "university 0 1 0 0 0 \n",
            "Again 0 1 0 0 0 \n",
            "graphs 0 2 0 0 0 \n",
            "larger 0 1 0 1 2 \n",
            "sections 0 1 0 0 0 \n",
            "technique 0 1 1 0 3 \n",
            "beyond 0 1 0 0 0 \n",
            "scope 0 1 0 0 0 \n",
            "currently 0 1 0 0 0 \n",
            "Going 0 1 0 0 0 \n",
            "direction 0 1 0 0 0 \n",
            "know 0 1 0 0 3 \n",
            "priori 0 1 0 0 0 \n",
            "domain 0 1 0 0 0 \n",
            "set 0 2 5 0 9 \n",
            "topics 0 1 0 0 1 \n",
            "constrain 0 2 0 0 0 \n",
            "returned 0 1 0 0 0 \n",
            "want 0 4 1 0 3 \n",
            "Finance 0 1 0 0 1 \n",
            "Banking 0 1 0 0 0 \n",
            "5 0 3 1 1 3 \n",
            "000 0 2 3 0 0 \n",
            "euros 0 2 0 0 0 \n",
            "enriched_sent 0 4 0 0 0 \n",
            "wordnet_synsets_for_domain 0 1 0 0 0 \n",
            "lemmas_for_synset 0 3 0 0 0 \n",
            "synset 0 1 0 0 0 \n",
            "variants 0 1 0 0 0 \n",
            "enriched 0 1 0 0 0 \n",
            "extend 0 1 0 0 0 \n",
            "lemma_names 0 1 0 0 0 \n",
            "format 0 1 0 0 0 \n",
            "else 0 1 0 0 0 \n",
            "require 0 1 1 0 0 \n",
            "draw_off 0 1 0 0 0 \n",
            "draw 0 1 0 0 1 \n",
            "take_out 0 1 0 0 0 \n",
            "play 0 1 2 0 0 \n",
            "list 0 1 6 0 4 \n",
            "combinatorial 0 1 0 0 0 \n",
            "explosion 0 1 0 0 0 \n",
            "without 0 1 1 0 2 \n",
            "constraints 0 1 0 0 0 \n",
            "Imagine 0 2 0 0 0 \n",
            "millions 0 2 0 0 0 \n",
            "d 0 2 0 0 9 \n",
            "searches 0 1 0 0 0 \n",
            "avoid 0 1 0 0 0 \n",
            "every 0 1 1 1 0 \n",
            "query 0 1 0 0 9 \n",
            "days 0 1 0 0 0 \n",
            "weeks 0 1 0 0 0 \n",
            "months 0 1 0 0 0 \n",
            "compute 0 1 0 0 0 \n",
            "Scattertext 0 1 0 0 0 \n",
            "Sometimes 0 1 0 0 0 \n",
            "encountered 0 1 0 0 0 \n",
            "trying 0 2 0 0 0 \n",
            "yet 0 1 0 0 2 \n",
            "corpus 0 4 1 0 1 \n",
            "dataset 0 1 1 0 1 \n",
            "interactive 0 2 0 0 0 \n",
            "visualization 0 2 0 0 1 \n",
            "scattertext 0 3 0 0 0 \n",
            "genius 0 1 0 0 0 \n",
            "Jason 0 1 0 0 0 \n",
            "Kessler 0 1 0 0 0 \n",
            "party 0 2 1 0 2 \n",
            "conventions 0 1 0 0 0 \n",
            "during 0 3 0 0 0 \n",
            "2012 0 1 0 0 0 \n",
            "US 0 1 0 0 1 \n",
            "Presidential 0 1 0 0 0 \n",
            "elections 0 1 0 0 0 \n",
            "cell 0 1 0 0 0 \n",
            "number 0 1 0 1 7 \n",
            "crunching 0 1 0 0 0 \n",
            "worth 0 2 0 0 0 \n",
            "wait 0 2 0 0 0 \n",
            "st 0 4 0 0 0 \n",
            "merge_entities 0 2 0 0 0 \n",
            "create_pipe 0 2 0 0 0 \n",
            "merge_noun_chunks 0 2 0 0 0 \n",
            "convention_df 0 3 0 0 0 \n",
            "SampleCorpora 0 1 0 0 0 \n",
            "ConventionData2012 0 1 0 0 0 \n",
            "get_data 0 1 0 0 0 \n",
            "CorpusFromPandas 0 1 0 0 0 \n",
            "category_col 0 1 0 0 0 \n",
            "text_col 0 1 0 0 0 \n",
            "Once 0 1 0 0 0 \n",
            "ready 0 1 0 0 1 \n",
            "generate 0 1 0 1 2 \n",
            "produce_scattertext_explorer 0 1 0 0 0 \n",
            "category 0 1 1 0 6 \n",
            "democrat 0 1 0 0 0 \n",
            "category_name 0 1 0 0 0 \n",
            "Democratic 0 1 0 0 0 \n",
            "not_category_name 0 1 0 0 0 \n",
            "Republican 0 2 0 0 0 \n",
            "width_in_pixels 0 1 0 0 0 \n",
            "1000 0 1 0 0 0 \n",
            "metadata 0 1 0 0 0 \n",
            "give 0 1 0 0 2 \n",
            "minute 0 1 0 0 0 \n",
            "IPython 0 1 0 0 0 \n",
            "display 0 1 0 0 5 \n",
            "IFrame 0 2 0 0 0 \n",
            "file_name 0 3 0 0 0 \n",
            "foo 0 1 0 0 0 \n",
            "wb 0 1 0 0 0 \n",
            "f 0 2 0 0 4 \n",
            "encode 0 1 0 0 0 \n",
            "utf 0 1 0 0 0 \n",
            "8 0 2 3 1 3 \n",
            "src 0 1 0 0 0 \n",
            "width 0 1 0 0 2 \n",
            "1200 0 1 0 0 0 \n",
            "height 0 1 0 0 2 \n",
            "700 0 1 0 0 0 \n",
            "past 0 2 1 2 2 \n",
            "organization 0 1 0 0 0 \n",
            "Suppose 0 1 0 0 0 \n",
            "team 0 1 0 0 0 \n",
            "needed 0 2 0 0 0 \n",
            "customers 0 1 0 0 0 \n",
            "talking 0 2 0 0 0 \n",
            "come 0 1 0 4 1 \n",
            "handy 0 1 0 0 0 \n",
            "cluster 0 1 0 0 0 \n",
            "k 0 1 2 0 2 \n",
            "NPS 0 1 0 0 0 \n",
            "scores 0 1 0 0 0 \n",
            "evaluation 0 1 1 0 0 \n",
            "metric 0 1 0 0 0 \n",
            "replace 0 1 0 0 0 \n",
            "Democrat 0 1 0 0 0 \n",
            "dimension 0 1 0 0 0 \n",
            "components 0 2 0 0 1 \n",
            "clustering 0 1 0 0 0 \n",
            "Summary 0 1 1 0 0 \n",
            "Five 0 1 0 0 0 \n",
            "asked 0 1 0 0 0 \n",
            "answer 0 1 0 0 2 \n",
            "would 0 1 3 0 2 \n",
            "everything 0 2 0 0 0 \n",
            "kitchen 0 1 0 0 0 \n",
            "sink 0 1 0 0 0 \n",
            "relatively 0 2 0 0 2 \n",
            "academic 0 3 0 0 1 \n",
            "Another 0 2 1 0 1 \n",
            "CoreNLP 0 2 0 0 0 \n",
            "Stanford 0 1 0 0 0 \n",
            "Also 0 2 0 0 2 \n",
            "albeit 0 1 0 0 0 \n",
            "powerful 0 1 0 0 1 \n",
            "though 0 1 0 0 1 \n",
            "integrate 0 1 0 0 0 \n",
            "production 0 1 0 0 3 \n",
            "corner 0 1 0 0 0 \n",
            "began 0 3 0 0 0 \n",
            "change 0 2 1 0 0 \n",
            "principal 0 1 0 0 0 \n",
            "authors 0 1 0 0 0 \n",
            "Matthew 0 1 0 0 0 \n",
            "Honnibal 0 1 0 0 0 \n",
            "Ines 0 1 0 0 0 \n",
            "Montani 0 1 0 0 0 \n",
            "launched 0 1 0 0 0 \n",
            "2015 0 2 0 0 0 \n",
            "adoption 0 1 0 0 0 \n",
            "rapid 0 2 0 0 0 \n",
            "They 0 1 2 1 6 \n",
            "focused 0 2 0 1 0 \n",
            "opinionated 0 1 0 0 0 \n",
            "well 0 2 0 1 3 \n",
            "no 0 2 0 0 3 \n",
            "less 0 1 0 0 2 \n",
            "provided 0 1 0 0 0 \n",
            "workflows 0 1 0 0 0 \n",
            "faster 0 1 0 0 1 \n",
            "execution 0 1 0 0 0 \n",
            "alternatives 0 1 0 0 0 \n",
            "Based 0 1 1 0 1 \n",
            "priorities 0 1 0 0 0 \n",
            "became 0 1 0 0 0 \n",
            "sort 0 1 0 0 0 \n",
            "Since 0 1 0 0 1 \n",
            "consistently 0 1 0 0 0 \n",
            "being 0 2 1 0 3 \n",
            "depending 0 1 0 0 2 \n",
            "directions 0 1 0 0 0 \n",
            "commercial 0 1 0 0 0 \n",
            "said 0 1 1 0 0 \n",
            "incorporate 0 1 0 0 0 \n",
            "SOTA 0 3 0 0 0 \n",
            "effectively 0 1 1 0 0 \n",
            "becoming 0 1 0 0 0 \n",
            "conduit 0 1 0 0 0 \n",
            "moving 0 1 0 0 0 \n",
            "important 0 1 0 1 3 \n",
            "note 0 1 0 0 0 \n",
            "got 0 1 0 0 0 \n",
            "boost 0 1 0 0 0 \n",
            "mid 0 1 0 0 0 \n",
            "2000 0 1 0 0 0 \n",
            "win 0 1 0 0 0 \n",
            "international 0 1 0 0 0 \n",
            "competitions 0 1 0 0 0 \n",
            "occurred 0 1 0 0 0 \n",
            "2017 0 1 0 1 0 \n",
            "2018 0 1 0 1 2 \n",
            "successes 0 1 0 0 0 \n",
            "previous 0 1 1 0 0 \n",
            "ELMo 0 1 0 0 0 \n",
            "embedding 0 1 7 0 0 \n",
            "Allen 0 1 0 0 0 \n",
            "followed 0 1 0 0 1 \n",
            "BERT 0 2 0 0 0 \n",
            "recently 0 1 0 0 0 \n",
            "ERNIE 0 1 0 0 0 \n",
            "Baidu 0 1 0 0 0 \n",
            "giants 0 1 0 0 0 \n",
            "gifted 0 1 0 0 0 \n",
            "rest 0 1 0 0 0 \n",
            "Sesame 0 2 0 0 0 \n",
            "Street 0 2 0 0 0 \n",
            "repertoire 0 1 0 0 0 \n",
            "embedded 0 1 1 0 0 \n",
            "state 0 1 0 0 0 \n",
            "art 0 1 0 0 0 \n",
            "Speaking 0 1 0 0 0 \n",
            "keep 0 2 0 0 0 \n",
            "track 0 1 0 0 1 \n",
            "eye 0 1 0 0 0 \n",
            "Progress 0 1 0 0 0 \n",
            "Papers 0 1 0 0 0 \n",
            "Code 0 3 0 2 6 \n",
            "shifted 0 1 0 0 0 \n",
            "dramatically 0 1 0 0 0 \n",
            "arose 0 1 0 0 0 \n",
            "fore 0 1 0 0 0 \n",
            "Circa 0 2 0 0 0 \n",
            "2014 0 1 0 1 0 \n",
            "shown 0 1 0 0 5 \n",
            "count 0 1 0 0 1 \n",
            "keyword 0 1 0 0 0 \n",
            "target 0 1 0 0 0 \n",
            "underwhelming 0 1 0 0 0 \n",
            "analyzing 0 1 2 0 2 \n",
            "thousands 0 1 0 0 2 \n",
            "vendor 0 1 0 0 0 \n",
            "industrial 0 1 0 0 0 \n",
            "supply 0 1 0 0 0 \n",
            "chain 0 1 0 0 0 \n",
            "optimization 0 1 0 0 1 \n",
            "hundreds 0 1 0 0 0 \n",
            "policyholders 0 1 0 0 0 \n",
            "gazillions 0 1 0 0 0 \n",
            "regarding 0 1 0 0 0 \n",
            "disclosures 0 1 0 0 0 \n",
            "contemporary 0 1 0 0 0 \n",
            "tends 0 1 0 0 0 \n",
            "construction 0 1 0 2 0 \n",
            "increasingly 0 1 0 0 0 \n",
            "numbers 0 1 0 1 0 \n",
            "summarized 0 1 0 0 0 \n",
            "Universe 0 1 0 0 0 \n",
            "dives 0 1 0 0 0 \n",
            "field 0 1 1 0 1 \n",
            "evolving 0 1 0 0 0 \n",
            "selections 0 1 0 0 0 \n",
            "universe 0 1 0 0 0 \n",
            "Blackstone 0 1 0 0 0 \n",
            "legal 0 1 0 0 0 \n",
            "Kindred 0 1 0 0 0 \n",
            "extracting 0 1 0 0 2 \n",
            "biomedical 0 1 0 0 0 \n",
            "Pharma 0 1 0 0 0 \n",
            "mordecai 0 1 0 0 0 \n",
            "geographic 0 1 0 0 0 \n",
            "Prodigy 0 1 0 0 0 \n",
            "loop 0 1 0 0 0 \n",
            "annotation 0 1 0 0 0 \n",
            "labeling 0 1 8 0 0 \n",
            "datasets 0 1 2 0 2 \n",
            "Rasa 0 2 0 0 0 \n",
            "chat 0 1 0 0 0 \n",
            "apps 0 1 0 1 0 \n",
            "couple 0 1 0 0 0 \n",
            "super 0 1 0 0 0 \n",
            "new 0 1 1 0 5 \n",
            "items 0 1 1 0 0 \n",
            "mention 0 1 0 0 0 \n",
            "pytorch 0 1 0 0 0 \n",
            "transformers 0 1 0 0 0 \n",
            "fine 0 1 0 0 0 \n",
            "tune 0 1 0 0 0 \n",
            "transfer 0 1 0 0 0 \n",
            "characters 0 1 0 0 1 \n",
            "friends 0 1 0 0 0 \n",
            "GPT 0 1 0 0 0 \n",
            "XLNet 0 1 0 0 0 \n",
            "IRL 0 1 0 0 0 \n",
            "conference 0 1 0 0 0 \n",
            "videos 0 1 0 1 0 \n",
            "talks 0 1 0 0 0 \n",
            "hopefully 0 1 0 0 0 \n",
            "wish 0 1 0 0 0 \n",
            "Practical 0 1 0 0 0 \n",
            "Techniques 0 1 0 0 0 \n",
            "Share 0 1 0 0 2 \n",
            "Subscribe 0 1 0 0 3 \n",
            "Newsletter 0 1 0 0 9 \n",
            "Receive 0 1 0 0 0 \n",
            "tips 0 1 0 0 0 \n",
            "tutorials 0 1 0 0 0 \n",
            "leading 0 1 0 0 2 \n",
            "leaders 0 1 0 0 2 \n",
            "right 0 1 0 0 0 \n",
            "inbox 0 1 0 0 0 \n",
            "Other 0 1 0 0 2 \n",
            "class 0 1 1 0 0 \n",
            "v3 0 1 0 0 0 \n",
            "David 0 3 0 0 0 \n",
            "Bloch 0 1 0 0 0 \n",
            "Comparing 0 1 0 0 0 \n",
            "Functionality 0 1 0 0 0 \n",
            "Libraries 0 1 1 0 1 \n",
            "guest 0 1 0 0 0 \n",
            "Maziyar 0 2 0 0 0 \n",
            "Panahi 0 2 0 0 0 \n",
            "Talby 0 1 0 0 0 \n",
            "cheat 0 1 0 0 0 \n",
            "sheet 0 1 0 0 0 \n",
            "choosing 0 1 0 0 0 \n",
            "Ta 0 1 0 0 0 \n",
            "around 0 1 0 0 2 \n",
            "spreadsheets 0 1 0 0 1 \n",
            "analyse 0 1 0 0 0 \n",
            "daily 0 1 0 0 0 \n",
            "basis 0 1 1 0 0 \n",
            "weather 0 1 0 0 0 \n",
            "forecast 0 1 0 0 0 \n",
            "Dr 0 1 0 0 0 \n",
            "J 0 1 0 0 0 \n",
            "Rogel 0 1 0 0 0 \n",
            "Salazar 0 1 0 0 0 \n",
            "22 0 1 0 0 5 \n",
            "135 0 1 0 0 1 \n",
            "Townsend 0 1 0 0 0 \n",
            "St 0 1 0 0 0 \n",
            "Floor 0 1 0 0 0 \n",
            "5San 0 1 0 0 0 \n",
            "Francisco 0 1 0 0 0 \n",
            "CA 0 1 0 0 0 \n",
            "94107 0 1 0 0 0 \n",
            "415 0 1 0 0 0 \n",
            "570 0 1 0 0 0 \n",
            "2425 0 1 0 0 0 \n",
            "Product 0 1 0 0 3 \n",
            "Dictionary 0 1 0 0 0 \n",
            "Start 0 0 2 0 0 \n",
            "Articles 0 0 2 0 0 \n",
            "Vision 0 0 1 0 3 \n",
            "Visualization 0 0 1 0 4 \n",
            "Interview 0 0 2 0 0 \n",
            "Questions 0 0 2 1 0 \n",
            "Infographics 0 0 1 0 0 \n",
            "Podcasts 0 0 1 0 0 \n",
            "E 0 0 1 0 0 \n",
            "Books 0 0 1 0 2 \n",
            "Companies 0 0 2 0 0 \n",
            "Datahack 0 0 1 0 0 \n",
            "Summit 0 0 1 0 0 \n",
            "Glossary 0 0 1 0 0 \n",
            "Archive 0 0 1 0 2 \n",
            "Write 0 0 2 0 0 \n",
            "Article 0 0 3 0 0 \n",
            "Courses 0 0 6 0 0 \n",
            "Certified 0 0 1 0 0 \n",
            "ML 0 0 1 0 7 \n",
            "BlackBelt 0 0 1 0 0 \n",
            "Plus 0 0 1 0 0 \n",
            "Immersive 0 0 1 0 0 \n",
            "Bootcamp 0 0 1 0 0 \n",
            "Blogathon 0 0 2 0 0 \n",
            "Creators 0 0 1 0 0 \n",
            "Club 0 0 1 0 0 \n",
            "Join 0 0 2 0 1 \n",
            "Manage 0 0 2 0 0 \n",
            "AV 0 0 2 0 0 \n",
            "Account 0 0 2 0 0 \n",
            "My 0 0 8 1 1 \n",
            "Hackathons 0 0 3 0 0 \n",
            "Bookmarks 0 0 2 0 0 \n",
            "Applied 0 0 2 0 0 \n",
            "Out 0 0 4 0 1 \n",
            "Home 0 0 1 1 1 \n",
            "Related 0 0 3 0 1 \n",
            "Free 0 0 2 0 2 \n",
            "Movie 0 0 1 0 0 \n",
            "Review 0 0 2 0 0 \n",
            "Example 0 0 3 0 8 \n",
            "Facebook 0 0 1 0 1 \n",
            "Twitter 0 0 1 0 7 \n",
            "Linkedin 0 0 1 0 0 \n",
            "Parlad 0 0 2 0 0 \n",
            "Neupane 0 0 2 0 0 \n",
            "Published 0 0 1 0 1 \n",
            "On 0 0 3 0 2 \n",
            "December 0 0 2 0 0 \n",
            "11 0 0 2 6 2 \n",
            "Last 0 0 1 0 1 \n",
            "11th 0 0 1 0 0 \n",
            "Advanced 0 0 1 0 0 \n",
            "Classification 0 0 1 0 4 \n",
            "Entertainment 0 0 1 0 0 \n",
            "Programming 0 0 1 0 5 \n",
            "Project 0 0 1 2 3 \n",
            "Supervised 0 0 1 0 0 \n",
            "Unstructured 0 0 1 0 0 \n",
            "published 0 0 2 0 0 \n",
            "improved 0 0 1 0 0 \n",
            "tremendously 0 0 1 0 0 \n",
            "needing 0 0 1 0 0 \n",
            "underlying 0 0 3 0 1 \n",
            "hardware 0 0 1 0 0 \n",
            "infrastructure 0 0 1 0 0 \n",
            "Users 0 0 1 0 0 \n",
            "program 0 0 2 0 3 \n",
            "old 0 0 1 0 0 \n",
            "beneficiary 0 0 1 0 0 \n",
            "effect 0 0 3 0 0 \n",
            "unlimited 0 0 1 0 0 \n",
            "branches 0 0 1 0 0 \n",
            "gives 0 0 1 1 4 \n",
            "deliver 0 0 2 0 0 \n",
            "very 0 0 2 0 6 \n",
            "successful 0 0 1 0 1 \n",
            "resource 0 0 1 0 0 \n",
            "speeches 0 0 2 0 0 \n",
            "plenty 0 0 1 0 0 \n",
            "hard 0 0 1 0 1 \n",
            "mine 0 0 1 0 0 \n",
            "Written 0 0 1 0 0 \n",
            "contain 0 0 2 0 0 \n",
            "because 0 0 2 0 1 \n",
            "intelligent 0 0 1 0 1 \n",
            "writing 0 0 1 0 0 \n",
            "primary 0 0 1 0 0 \n",
            "communication 0 0 1 0 1 \n",
            "cognitive 0 0 1 0 0 \n",
            "assistant 0 0 1 0 0 \n",
            "filtering 0 0 1 0 1 \n",
            "fake 0 0 2 0 0 \n",
            "news 0 0 2 0 1 \n",
            "will 0 0 11 4 24 \n",
            "cover 0 0 2 0 1 \n",
            "Mainly 0 0 1 0 0 \n",
            "focusing 0 0 1 1 0 \n",
            "Words 0 0 4 1 5 \n",
            "Sequence 0 0 4 0 0 \n",
            "Analysis 0 0 4 0 9 \n",
            "vector 0 0 5 3 1 \n",
            "probabilistic 0 0 4 0 0 \n",
            "sequential 0 0 2 0 0 \n",
            "reorganization 0 0 2 0 0 \n",
            "fifty 0 0 2 0 0 \n",
            "thousand 0 0 2 0 0 \n",
            "IMDB 0 0 5 0 0 \n",
            "movie 0 0 6 0 0 \n",
            "reviewer 0 0 2 0 0 \n",
            "Our 0 0 4 0 3 \n",
            "goal 0 0 3 0 2 \n",
            "review 0 0 3 0 0 \n",
            "posted 0 0 2 1 0 \n",
            "user 0 0 6 0 16 \n",
            "positive 0 0 2 0 1 \n",
            "negative 0 0 2 0 1 \n",
            "Topic 0 0 1 0 0 \n",
            "List 0 0 1 0 3 \n",
            "Understand 0 0 1 0 1 \n",
            "Sequences 0 0 2 0 0 \n",
            "Vector 0 0 6 0 2 \n",
            "Semantic 0 0 7 0 3 \n",
            "Probabilistic 0 0 2 0 0 \n",
            "Models 0 0 1 0 2 \n",
            "Parsers 0 0 1 0 0 \n",
            "Semantics 0 0 1 0 0 \n",
            "Performing 0 0 1 0 0 \n",
            "cars 0 0 1 0 0 \n",
            "smartphones 0 0 1 0 0 \n",
            "speakers 0 0 1 0 0 \n",
            "websites 0 0 2 0 1 \n",
            "Translator 0 0 3 0 0 \n",
            "translator 0 0 2 1 0 \n",
            "wrote 0 0 1 0 0 \n",
            "desire 0 0 1 0 0 \n",
            "google 0 0 2 0 1 \n",
            "noises 0 0 1 0 0 \n",
            "CNN 0 0 1 0 0 \n",
            "native 0 0 1 0 0 \n",
            "reduces 0 0 1 0 0 \n",
            "asking 0 0 2 0 0 \n",
            "needs 0 0 2 2 1 \n",
            "cans 0 0 1 0 0 \n",
            "ask 0 0 1 0 0 \n",
            "problem 0 0 1 3 1 \n",
            "AMAZON 0 0 1 0 0 \n",
            "robust 0 0 1 0 0 \n",
            "asks 0 0 1 0 0 \n",
            "converts 0 0 1 0 1 \n",
            "understandable 0 0 1 0 0 \n",
            "internal 0 0 1 0 0 \n",
            "call 0 0 1 0 1 \n",
            "toke 0 0 1 0 0 \n",
            "goes 0 0 2 0 1 \n",
            "idea 0 0 2 0 0 \n",
            "retrieval 0 0 2 0 2 \n",
            "IR 0 0 2 0 1 \n",
            "deals 0 0 1 0 2 \n",
            "storage 0 0 1 0 0 \n",
            "repositories 0 0 1 0 0 \n",
            "retrieve 0 0 1 0 1 \n",
            "only 0 0 4 3 6 \n",
            "trim 0 0 1 0 0 \n",
            "unnecessary 0 0 1 0 2 \n",
            "Application 0 0 2 0 1 \n",
            "Translation 0 0 1 3 2 \n",
            "Information 0 0 1 0 2 \n",
            "Question 0 0 1 0 1 \n",
            "Answering 0 0 1 0 1 \n",
            "ChatBot 0 0 1 0 0 \n",
            "Summarization 0 0 1 0 2 \n",
            "Mining 0 0 1 0 3 \n",
            "sign 0 0 1 0 0 \n",
            "properly 0 0 2 0 1 \n",
            "symbols 0 0 1 0 0 \n",
            "sequence 0 0 9 0 0 \n",
            "clarification 0 0 1 0 0 \n",
            "categorizing 0 0 1 0 0 \n",
            "group 0 0 4 0 3 \n",
            "predefined 0 0 2 0 0 \n",
            "topic 0 0 1 0 1 \n",
            "Rule 0 0 1 0 4 \n",
            "Hybrid 0 0 4 0 0 \n",
            "separated 0 0 1 0 1 \n",
            "organized 0 0 1 0 0 \n",
            "handicraft 0 0 2 0 0 \n",
            "linguistic 0 0 3 0 0 \n",
            "Those 0 0 1 0 0 \n",
            "define 0 0 1 0 3 \n",
            "characterized 0 0 1 0 0 \n",
            "groups 0 0 1 0 1 \n",
            "Donald 0 0 1 0 0 \n",
            "Trump 0 0 1 0 0 \n",
            "Boris 0 0 1 0 0 \n",
            "Johnson 0 0 1 0 0 \n",
            "categorized 0 0 3 0 1 \n",
            "politics 0 0 1 0 0 \n",
            "People 0 0 1 0 0 \n",
            "LeBron 0 0 1 0 0 \n",
            "James 0 0 1 0 0 \n",
            "Ronaldo 0 0 1 0 0 \n",
            "sports 0 0 1 0 0 \n",
            "classifier 0 0 3 0 0 \n",
            "learns 0 0 2 0 0 \n",
            "observation 0 0 1 0 0 \n",
            "User 0 0 1 0 0 \n",
            "prelabeled 0 0 1 0 0 \n",
            "tarin 0 0 1 0 0 \n",
            "collects 0 0 1 0 0 \n",
            "strategy 0 0 1 0 0 \n",
            "inputs 0 0 1 0 0 \n",
            "continuously 0 0 1 0 0 \n",
            "bag 0 0 2 0 9 \n",
            "extension 0 0 1 0 0 \n",
            "represents 0 0 1 0 1 \n",
            "frequency 0 0 1 0 11 \n",
            "dictionary 0 0 1 1 7 \n",
            "Nai 0 0 1 0 0 \n",
            "Bayer 0 0 1 0 0 \n",
            "SVM 0 0 1 0 1 \n",
            "third 0 0 2 0 5 \n",
            "Approach 0 0 1 0 0 \n",
            "tag 0 0 1 0 1 \n",
            "train 0 0 4 0 2 \n",
            "compared 0 0 1 0 1 \n",
            "something 0 0 1 0 4 \n",
            "improve 0 0 3 0 2 \n",
            "manually 0 0 1 0 0 \n",
            "method 0 0 3 3 5 \n",
            "implement 0 0 1 0 2 \n",
            "defines 0 0 1 0 0 \n",
            "interprets 0 0 1 0 0 \n",
            "main 0 0 1 0 1 \n",
            "alike 0 0 1 0 0 \n",
            "divide 0 0 2 0 1 \n",
            "multi 0 0 1 0 0 \n",
            "dimensional 0 0 2 0 0 \n",
            "Embedding 0 0 4 0 0 \n",
            "translates 0 0 1 1 0 \n",
            "spares 0 0 1 0 0 \n",
            "vectors 0 0 1 1 2 \n",
            "low 0 0 1 0 1 \n",
            "preserves 0 0 1 0 0 \n",
            "type 0 0 1 0 5 \n",
            "types 0 0 1 0 2 \n",
            "Word2vec 0 0 1 0 0 \n",
            "Doc2Vec 0 0 3 0 0 \n",
            "Word2Vec 0 0 1 0 0 \n",
            "standalone 0 0 1 0 0 \n",
            "analyzes 0 0 1 0 1 \n",
            "calculate 0 0 1 0 2 \n",
            "probability 0 0 2 0 0 \n",
            "occurring 0 0 1 0 0 \n",
            "00013131 0 0 1 0 0 \n",
            "percent 0 0 1 0 0 \n",
            "Labeling 0 0 1 0 1 \n",
            "typical 0 0 1 0 0 \n",
            "assigns 0 0 1 0 0 \n",
            "someone 0 0 1 0 0 \n",
            "says 0 0 1 0 0 \n",
            "tom 0 0 4 0 0 \n",
            "hanks 0 0 2 0 0 \n",
            "Play 0 0 1 0 0 \n",
            "Movies 0 0 1 0 0 \n",
            "Tom 0 0 1 0 0 \n",
            "Hanks 0 0 1 0 0 \n",
            "divides 0 0 2 0 0 \n",
            "LSTM 0 0 3 1 0 \n",
            "Parsing 0 0 2 0 2 \n",
            "phase 0 0 1 0 0 \n",
            "syntactic 0 0 2 0 2 \n",
            "constituent 0 0 2 0 0 \n",
            "ate 0 0 2 0 0 \n",
            "apple 0 0 2 0 0 \n",
            "divided 0 0 1 0 0 \n",
            "determiner 0 0 1 0 2 \n",
            "discuss 0 0 1 0 1 \n",
            "classified 0 0 1 0 1 \n",
            "algorithm 0 0 1 0 6 \n",
            "discover 0 0 2 0 0 \n",
            "covers 0 0 2 0 0 \n",
            "Bags 0 0 1 0 0 \n",
            "bidirectional 0 0 1 0 0 \n",
            "labeled 0 0 1 0 0 \n",
            "TensorFlow 0 0 1 0 0 \n",
            "steps 0 0 1 0 0 \n",
            "cleaning 0 0 1 0 1 \n",
            "balance 0 0 1 0 0 \n",
            "sampling 0 0 1 0 0 \n",
            "Parser 0 0 1 0 1 \n",
            "component 0 0 1 0 0 \n",
            "separates 0 0 1 0 0 \n",
            "details 0 0 2 0 5 \n",
            "heart 0 0 1 0 1 \n",
            "communicate 0 0 1 0 1 \n",
            "written 0 0 1 0 0 \n",
            "conversation 0 0 1 0 0 \n",
            "lengthy 0 0 1 0 0 \n",
            "articles 0 0 1 0 1 \n",
            "books 0 0 1 0 0 \n",
            "seeks 0 0 1 0 0 \n",
            "constructing 0 0 1 0 0 \n",
            "convey 0 0 1 0 0 \n",
            "feedback 0 0 1 0 1 \n",
            "Sample 0 0 1 0 0 \n",
            "Importing 0 0 2 0 2 \n",
            "necessary 0 0 7 1 1 \n",
            "defined 0 0 1 0 0 \n",
            "kaggle 0 0 5 0 0 \n",
            "python 0 0 3 0 1 \n",
            "Docker 0 0 1 0 0 \n",
            "image 0 0 1 0 5 \n",
            "github 0 0 1 0 0 \n",
            "com 0 0 1 8 2 \n",
            "docker 0 0 1 0 0 \n",
            "here 0 0 1 0 3 \n",
            "several 0 0 1 1 2 \n",
            "numpy 0 0 1 0 0 \n",
            "np 0 0 1 0 0 \n",
            "linear 0 0 1 0 0 \n",
            "algebra 0 0 1 0 0 \n",
            "CSV 0 0 2 0 0 \n",
            "file 0 0 3 0 8 \n",
            "O 0 0 1 0 0 \n",
            "read_csv 0 0 1 0 0 \n",
            "Input 0 0 1 0 0 \n",
            "files 0 0 3 1 0 \n",
            "directory 0 0 3 0 0 \n",
            "running 0 0 2 0 0 \n",
            "clicking 0 0 1 0 1 \n",
            "pressing 0 0 1 0 0 \n",
            "Shift 0 0 1 0 0 \n",
            "under 0 0 1 3 1 \n",
            "os 0 0 4 0 0 \n",
            "dirname 0 0 2 0 0 \n",
            "filenames 0 0 2 0 0 \n",
            "walk 0 0 1 0 0 \n",
            "filename 0 0 2 0 0 \n",
            "path 0 0 2 0 1 \n",
            "20GB 0 0 1 0 0 \n",
            "current 0 0 2 1 1 \n",
            "gets 0 0 1 1 1 \n",
            "preserved 0 0 1 0 0 \n",
            "version 0 0 1 0 0 \n",
            "Save 0 0 1 0 0 \n",
            "Run 0 0 1 0 0 \n",
            "temporary 0 0 1 0 0 \n",
            "temp 0 0 1 0 0 \n",
            "won 0 0 1 0 0 \n",
            "saved 0 0 1 0 0 \n",
            "outside 0 0 1 0 0 \n",
            "session 0 0 1 0 0 \n",
            "matplotlib 0 0 2 0 0 \n",
            "pyplot 0 0 2 0 0 \n",
            "plt 0 0 2 0 0 \n",
            "tkinter 0 0 1 0 0 \n",
            "seaborn 0 0 1 0 0 \n",
            "sns 0 0 2 0 0 \n",
            "scipy 0 0 1 0 0 \n",
            "tensorflow 0 0 5 0 0 \n",
            "tf 0 0 2 0 1 \n",
            "tensorflow_hub 0 0 1 0 0 \n",
            "hub 0 0 1 0 0 \n",
            "tensorflow_datasets 0 0 1 0 0 \n",
            "tfds 0 0 2 0 0 \n",
            "keras 0 0 4 0 0 \n",
            "Sequential 0 0 1 0 0 \n",
            "layers 0 0 1 0 0 \n",
            "Dense 0 0 1 0 0 \n",
            "sklearn 0 0 3 0 1 \n",
            "model_selection 0 0 1 0 0 \n",
            "train_test_split 0 0 1 0 0 \n",
            "confusion_matrix 0 0 1 0 0 \n",
            "classification_report 0 0 1 0 0 \n",
            "cells 0 0 1 0 0 \n",
            "takes 0 0 1 0 3 \n",
            "please 0 0 2 0 2 \n",
            "once 0 0 1 0 0 \n",
            "Split 0 0 1 0 0 \n",
            "training 0 0 2 0 1 \n",
            "60 0 0 3 0 1 \n",
            "40 0 0 1 0 1 \n",
            "15 0 0 3 1 4 \n",
            "validation 0 0 1 0 0 \n",
            "testing 0 0 1 0 0 \n",
            "original_train_data 0 0 1 0 0 \n",
            "original_validation_data 0 0 1 0 0 \n",
            "original_test_data 0 0 1 0 0 \n",
            "imdb_reviews 0 0 1 0 0 \n",
            "split 0 0 1 0 0 \n",
            "as_supervised 0 0 1 0 0 \n",
            "Keras 0 0 1 0 0 \n",
            "tokanizing 0 0 1 0 0 \n",
            "word_index 0 0 2 0 0 \n",
            "imdb 0 0 1 0 0 \n",
            "get_word_index 0 0 1 0 0 \n",
            "imdb_word_index 0 0 1 0 0 \n",
            "json 0 0 1 0 0 \n",
            "16 0 0 1 0 2 \n",
            "14 0 0 1 1 3 \n",
            "9 0 0 1 3 3 \n",
            "6 0 0 1 1 2 \n",
            "18 0 0 1 0 2 \n",
            "4 0 0 1 1 3 \n",
            "br 0 0 1 0 0 \n",
            "7 0 0 1 2 2 \n",
            "13 0 0 1 1 2 \n",
            "film 0 0 1 0 0 \n",
            "19 0 0 3 0 2 \n",
            "12 0 0 1 3 2 \n",
            "Positive 0 0 1 0 0 \n",
            "Negative 0 0 1 0 0 \n",
            "Comparision 0 0 1 0 0 \n",
            "Creating 0 0 1 0 1 \n",
            "Train 0 0 1 0 0 \n",
            "Test 0 0 1 0 0 \n",
            "Splitting 0 0 1 0 0 \n",
            "fitting 0 0 1 0 0 \n",
            "Overview 0 0 2 0 1 \n",
            "Confusion 0 0 1 0 0 \n",
            "Matrix 0 0 1 0 0 \n",
            "Correlation 0 0 1 0 0 \n",
            "Report 0 0 1 0 0 \n",
            "publicly 0 0 1 0 0 \n",
            "Tensorflow 0 0 1 0 1 \n",
            "follow 0 0 2 0 0 \n",
            "GitHub 0 0 1 0 1 \n",
            "Repository 0 0 1 0 2 \n",
            "conclusion 0 0 1 0 0 \n",
            "opportunities 0 0 2 0 0 \n",
            "tremendous 0 0 1 0 0 \n",
            "Knowledge 0 0 1 0 0 \n",
            "impossible 0 0 1 0 0 \n",
            "five 0 0 1 0 1 \n",
            "rise 0 0 1 0 0 \n",
            "made 0 0 1 0 0 \n",
            "still 0 0 1 0 1 \n",
            "blogathondoc2vevparsingtext 0 0 1 0 0 \n",
            "classificationwor2vec 0 0 1 0 0 \n",
            "Duration 0 0 1 0 0 \n",
            "Oct 0 0 3 0 0 \n",
            "21 0 0 2 0 3 \n",
            "27 0 0 2 1 2 \n",
            "Table 0 0 1 0 3 \n",
            "contents 0 0 2 0 0 \n",
            "Author 0 0 1 0 1 \n",
            "Top 0 0 3 0 0 \n",
            "Authors 0 0 1 0 3 \n",
            "view 0 0 1 0 0 \n",
            "Download 0 0 2 0 0 \n",
            "Vidhya 0 0 6 0 0 \n",
            "App 0 0 2 0 0 \n",
            "Latest 0 0 1 0 4 \n",
            "Previous 0 0 1 0 0 \n",
            "Post 0 0 3 1 1 \n",
            "Kick 0 0 1 0 0 \n",
            "Journey 0 0 1 0 0 \n",
            "Pattern 0 0 1 0 3 \n",
            "Recognition 0 0 1 0 9 \n",
            "Leave 0 0 1 0 0 \n",
            "Reply 0 0 1 0 4 \n",
            "Your 0 0 1 0 1 \n",
            "address 0 0 1 0 0 \n",
            "Required 0 0 1 0 0 \n",
            "marked 0 0 1 0 0 \n",
            "Cancel 0 0 1 0 1 \n",
            "reply 0 0 1 0 1 \n",
            "Notify 0 0 2 0 0 \n",
            "Submit 0 0 1 0 0 \n",
            "Tutorial 0 0 1 0 8 \n",
            "Working 0 0 1 0 0 \n",
            "Harika 0 0 1 0 0 \n",
            "Bonthu 0 0 1 0 0 \n",
            "Aug 0 0 2 0 0 \n",
            "2021 0 0 1 0 3 \n",
            "Boost 0 0 1 0 0 \n",
            "Accuracy 0 0 1 0 0 \n",
            "Imbalanced 0 0 1 0 0 \n",
            "COVID 0 0 1 0 0 \n",
            "Mortality 0 0 1 0 0 \n",
            "Prediction 0 0 1 0 0 \n",
            "Using 0 0 1 3 10 \n",
            "GAN 0 0 1 0 0 \n",
            "Bala 0 0 1 0 0 \n",
            "Gangadhar 0 0 1 0 0 \n",
            "Thilak 0 0 1 0 0 \n",
            "Adiboina 0 0 1 0 0 \n",
            "Most 0 0 1 0 0 \n",
            "Comprehensive 0 0 1 0 0 \n",
            "Guide 0 0 1 0 0 \n",
            "K 0 0 1 0 0 \n",
            "Means 0 0 1 0 0 \n",
            "Clustering 0 0 1 0 0 \n",
            "Ever 0 0 1 0 0 \n",
            "Need 0 0 1 0 0 \n",
            "Pulkit 0 0 1 0 0 \n",
            "Sharma 0 0 2 0 0 \n",
            "Joins 0 0 2 0 0 \n",
            "Pandas 0 0 1 0 1 \n",
            "Master 0 0 1 0 0 \n",
            "Different 0 0 1 0 0 \n",
            "Types 0 0 1 0 0 \n",
            "Abhishek 0 0 1 0 0 \n",
            "Feb 0 0 1 0 0 \n",
            "Us 0 0 1 0 1 \n",
            "Team 0 0 1 0 4 \n",
            "Hackathon 0 0 1 0 0 \n",
            "Discussions 0 0 1 0 0 \n",
            "Apply 0 0 1 0 0 \n",
            "Trainings 0 0 1 0 0 \n",
            "Advertising 0 0 1 0 1 \n",
            "Visit 0 0 1 0 0 \n",
            "Copyright 0 0 1 1 0 \n",
            "2013 0 0 1 0 0 \n",
            "2022 0 0 1 0 20 \n",
            "Privacy 0 0 3 0 5 \n",
            "Policy 0 0 4 0 3 \n",
            "Terms 0 0 2 0 1 \n",
            "Refund 0 0 1 0 0 \n",
            "cookies 0 0 13 0 24 \n",
            "traffic 0 0 1 0 1 \n",
            "agree 0 0 1 0 0 \n",
            "AcceptPrivacy 0 0 1 0 0 \n",
            "Cookies 0 0 1 0 0 \n",
            "Close 0 0 1 0 1 \n",
            "website 0 0 8 0 10 \n",
            "navigate 0 0 1 0 1 \n",
            "stored 0 0 2 0 2 \n",
            "browser 0 0 2 0 2 \n",
            "basic 0 0 2 0 6 \n",
            "functionalities 0 0 2 0 3 \n",
            "consent 0 0 2 0 9 \n",
            "option 0 0 1 0 2 \n",
            "opt 0 0 1 0 1 \n",
            "opting 0 0 1 0 1 \n",
            "affect 0 0 1 0 1 \n",
            "browsing 0 0 1 0 1 \n",
            "Necessary 0 0 3 0 4 \n",
            "Always 0 0 1 0 1 \n",
            "Enabled 0 0 1 0 1 \n",
            "absolutely 0 0 1 0 1 \n",
            "ensures 0 0 1 0 0 \n",
            "security 0 0 1 0 1 \n",
            "store 0 0 1 0 7 \n",
            "personal 0 0 2 0 2 \n",
            "Non 0 0 2 0 1 \n",
            "Any 0 0 1 0 1 \n",
            "particularly 0 0 1 0 0 \n",
            "collect 0 0 1 0 2 \n",
            "ads 0 0 1 0 2 \n",
            "termed 0 0 1 0 0 \n",
            "mandatory 0 0 1 0 0 \n",
            "procure 0 0 1 0 0 \n",
            "prior 0 0 1 0 0 \n",
            "Please 0 0 1 0 1 \n",
            "signup 0 0 1 0 0 \n",
            "CCTP 0 0 0 1 0 \n",
            "607 0 0 0 1 0 \n",
            "Big 0 0 0 1 0 \n",
            "Ideas 0 0 0 1 0 \n",
            "Spring 0 0 0 1 0 \n",
            "Menu 0 0 0 1 0 \n",
            "Course 0 0 0 6 0 \n",
            "Syllabus 0 0 0 1 0 \n",
            "Prof 0 0 0 1 0 \n",
            "Irvine 0 0 0 4 0 \n",
            "Weekly 0 0 0 2 0 \n",
            "Writing 0 0 0 3 0 \n",
            "Instructions 0 0 0 2 0 \n",
            "Final 0 0 0 2 4 \n",
            "replacement 0 0 0 1 0 \n",
            "As 0 0 0 2 8 \n",
            "seen 0 0 0 1 1 \n",
            "readings 0 0 0 1 0 \n",
            "module 0 0 0 2 0 \n",
            "Because 0 0 0 1 0 \n",
            "deconstruct 0 0 0 3 0 \n",
            "reconstruct 0 0 0 1 0 \n",
            "depend 0 0 0 1 0 \n",
            "addresses 0 0 0 1 0 \n",
            "complexity 0 0 0 2 0 \n",
            "ambiguity 0 0 0 1 0 \n",
            "PBS 0 0 0 2 0 \n",
            "Crash 0 0 0 4 0 \n",
            "video 0 0 0 6 0 \n",
            "breaks 0 0 0 1 1 \n",
            "Deconstructing 0 0 0 1 0 \n",
            "smaller 0 0 0 1 1 \n",
            "processed 0 0 0 1 0 \n",
            "order 0 0 0 1 2 \n",
            "Development 0 0 0 1 0 \n",
            "Phrase 0 0 0 1 1 \n",
            "Structure 0 0 0 1 1 \n",
            "Rules 0 0 0 1 0 \n",
            "encapsulate 0 0 0 1 0 \n",
            "phrase 0 0 0 1 7 \n",
            "structures 0 0 0 2 0 \n",
            "trees 0 0 0 1 0 \n",
            "Image 0 0 0 4 0 \n",
            "retrieved 0 0 0 4 0 \n",
            "www 0 0 0 8 2 \n",
            "youtube 0 0 0 8 0 \n",
            "watch 0 0 0 8 0 \n",
            "fOvTtapxa9c 0 0 0 2 0 \n",
            "Parse 0 0 0 1 0 \n",
            "Trees 0 0 0 1 1 \n",
            "likely 0 0 0 1 0 \n",
            "explains 0 0 0 1 0 \n",
            "Additionally 0 0 0 1 0 \n",
            "Looking 0 0 0 1 0 \n",
            "Network 0 0 0 5 0 \n",
            "works 0 0 0 2 4 \n",
            "Emportium 0 0 0 1 0 \n",
            "describes 0 0 0 1 0 \n",
            "network 0 0 0 4 0 \n",
            "solver 0 0 0 1 0 \n",
            "job 0 0 0 1 1 \n",
            "solve 0 0 0 1 0 \n",
            "French 0 0 0 6 0 \n",
            "learned 0 0 0 2 1 \n",
            "brains 0 0 0 1 0 \n",
            "convert 0 0 0 2 0 \n",
            "Recurrent 0 0 0 3 0 \n",
            "Step 0 0 0 2 0 \n",
            "Take 0 0 0 1 0 \n",
            "Convert 0 0 0 1 0 \n",
            "AIpXjFwVdIE 0 0 0 4 0 \n",
            "According 0 0 0 1 0 \n",
            "Encoder 0 0 0 2 0 \n",
            "Decoder 0 0 0 1 0 \n",
            "Architecture 0 0 0 2 0 \n",
            "pictured 0 0 0 1 0 \n",
            "medium 0 0 0 1 0 \n",
            "length 0 0 0 1 0 \n",
            "Cho 0 0 0 1 0 \n",
            "et 0 0 0 1 1 \n",
            "al 0 0 0 1 1 \n",
            "Emporium 0 0 0 1 0 \n",
            "tested 0 0 0 1 0 \n",
            "RNN 0 0 0 1 0 \n",
            "longer 0 0 0 1 0 \n",
            "translations 0 0 0 1 0 \n",
            "did 0 0 0 1 1 \n",
            "lack 0 0 0 1 1 \n",
            "present 0 0 0 1 5 \n",
            "While 0 0 0 1 0 \n",
            "generating 0 0 0 2 1 \n",
            "10th 0 0 0 1 0 \n",
            "looks 0 0 0 3 0 \n",
            "nine 0 0 0 1 1 \n",
            "looking 0 0 0 1 2 \n",
            "both 0 0 0 1 3 \n",
            "Therefore 0 0 0 1 4 \n",
            "BiDirectional 0 0 0 2 0 \n",
            "Bidirectional 0 0 0 1 0 \n",
            "Vs 0 0 0 1 0 \n",
            "should 0 0 0 1 1 \n",
            "align 0 0 0 1 0 \n",
            "additional 0 0 0 1 0 \n",
            "unit 0 0 0 1 0 \n",
            "attention 0 0 0 3 0 \n",
            "mechanism 0 0 0 3 0 \n",
            "Process 0 0 0 1 0 \n",
            "Layer 0 0 0 1 0 \n",
            "Breakdown 0 0 0 1 0 \n",
            "encoder 0 0 0 1 0 \n",
            "assigned 0 0 0 1 1 \n",
            "focus 0 0 0 1 1 \n",
            "decoder 0 0 0 1 0 \n",
            "determined 0 0 0 1 0 \n",
            "Works 0 0 0 3 0 \n",
            "Cited 0 0 0 2 0 \n",
            "CrashCourse 0 0 0 3 0 \n",
            "Structures 0 0 0 1 0 \n",
            "YouTube 0 0 0 4 0 \n",
            "DuDz6B4cqVc 0 0 0 1 0 \n",
            "Intelligence 0 0 0 2 7 \n",
            "34 0 0 0 1 2 \n",
            "z 0 0 0 1 0 \n",
            "EtmaFJieY 0 0 0 1 0 \n",
            "36 0 0 0 1 1 \n",
            "CS 0 0 0 1 0 \n",
            "Dojo 0 0 0 1 0 \n",
            "Algorithm 0 0 0 1 3 \n",
            "Explained 0 0 0 1 1 \n",
            "Thierry 0 0 0 1 0 \n",
            "Poibeau 0 0 0 1 0 \n",
            "Cambridge 0 0 0 1 0 \n",
            "MA 0 0 0 1 0 \n",
            "Selections 0 0 0 1 0 \n",
            "entry 0 0 0 1 0 \n",
            "Week 0 0 0 13 0 \n",
            "February 0 0 0 1 0 \n",
            "Adey 0 0 0 2 0 \n",
            "Zegeye 0 0 0 2 0 \n",
            "navigation 0 0 0 1 1 \n",
            "Deblackboxing 0 0 0 1 0 \n",
            "Pidgin 0 0 0 1 0 \n",
            "Search 0 0 0 1 6 \n",
            "Recent 0 0 0 1 1 \n",
            "Posts 0 0 0 1 1 \n",
            "Design 0 0 0 1 0 \n",
            "Ethical 0 0 0 1 0 \n",
            "Implications 0 0 0 1 0 \n",
            "Explainable 0 0 0 1 0 \n",
            "XAI 0 0 0 1 0 \n",
            "Music 0 0 0 2 0 \n",
            "To 0 0 0 1 2 \n",
            "Ears 0 0 0 1 0 \n",
            "De 0 0 0 1 0 \n",
            "Blackboxing 0 0 0 1 0 \n",
            "Spotify 0 0 0 1 0 \n",
            "Recommendation 0 0 0 1 2 \n",
            "Engine 0 0 0 1 0 \n",
            "Gender 0 0 0 1 0 \n",
            "Bias 0 0 0 1 1 \n",
            "Challenges 0 0 0 1 0 \n",
            "Algorithmic 0 0 0 1 0 \n",
            "Composition 0 0 0 1 0 \n",
            "Chatting 0 0 0 1 0 \n",
            "Way 0 0 0 1 0 \n",
            "High 0 0 0 1 0 \n",
            "Brand 0 0 0 1 0 \n",
            "Equity 0 0 0 1 0 \n",
            "Class 0 0 0 1 0 \n",
            "Seminar 0 0 0 1 0 \n",
            "MembersKevin 0 0 0 1 0 \n",
            "Ackermann 0 0 0 1 0 \n",
            "Linda 0 0 0 1 0 \n",
            "Bardha 0 0 0 1 0 \n",
            "Annaliese 0 0 0 1 0 \n",
            "Blank 0 0 0 1 0 \n",
            "Beiyuan 0 0 0 1 0 \n",
            "Gu 0 0 0 1 0 \n",
            "Dominique 0 0 0 1 0 \n",
            "Haywood 0 0 0 1 0 \n",
            "Yajing 0 0 0 1 0 \n",
            "Hu 0 0 0 1 0 \n",
            "Proma 0 0 0 1 0 \n",
            "Huq 0 0 0 1 0 \n",
            "Deborah 0 0 0 1 0 \n",
            "Oliveros 0 0 0 1 0 \n",
            "Zachary 0 0 0 1 0 \n",
            "Omer 0 0 0 1 0 \n",
            "Shahin 0 0 0 1 0 \n",
            "Rafikian 0 0 0 1 0 \n",
            "Beiyue 0 0 0 1 0 \n",
            "Wang 0 0 0 1 0 \n",
            "Tianyi 0 0 0 1 0 \n",
            "Zhao 0 0 0 1 0 \n",
            "Professor 0 0 0 1 0 \n",
            "Site 0 0 0 2 0 \n",
            "AdministratorMartin 0 0 0 1 0 \n",
            "Uses 0 0 0 1 0 \n",
            "Martin 0 0 0 2 0 \n",
            "student 0 0 0 1 0 \n",
            "licensed 0 0 0 1 0 \n",
            "Creative 0 0 0 4 0 \n",
            "Commons 0 0 0 4 0 \n",
            "Attribution 0 0 0 2 0 \n",
            "Noncommercial 0 0 0 1 0 \n",
            "No 0 0 0 1 0 \n",
            "Derivative 0 0 0 1 0 \n",
            "United 0 0 0 1 0 \n",
            "States 0 0 0 1 0 \n",
            "educational 0 0 0 1 0 \n",
            "permitted 0 0 0 1 0 \n",
            "attribution 0 0 0 2 1 \n",
            "quoted 0 0 0 1 0 \n",
            "property 0 0 0 2 0 \n",
            "copyright 0 0 0 1 0 \n",
            "holders 0 0 0 1 0 \n",
            "creative 0 0 0 1 0 \n",
            "students 0 0 0 1 1 \n",
            "seminar 0 0 0 1 0 \n",
            "respective 0 0 0 1 2 \n",
            "writers 0 0 0 1 0 \n",
            "referenced 0 0 0 1 1 \n",
            "faculty 0 0 0 1 0 \n",
            "georgetown 0 0 0 2 0 \n",
            "edu 0 0 0 2 0 \n",
            "irvinem 0 0 0 2 0 \n",
            "NonCommercial 0 0 0 1 0 \n",
            "NoDerivs 0 0 0 1 0 \n",
            "Meta 0 0 0 1 0 \n",
            "Log 0 0 0 1 0 \n",
            "Entries 0 0 0 1 0 \n",
            "RSS 0 0 0 2 0 \n",
            "Comments 0 0 0 1 1 \n",
            "Trends 0 0 0 0 1 \n",
            "Shop 0 0 0 0 5 \n",
            "Categories 0 0 0 0 2 \n",
            "Fairness 0 0 0 0 3 \n",
            "Bioinformatics 0 0 0 0 3 \n",
            "Computing 0 0 0 0 6 \n",
            "Cybersecurity 0 0 0 0 3 \n",
            "Engineering 0 0 0 0 9 \n",
            "DevOps 0 0 0 0 3 \n",
            "Editorial 0 0 0 0 13 \n",
            "Ethics 0 0 0 0 4 \n",
            "Future 0 0 0 0 3 \n",
            "Game 0 0 0 0 3 \n",
            "Theory 0 0 0 0 3 \n",
            "Automated 0 0 0 0 3 \n",
            "Art 0 0 0 0 3 \n",
            "Mathematics 0 0 0 0 4 \n",
            "Neuroscience 0 0 0 0 3 \n",
            "Opinion 0 0 0 0 3 \n",
            "Optimization 0 0 0 0 3 \n",
            "Physics 0 0 0 0 3 \n",
            "Security 0 0 0 0 3 \n",
            "Program 0 0 0 0 3 \n",
            "Probability 0 0 0 0 3 \n",
            "Quantum 0 0 0 0 3 \n",
            "Research 0 0 0 0 4 \n",
            "Robotics 0 0 0 0 3 \n",
            "Scholarly 0 0 0 0 5 \n",
            "Cars 0 0 0 0 3 \n",
            "Software 0 0 0 0 3 \n",
            "Statistics 0 0 0 0 4 \n",
            "Systems 0 0 0 0 4 \n",
            "Web 0 0 0 0 3 \n",
            "Scraping 0 0 0 0 3 \n",
            "Sponsors 0 0 0 0 6 \n",
            "Tutorials 0 0 0 0 7 \n",
            "eBooks 0 0 0 0 2 \n",
            "Name 0 0 0 0 14 \n",
            "Towards 0 0 0 0 30 \n",
            "Legal 0 0 0 0 1 \n",
            "Inc 0 0 0 0 5 \n",
            "Description 0 0 0 0 1 \n",
            "publication 0 0 0 0 2 \n",
            "Read 0 0 0 0 3 \n",
            "thought 0 0 0 0 2 \n",
            "decision 0 0 0 0 2 \n",
            "makers 0 0 0 0 2 \n",
            "Website 0 0 0 0 1 \n",
            "towardsai 0 0 0 0 9 \n",
            "net 0 0 0 0 8 \n",
            "Publisher 0 0 0 0 1 \n",
            "publisher 0 0 0 0 2 \n",
            "Diversity 0 0 0 0 1 \n",
            "Masthead 0 0 0 0 1 \n",
            "Founder 0 0 0 0 1 \n",
            "Roberto 0 0 0 0 6 \n",
            "Iriondo 0 0 0 0 6 \n",
            "Cover 0 0 0 0 1 \n",
            "Logo 0 0 0 0 1 \n",
            "Areas 0 0 0 0 1 \n",
            "Served 0 0 0 0 1 \n",
            "Worldwide 0 0 0 0 1 \n",
            "Alternate 0 0 0 0 11 \n",
            "towards 0 0 0 0 2 \n",
            "ai 0 0 0 0 4 \n",
            "tai 0 0 0 0 1 \n",
            "toward 0 0 0 0 2 \n",
            "pub 0 0 0 0 1 \n",
            "Follow 0 0 0 0 1 \n",
            "LinkedIn 0 0 0 0 1 \n",
            "Instagram 0 0 0 0 1 \n",
            "Youtube 0 0 0 0 1 \n",
            "Github 0 0 0 0 2 \n",
            "Business 0 0 0 0 3 \n",
            "Maps 0 0 0 0 1 \n",
            "Discord 0 0 0 0 1 \n",
            "Medium 0 0 0 0 5 \n",
            "Flipboard 0 0 0 0 1 \n",
            "Publication 0 0 0 0 5 \n",
            "Feed 0 0 0 0 1 \n",
            "Contribute 0 0 0 0 1 \n",
            "stars 0 0 0 0 1 \n",
            "497 0 0 0 0 1 \n",
            "Frequently 0 0 0 0 1 \n",
            "Used 0 0 0 0 1 \n",
            "Contextual 0 0 0 0 1 \n",
            "References 0 0 0 0 2 \n",
            "Remember 0 0 0 0 1 \n",
            "copy 0 0 0 0 1 \n",
            "IDs 0 0 0 0 1 \n",
            "whenever 0 0 0 0 1 \n",
            "URL 0 0 0 0 1 \n",
            "304b2e42315e 0 0 0 0 1 \n",
            "enthusiasts 0 0 0 0 1 \n",
            "161 0 0 0 0 1 \n",
            "likes 0 0 0 0 1 \n",
            "Updated 0 0 0 0 1 \n",
            "October 0 0 0 0 7 \n",
            "Pratik 0 0 0 0 2 \n",
            "Shukla 0 0 0 0 3 \n",
            "Pixabay 0 0 0 0 1 \n",
            "basics 0 0 0 0 5 \n",
            "sample 0 0 0 0 13 \n",
            "implementation 0 0 0 0 8 \n",
            "explore 0 0 0 0 1 \n",
            "toolkit 0 0 0 0 1 \n",
            "Afterward 0 0 0 0 1 \n",
            "coding 0 0 0 0 2 \n",
            "implementations 0 0 0 0 1 \n",
            "Colab 0 0 0 0 4 \n",
            "Contents 0 0 0 0 1 \n",
            "Applications 0 0 0 0 3 \n",
            "Current 0 0 0 0 2 \n",
            "challenges 0 0 0 0 2 \n",
            "Easy 0 0 0 0 2 \n",
            "Exploring 0 0 0 0 2 \n",
            "Features 0 0 0 0 7 \n",
            "Stemming 0 0 0 0 8 \n",
            "Lemmatization 0 0 0 0 8 \n",
            "PoS 0 0 0 0 13 \n",
            "Chunking 0 0 0 0 6 \n",
            "Chinking 0 0 0 0 5 \n",
            "Entity 0 0 0 0 7 \n",
            "Bag 0 0 0 0 5 \n",
            "Computers 0 0 0 0 1 \n",
            "tabular 0 0 0 0 1 \n",
            "However 0 0 0 0 7 \n",
            "speak 0 0 0 0 1 \n",
            "clear 0 0 0 0 1 \n",
            "interpret 0 0 0 0 3 \n",
            "subfield 0 0 0 0 1 \n",
            "depth 0 0 0 0 1 \n",
            "interactions 0 0 0 0 1 \n",
            "Chatbot 0 0 0 0 1 \n",
            "Intelligent 0 0 0 0 1 \n",
            "Classifications 0 0 0 0 2 \n",
            "Character 0 0 0 0 1 \n",
            "Spell 0 0 0 0 1 \n",
            "Checking 0 0 0 0 2 \n",
            "Detection 0 0 0 0 2 \n",
            "Autocomplete 0 0 0 0 2 \n",
            "Predictive 0 0 0 0 2 \n",
            "Typing 0 0 0 0 1 \n",
            "Figure 0 0 0 0 140 \n",
            "Revealing 0 0 0 0 1 \n",
            "listening 0 0 0 0 1 \n",
            "considerably 0 0 0 0 1 \n",
            "misunderstand 0 0 0 0 1 \n",
            "thing 0 0 0 0 1 \n",
            "differently 0 0 0 0 1 \n",
            "interpretation 0 0 0 0 2 \n",
            "saw 0 0 0 0 5 \n",
            "hill 0 0 0 0 6 \n",
            "telescope 0 0 0 0 6 \n",
            "interpretations 0 0 0 0 2 \n",
            "watched 0 0 0 0 1 \n",
            "him 0 0 0 0 2 \n",
            "my 0 0 0 0 4 \n",
            "m 0 0 0 0 2 \n",
            "Can 0 0 0 0 2 \n",
            "formation 0 0 0 0 2 \n",
            "second 0 0 0 0 7 \n",
            "represent 0 0 0 0 4 \n",
            "holds 0 0 0 0 1 \n",
            "food 0 0 0 0 1 \n",
            "liquid 0 0 0 0 1 \n",
            "Hence 0 0 0 0 2 \n",
            "deterministic 0 0 0 0 2 \n",
            "suitable 0 0 0 0 2 \n",
            "situations 0 0 0 0 3 \n",
            "freezing 0 0 0 0 1 \n",
            "temperature 0 0 0 0 1 \n",
            "lead 0 0 0 0 2 \n",
            "death 0 0 0 0 1 \n",
            "hot 0 0 0 0 2 \n",
            "coffee 0 0 0 0 1 \n",
            "burn 0 0 0 0 1 \n",
            "skin 0 0 0 0 1 \n",
            "requires 0 0 0 0 1 \n",
            "manual 0 0 0 0 1 \n",
            "effort 0 0 0 0 1 \n",
            "amounts 0 0 0 0 2 \n",
            "tries 0 0 0 0 2 \n",
            "derive 0 0 0 0 1 \n",
            "After 0 0 0 0 1 \n",
            "trained 0 0 0 0 2 \n",
            "outcomes 0 0 0 0 1 \n",
            "deduction 0 0 0 0 1 \n",
            "Lexical 0 0 0 0 1 \n",
            "With 0 0 0 0 4 \n",
            "whole 0 0 0 0 3 \n",
            "Syntactic 0 0 0 0 2 \n",
            "arranging 0 0 0 0 1 \n",
            "manner 0 0 0 0 1 \n",
            "shows 0 0 0 0 5 \n",
            "relationship 0 0 0 0 1 \n",
            "shop 0 0 0 0 1 \n",
            "house 0 0 0 0 1 \n",
            "pass 0 0 0 0 2 \n",
            "c 0 0 0 0 10 \n",
            "draws 0 0 0 0 1 \n",
            "exact 0 0 0 0 3 \n",
            "meaningfulness 0 0 0 0 1 \n",
            "Sentences 0 0 0 0 2 \n",
            "ice 0 0 0 0 1 \n",
            "cream 0 0 0 0 1 \n",
            "Disclosure 0 0 0 0 2 \n",
            "Integration 0 0 0 0 1 \n",
            "considers 0 0 0 0 2 \n",
            "ends 0 0 0 0 1 \n",
            "He 0 0 0 0 2 \n",
            "Pragmatic 0 0 0 0 2 \n",
            "overall 0 0 0 0 1 \n",
            "deriving 0 0 0 0 1 \n",
            "overview 0 0 0 0 2 \n",
            "beginners 0 0 0 0 1 \n",
            "Breaking 0 0 0 0 1 \n",
            "Tagging 0 0 0 0 2 \n",
            "Building 0 0 0 0 3 \n",
            "vocabulary 0 0 0 0 2 \n",
            "Linking 0 0 0 0 1 \n",
            "Extracting 0 0 0 0 1 \n",
            "Transforming 0 0 0 0 1 \n",
            "Ambiguity 0 0 0 0 1 \n",
            "usually 0 0 0 0 1 \n",
            "exciting 0 0 0 0 2 \n",
            "ease 0 0 0 0 1 \n",
            "Tokenization 0 0 0 0 4 \n",
            "Packages 0 0 0 0 1 \n",
            "Pros 0 0 0 0 5 \n",
            "cons 0 0 0 0 11 \n",
            "designed 0 0 0 0 2 \n",
            "fast 0 0 0 0 2 \n",
            "focuses 0 0 0 0 1 \n",
            "providing 0 0 0 0 1 \n",
            "Dependency 0 0 0 0 1 \n",
            "autocorrect 0 0 0 0 1 \n",
            "Analyzing 0 0 0 0 1 \n",
            "Gensim 0 0 0 0 3 \n",
            "purpose 0 0 0 0 1 \n",
            "handles 0 0 0 0 1 \n",
            "Latent 0 0 0 0 1 \n",
            "matrix 0 0 0 0 1 \n",
            "factorization 0 0 0 0 1 \n",
            "Converting 0 0 0 0 1 \n",
            "Finding 0 0 0 0 10 \n",
            "straightforward 0 0 0 0 2 \n",
            "syntax 0 0 0 0 1 \n",
            "scientific 0 0 0 0 2 \n",
            "highly 0 0 0 0 1 \n",
            "valuable 0 0 0 0 2 \n",
            "Spelling 0 0 0 0 2 \n",
            "correction 0 0 0 0 1 \n",
            "TextBlob 0 0 0 0 3 \n",
            "textual 0 0 0 0 1 \n",
            "Noun 0 0 0 0 9 \n",
            "Wordnet 0 0 0 0 15 \n",
            "Correction 0 0 0 0 1 \n",
            "dig 0 0 0 0 1 \n",
            "Small 0 0 0 0 1 \n",
            "snippet 0 0 0 0 11 \n",
            "string 0 0 0 0 2 \n",
            "notice 0 0 0 0 6 \n",
            "String 0 0 0 0 1 \n",
            "675 0 0 0 0 1 \n",
            "Import 0 0 0 0 1 \n",
            "Sentence 0 0 0 0 5 \n",
            "tokenizing 0 0 0 0 5 \n",
            "sent_tokenize 0 0 0 0 2 \n",
            "tokenize 0 0 0 0 3 \n",
            "total 0 0 0 0 3 \n",
            "word_tokenize 0 0 0 0 2 \n",
            "144 0 0 0 0 1 \n",
            "Find 0 0 0 0 1 \n",
            "distribution 0 0 0 0 5 \n",
            "FreqDist 0 0 0 0 1 \n",
            "Printing 0 0 0 0 2 \n",
            "ten 0 0 0 0 2 \n",
            "Notice 0 0 0 0 8 \n",
            "punctuation 0 0 0 0 13 \n",
            "marks 0 0 0 0 13 \n",
            "stopwords 0 0 0 0 6 \n",
            "actual 0 0 0 0 3 \n",
            "Plot 0 0 0 0 1 \n",
            "plot 0 0 0 0 1 \n",
            "Plotting 0 0 0 0 3 \n",
            "period 0 0 0 0 1 \n",
            "times 0 0 0 0 6 \n",
            "Analytically 0 0 0 0 1 \n",
            "removing 0 0 0 0 3 \n",
            "Remove 0 0 0 0 1 \n",
            "isalpha 0 0 0 0 2 \n",
            "separate 0 0 0 0 4 \n",
            "words_no_punc 0 0 0 0 2 \n",
            "lower 0 0 0 0 3 \n",
            "exclude 0 0 0 0 4 \n",
            "creating 0 0 0 0 1 \n",
            "excluded 0 0 0 0 2 \n",
            "cross 0 0 0 0 1 \n",
            "h 0 0 0 0 2 \n",
            "23 0 0 0 0 2 \n",
            "24 0 0 0 0 5 \n",
            "others 0 0 0 0 2 \n",
            "coordinating 0 0 0 0 1 \n",
            "conjunctions 0 0 0 0 1 \n",
            "26 0 0 0 0 2 \n",
            "j 0 0 0 0 2 \n",
            "Removing 0 0 0 0 1 \n",
            "Cleaning 0 0 0 0 1 \n",
            "28 0 0 0 0 3 \n",
            "Cleaned 0 0 0 0 1 \n",
            "Displaying 0 0 0 0 2 \n",
            "final 0 0 0 0 7 \n",
            "30 0 0 0 0 7 \n",
            "showing 0 0 0 0 7 \n",
            "chart 0 0 0 0 1 \n",
            "frequent 0 0 0 0 4 \n",
            "bolder 0 0 0 0 1 \n",
            "font 0 0 0 0 4 \n",
            "thinner 0 0 0 0 1 \n",
            "fonts 0 0 0 0 3 \n",
            "beneficial 0 0 0 0 1 \n",
            "glance 0 0 0 0 1 \n",
            "analyzed 0 0 0 0 2 \n",
            "Properties 0 0 0 0 1 \n",
            "font_path 0 0 0 0 1 \n",
            "specifies 0 0 0 0 8 \n",
            "canvas 0 0 0 0 3 \n",
            "min_font_size 0 0 0 0 1 \n",
            "smallest 0 0 0 0 1 \n",
            "size 0 0 0 0 5 \n",
            "max_font_size 0 0 0 0 1 \n",
            "largest 0 0 0 0 1 \n",
            "font_step 0 0 0 0 1 \n",
            "max_words 0 0 0 0 1 \n",
            "maximum 0 0 0 0 1 \n",
            "cloud 0 0 0 0 9 \n",
            "eliminate 0 0 0 0 1 \n",
            "background_color 0 0 0 0 1 \n",
            "color 0 0 0 0 1 \n",
            "normalize_plurals 0 0 0 0 1 \n",
            "removes 0 0 0 0 1 \n",
            "trailing 0 0 0 0 1 \n",
            "documentation 0 0 0 0 1 \n",
            "WordCloud 0 0 0 0 1 \n",
            "31 0 0 0 0 2 \n",
            "32 0 0 0 0 2 \n",
            "displayed 0 0 0 0 3 \n",
            "shape 0 0 0 0 6 \n",
            "circle 0 0 0 0 3 \n",
            "33 0 0 0 0 2 \n",
            "Circle 0 0 0 0 1 \n",
            "35 0 0 0 0 2 \n",
            "mentioned 0 0 0 0 1 \n",
            "CloudAdvantages 0 0 0 0 1 \n",
            "engaging 0 0 0 0 1 \n",
            "casual 0 0 0 0 1 \n",
            "visually 0 0 0 0 1 \n",
            "appealing 0 0 0 0 1 \n",
            "Disadvantages 0 0 0 0 1 \n",
            "clean 0 0 0 0 2 \n",
            "normalize 0 0 0 0 1 \n",
            "languages 0 0 0 0 2 \n",
            "single 0 0 0 0 1 \n",
            "upon 0 0 0 0 1 \n",
            "study 0 0 0 0 2 \n",
            "studies 0 0 0 0 4 \n",
            "studying 0 0 0 0 2 \n",
            "studied 0 0 0 0 2 \n",
            "interpreter 0 0 0 0 1 \n",
            "Moreover 0 0 0 0 1 \n",
            "resolve 0 0 0 0 1 \n",
            "normalizes 0 0 0 0 1 \n",
            "truncating 0 0 0 0 3 \n",
            "stem 0 0 0 0 3 \n",
            "reduced 0 0 0 0 1 \n",
            "studi 0 0 0 0 2 \n",
            "Porter 0 0 0 0 4 \n",
            "Stemmer 0 0 0 0 15 \n",
            "below 0 0 0 0 2 \n",
            "truncate 0 0 0 0 1 \n",
            "stemmed 0 0 0 0 1 \n",
            "recognizable 0 0 0 0 1 \n",
            "37 0 0 0 0 1 \n",
            "SnowballStemmer 0 0 0 0 2 \n",
            "generates 0 0 0 0 2 \n",
            "porter 0 0 0 0 1 \n",
            "stemmer 0 0 0 0 2 \n",
            "supports 0 0 0 0 1 \n",
            "38 0 0 0 0 1 \n",
            "Languages 0 0 0 0 1 \n",
            "supported 0 0 0 0 1 \n",
            "snowball 0 0 0 0 1 \n",
            "39 0 0 0 0 1 \n",
            "Various 0 0 0 0 1 \n",
            "Algorithms 0 0 0 0 1 \n",
            "pros 0 0 0 0 6 \n",
            "Lovin 0 0 0 0 2 \n",
            "41 0 0 0 0 1 \n",
            "Dawson 0 0 0 0 2 \n",
            "42 0 0 0 0 1 \n",
            "Krovetz 0 0 0 0 2 \n",
            "43 0 0 0 0 1 \n",
            "Xerox 0 0 0 0 2 \n",
            "44 0 0 0 0 1 \n",
            "Snowball 0 0 0 0 2 \n",
            "45 0 0 0 0 1 \n",
            "scratch 0 0 0 0 1 \n",
            "math 0 0 0 0 1 \n",
            "detail 0 0 0 0 3 \n",
            "base 0 0 0 0 2 \n",
            "finds 0 0 0 0 1 \n",
            "instead 0 0 0 0 2 \n",
            "why 0 0 0 0 1 \n",
            "higher 0 0 0 0 6 \n",
            "crucial 0 0 0 0 3 \n",
            "tight 0 0 0 0 1 \n",
            "deadline 0 0 0 0 1 \n",
            "amortization 0 0 0 0 1 \n",
            "speed 0 0 0 0 1 \n",
            "outputs 0 0 0 0 1 \n",
            "choices 0 0 0 0 1 \n",
            "46 0 0 0 0 1 \n",
            "Lemmatizer 0 0 0 0 3 \n",
            "truncated 0 0 0 0 1 \n",
            "47 0 0 0 0 1 \n",
            "Lemmatizing 0 0 0 0 1 \n",
            "During 0 0 0 0 1 \n",
            "displays 0 0 0 0 1 \n",
            "demonstrating 0 0 0 0 5 \n",
            "lemmatizer 0 0 0 0 2 \n",
            "taking 0 0 0 0 1 \n",
            "apply 0 0 0 0 1 \n",
            "49 0 0 0 0 1 \n",
            "Simple 0 0 0 0 1 \n",
            "value 0 0 0 0 21 \n",
            "n 0 0 0 0 1 \n",
            "50 0 0 0 0 1 \n",
            "power 0 0 0 0 2 \n",
            "am 0 0 0 0 1 \n",
            "52 0 0 0 0 1 \n",
            "Why 0 0 0 0 1 \n",
            "53 0 0 0 0 1 \n",
            "Parts 0 0 0 0 1 \n",
            "Giving 0 0 0 0 1 \n",
            "correctly 0 0 0 0 1 \n",
            "Below 0 0 0 0 1 \n",
            "CC 0 0 0 0 1 \n",
            "Coordinating 0 0 0 0 2 \n",
            "Conjunction 0 0 0 0 2 \n",
            "conjunction 0 0 0 0 2 \n",
            "CD 0 0 0 0 1 \n",
            "Cardinal 0 0 0 0 2 \n",
            "Digit 0 0 0 0 1 \n",
            "55 0 0 0 0 1 \n",
            "digit 0 0 0 0 1 \n",
            "DT 0 0 0 0 1 \n",
            "Determiner 0 0 0 0 4 \n",
            "56 0 0 0 0 1 \n",
            "EX 0 0 0 0 1 \n",
            "Existential 0 0 0 0 2 \n",
            "57 0 0 0 0 1 \n",
            "FW 0 0 0 0 1 \n",
            "Foreign 0 0 0 0 2 \n",
            "58 0 0 0 0 1 \n",
            "IN 0 0 0 0 1 \n",
            "Preposition 0 0 0 0 2 \n",
            "Subordinating 0 0 0 0 2 \n",
            "59 0 0 0 0 1 \n",
            "JJ 0 0 0 0 1 \n",
            "Adjective 0 0 0 0 7 \n",
            "JJR 0 0 0 0 1 \n",
            "Comparative 0 0 0 0 2 \n",
            "61 0 0 0 0 1 \n",
            "comparative 0 0 0 0 2 \n",
            "JJS 0 0 0 0 1 \n",
            "Superlative 0 0 0 0 2 \n",
            "62 0 0 0 0 1 \n",
            "LS 0 0 0 0 1 \n",
            "Marker 0 0 0 0 1 \n",
            "63 0 0 0 0 1 \n",
            "marker 0 0 0 0 1 \n",
            "MD 0 0 0 0 1 \n",
            "Modal 0 0 0 0 1 \n",
            "64 0 0 0 0 1 \n",
            "NN 0 0 0 0 1 \n",
            "Singular 0 0 0 0 4 \n",
            "65 0 0 0 0 1 \n",
            "singular 0 0 0 0 4 \n",
            "NNS 0 0 0 0 1 \n",
            "Plural 0 0 0 0 2 \n",
            "66 0 0 0 0 1 \n",
            "plural 0 0 0 0 2 \n",
            "NNP 0 0 0 0 1 \n",
            "Proper 0 0 0 0 5 \n",
            "67 0 0 0 0 1 \n",
            "68 0 0 0 0 1 \n",
            "PDT 0 0 0 0 1 \n",
            "Predeterminer 0 0 0 0 2 \n",
            "69 0 0 0 0 1 \n",
            "Possessive 0 0 0 0 6 \n",
            "Endings 0 0 0 0 1 \n",
            "70 0 0 0 0 1 \n",
            "endings 0 0 0 0 1 \n",
            "PRP 0 0 0 0 2 \n",
            "Personal 0 0 0 0 2 \n",
            "Pronoun 0 0 0 0 7 \n",
            "71 0 0 0 0 1 \n",
            "72 0 0 0 0 1 \n",
            "RB 0 0 0 0 1 \n",
            "Adverb 0 0 0 0 10 \n",
            "73 0 0 0 0 1 \n",
            "RBR 0 0 0 0 1 \n",
            "74 0 0 0 0 1 \n",
            "RBS 0 0 0 0 1 \n",
            "75 0 0 0 0 1 \n",
            "superlative 0 0 0 0 1 \n",
            "RP 0 0 0 0 1 \n",
            "Particle 0 0 0 0 2 \n",
            "76 0 0 0 0 1 \n",
            "TO 0 0 0 0 1 \n",
            "77 0 0 0 0 1 \n",
            "UH 0 0 0 0 1 \n",
            "Interjection 0 0 0 0 2 \n",
            "78 0 0 0 0 1 \n",
            "VB 0 0 0 0 1 \n",
            "Verb 0 0 0 0 13 \n",
            "Base 0 0 0 0 1 \n",
            "Form 0 0 0 0 1 \n",
            "79 0 0 0 0 1 \n",
            "VBD 0 0 0 0 1 \n",
            "Past 0 0 0 0 2 \n",
            "Tense 0 0 0 0 3 \n",
            "80 0 0 0 0 1 \n",
            "tense 0 0 0 0 3 \n",
            "VBG 0 0 0 0 1 \n",
            "Present 0 0 0 0 3 \n",
            "Participle 0 0 0 0 2 \n",
            "81 0 0 0 0 1 \n",
            "participle 0 0 0 0 2 \n",
            "VBN 0 0 0 0 1 \n",
            "82 0 0 0 0 1 \n",
            "VBP 0 0 0 0 1 \n",
            "Third 0 0 0 0 2 \n",
            "Person 0 0 0 0 2 \n",
            "83 0 0 0 0 1 \n",
            "VBZ 0 0 0 0 1 \n",
            "84 0 0 0 0 1 \n",
            "Wh 0 0 0 0 4 \n",
            "85 0 0 0 0 1 \n",
            "WP 0 0 0 0 1 \n",
            "86 0 0 0 0 1 \n",
            "87 0 0 0 0 1 \n",
            "88 0 0 0 0 1 \n",
            "89 0 0 0 0 1 \n",
            "90 0 0 0 0 1 \n",
            "Full 0 0 0 0 3 \n",
            "book 0 0 0 0 1 \n",
            "infer 0 0 0 0 1 \n",
            "literally 0 0 0 0 1 \n",
            "individual 0 0 0 0 1 \n",
            "91 0 0 0 0 1 \n",
            "chunking 0 0 0 0 4 \n",
            "Before 0 0 0 0 2 \n",
            "Meaningful 0 0 0 0 1 \n",
            "significant 0 0 0 0 1 \n",
            "Phrases 0 0 0 0 5 \n",
            "NP 0 0 0 0 5 \n",
            "VP 0 0 0 0 3 \n",
            "ADJP 0 0 0 0 1 \n",
            "ADVP 0 0 0 0 1 \n",
            "Prepositional 0 0 0 0 1 \n",
            "PP 0 0 0 0 4 \n",
            "S 0 0 0 0 1 \n",
            "V 0 0 0 0 1 \n",
            "AP 0 0 0 0 1 \n",
            "92 0 0 0 0 1 \n",
            "optional 0 0 0 0 1 \n",
            "adjectives 0 0 0 0 4 \n",
            "RegexpParser 0 0 0 0 1 \n",
            "93 0 0 0 0 1 \n",
            "successfully 0 0 0 0 1 \n",
            "94 0 0 0 0 1 \n",
            "Successful 0 0 0 0 1 \n",
            "excludes 0 0 0 0 1 \n",
            "extractions 0 0 0 0 1 \n",
            "unuseful 0 0 0 0 2 \n",
            "scenarios 0 0 0 0 1 \n",
            "chinking 0 0 0 0 6 \n",
            "chunked 0 0 0 0 1 \n",
            "apart 0 0 0 0 1 \n",
            "inverted 0 0 0 0 1 \n",
            "curly 0 0 0 0 1 \n",
            "braces 0 0 0 0 1 \n",
            "95 0 0 0 0 1 \n",
            "From 0 0 0 0 2 \n",
            "96 0 0 0 0 1 \n",
            "fundamental 0 0 0 0 1 \n",
            "places 0 0 0 0 1 \n",
            "money 0 0 0 0 1 \n",
            "discussed 0 0 0 0 1 \n",
            "Content 0 0 0 0 1 \n",
            "Summarizing 0 0 0 0 1 \n",
            "resumes 0 0 0 0 1 \n",
            "Optimizing 0 0 0 0 1 \n",
            "Customer 0 0 0 0 1 \n",
            "Commonly 0 0 0 0 1 \n",
            "97 0 0 0 0 1 \n",
            "commonly 0 0 0 0 1 \n",
            "binary 0 0 0 0 8 \n",
            "98 0 0 0 0 1 \n",
            "99 0 0 0 0 1 \n",
            "Graph 0 0 0 0 2 \n",
            "equals 0 0 0 0 2 \n",
            "100 0 0 0 0 1 \n",
            "101 0 0 0 0 2 \n",
            "false 0 0 0 0 1 \n",
            "synonyms 0 0 0 0 3 \n",
            "antonyms 0 0 0 0 4 \n",
            "definitions 0 0 0 0 5 \n",
            "102 0 0 0 0 1 \n",
            "103 0 0 0 0 1 \n",
            "Gathering 0 0 0 0 1 \n",
            "104 0 0 0 0 1 \n",
            "105 0 0 0 0 1 \n",
            "Hypernyms 0 0 0 0 2 \n",
            "abstract 0 0 0 0 1 \n",
            "term 0 0 0 0 6 \n",
            "106 0 0 0 0 1 \n",
            "hypernym 0 0 0 0 1 \n",
            "Hyponyms 0 0 0 0 2 \n",
            "107 0 0 0 0 1 \n",
            "hyponym 0 0 0 0 1 \n",
            "Get 0 0 0 0 1 \n",
            "108 0 0 0 0 1 \n",
            "Synonyms 0 0 0 0 2 \n",
            "109 0 0 0 0 1 \n",
            "Antonyms 0 0 0 0 1 \n",
            "110 0 0 0 0 1 \n",
            "111 0 0 0 0 1 \n",
            "112 0 0 0 0 1 \n",
            "ratio 0 0 0 0 2 \n",
            "113 0 0 0 0 1 \n",
            "114 0 0 0 0 1 \n",
            "discard 0 0 0 0 1 \n",
            "occurrences 0 0 0 0 2 \n",
            "counts 0 0 0 0 1 \n",
            "summary 0 0 0 0 1 \n",
            "115 0 0 0 0 1 \n",
            "Raw 0 0 0 0 1 \n",
            "Clean 0 0 0 0 2 \n",
            "contains 0 0 0 0 8 \n",
            "Tokenize 0 0 0 0 1 \n",
            "Vocab 0 0 0 0 2 \n",
            "Generate 0 0 0 0 1 \n",
            "frequencies 0 0 0 0 3 \n",
            "Jim 0 0 0 0 1 \n",
            "Pam 0 0 0 0 1 \n",
            "traveled 0 0 0 0 1 \n",
            "bus 0 0 0 0 1 \n",
            "late 0 0 0 0 1 \n",
            "flight 0 0 0 0 2 \n",
            "Traveling 0 0 0 0 1 \n",
            "expensive 0 0 0 0 1 \n",
            "116 0 0 0 0 1 \n",
            "117 0 0 0 0 1 \n",
            "Combining 0 0 0 0 1 \n",
            "118 0 0 0 0 1 \n",
            "Combination 0 0 0 0 1 \n",
            "119 0 0 0 0 1 \n",
            "120 0 0 0 0 1 \n",
            "121 0 0 0 0 1 \n",
            "Output 0 0 0 0 2 \n",
            "122 0 0 0 0 1 \n",
            "Limitations 0 0 0 0 1 \n",
            "ignores 0 0 0 0 1 \n",
            "Preprocessing 0 0 0 0 1 \n",
            "preprocessing 0 0 0 0 1 \n",
            "cleansing 0 0 0 0 1 \n",
            "TF 0 0 0 0 34 \n",
            "IDF 0 0 0 0 40 \n",
            "stands 0 0 0 0 1 \n",
            "Term 0 0 0 0 3 \n",
            "Frequency 0 0 0 0 6 \n",
            "Inverse 0 0 0 0 3 \n",
            "Document 0 0 0 0 3 \n",
            "scoring 0 0 0 0 1 \n",
            "measure 0 0 0 0 1 \n",
            "score 0 0 0 0 4 \n",
            "intuition 0 0 0 0 1 \n",
            "appears 0 0 0 0 2 \n",
            "importance 0 0 0 0 2 \n",
            "fewer 0 0 0 0 2 \n",
            "maybe 0 0 0 0 1 \n",
            "cannot 0 0 0 0 1 \n",
            "dog 0 0 0 0 7 \n",
            "descriptions 0 0 0 0 5 \n",
            "wants 0 0 0 0 1 \n",
            "cute 0 0 0 0 6 \n",
            "closest 0 0 0 0 3 \n",
            "possibly 0 0 0 0 1 \n",
            "suppose 0 0 0 0 1 \n",
            "furry 0 0 0 0 1 \n",
            "doggo 0 0 0 0 4 \n",
            "lovely 0 0 0 0 1 \n",
            "description 0 0 0 0 5 \n",
            "forth 0 0 0 0 1 \n",
            "calculates 0 0 0 0 2 \n",
            "none 0 0 0 0 1 \n",
            "repeat 0 0 0 0 2 \n",
            "instrumental 0 0 0 0 1 \n",
            "Eventually 0 0 0 0 1 \n",
            "increases 0 0 0 0 1 \n",
            "discriminative 0 0 0 0 1 \n",
            "Simply 0 0 0 0 1 \n",
            "put 0 0 0 0 1 \n",
            "rarer 0 0 0 0 1 \n",
            "vice 0 0 0 0 1 \n",
            "versa 0 0 0 0 1 \n",
            "123 0 0 0 0 1 \n",
            "Calculation 0 0 0 0 1 \n",
            "Represent 0 0 0 0 1 \n",
            "table 0 0 0 0 1 \n",
            "124 0 0 0 0 1 \n",
            "125 0 0 0 0 1 \n",
            "Calculating 0 0 0 0 12 \n",
            "formula 0 0 0 0 4 \n",
            "126 0 0 0 0 1 \n",
            "127 0 0 0 0 1 \n",
            "Resulting 0 0 0 0 1 \n",
            "128 0 0 0 0 1 \n",
            "129 0 0 0 0 1 \n",
            "multiplication 0 0 0 0 2 \n",
            "130 0 0 0 0 1 \n",
            "resulting 0 0 0 0 1 \n",
            "discriminate 0 0 0 0 1 \n",
            "smoothing 0 0 0 0 1 \n",
            "variation 0 0 0 0 1 \n",
            "log 0 0 0 0 6 \n",
            "again 0 0 0 0 1 \n",
            "131 0 0 0 0 1 \n",
            "132 0 0 0 0 1 \n",
            "133 0 0 0 0 1 \n",
            "calculations 0 0 0 0 1 \n",
            "formulas 0 0 0 0 1 \n",
            "Actual 0 0 0 0 2 \n",
            "Calculations 0 0 0 0 1 \n",
            "134 0 0 0 0 1 \n",
            "calculation 0 0 0 0 1 \n",
            "Formula 0 0 0 0 1 \n",
            "136 0 0 0 0 1 \n",
            "Applying 0 0 0 0 1 \n",
            "137 0 0 0 0 1 \n",
            "138 0 0 0 0 1 \n",
            "139 0 0 0 0 1 \n",
            "140 0 0 0 0 1 \n",
            "Conclusion 0 0 0 0 1 \n",
            "hope 0 0 0 0 1 \n",
            "enjoyed 0 0 0 0 1 \n",
            "reading 0 0 0 0 1 \n",
            "suggestions 0 0 0 0 1 \n",
            "continue 0 0 0 0 1 \n",
            "views 0 0 0 0 2 \n",
            "expressed 0 0 0 0 1 \n",
            "author 0 0 0 0 3 \n",
            "Carnegie 0 0 0 0 2 \n",
            "Mellon 0 0 0 0 2 \n",
            "University 0 0 0 0 2 \n",
            "nor 0 0 0 0 1 \n",
            "directly 0 0 0 0 1 \n",
            "indirectly 0 0 0 0 1 \n",
            "associated 0 0 0 0 1 \n",
            "writings 0 0 0 0 1 \n",
            "intend 0 0 0 0 1 \n",
            "rather 0 0 0 0 1 \n",
            "reflection 0 0 0 0 1 \n",
            "thinking 0 0 0 0 1 \n",
            "catalyst 0 0 0 0 1 \n",
            "discussion 0 0 0 0 1 \n",
            "improvement 0 0 0 0 1 \n",
            "Citation 0 0 0 0 1 \n",
            "contexts 0 0 0 0 1 \n",
            "cite 0 0 0 0 1 \n",
            "BibTex 0 0 0 0 1 \n",
            "citation 0 0 0 0 1 \n",
            "pratik_iriondo_2020 0 0 0 0 1 \n",
            "title 0 0 0 0 1 \n",
            "journal 0 0 0 0 1 \n",
            "year 0 0 0 0 1 \n",
            "month 0 0 0 0 1 \n",
            "Jul 0 0 0 0 1 \n",
            "gathered 0 0 0 0 1 \n",
            "American 0 0 0 0 1 \n",
            "Literature 0 0 0 0 1 \n",
            "americanliterature 0 0 0 0 1 \n",
            "KDnuggets 0 0 0 0 1 \n",
            "kdnuggets 0 0 0 0 1 \n",
            "wtf 0 0 0 0 1 \n",
            "idf 0 0 0 0 1 \n",
            "Multidisciplinary 0 0 0 0 2 \n",
            "Journal 0 0 0 0 2 \n",
            "head 0 0 0 0 1 \n",
            "Snorkel 0 0 0 0 2 \n",
            "Silicon 0 0 0 0 1 \n",
            "Valley 0 0 0 0 1 \n",
            "unicorn 0 0 0 0 1 \n",
            "empowers 0 0 0 0 1 \n",
            "scientists 0 0 0 0 1 \n",
            "developers 0 0 0 0 1 \n",
            "scalable 0 0 0 0 1 \n",
            "Flow 0 0 0 0 1 \n",
            "centric 0 0 0 0 1 \n",
            "platform 0 0 0 0 1 \n",
            "co 0 0 0 0 7 \n",
            "founder 0 0 0 0 2 \n",
            "advisor 0 0 0 0 1 \n",
            "CEO 0 0 0 0 1 \n",
            "internationally 0 0 0 0 1 \n",
            "servicing 0 0 0 0 1 \n",
            "startups 0 0 0 0 2 \n",
            "enterprises 0 0 0 0 2 \n",
            "clients 0 0 0 0 1 \n",
            "Asia 0 0 0 0 2 \n",
            "Europe 0 0 0 0 1 \n",
            "Previously 0 0 0 0 1 \n",
            "acquired 0 0 0 0 1 \n",
            "engineer 0 0 0 0 1 \n",
            "marketing 0 0 0 0 2 \n",
            "Department 0 0 0 0 1 \n",
            "strategist 0 0 0 0 1 \n",
            "helped 0 0 0 0 1 \n",
            "goals 0 0 0 0 1 \n",
            "significantly 0 0 0 0 1 \n",
            "ROIs 0 0 0 0 1 \n",
            "MBZUAI 0 0 0 0 1 \n",
            "Gather 0 0 0 0 1 \n",
            "Determined 0 0 0 0 2 \n",
            "Lambda 0 0 0 0 1 \n",
            "Labs 0 0 0 0 1 \n",
            "Udacity 0 0 0 0 1 \n",
            "Setting 0 0 0 0 1 \n",
            "Alpaca 0 0 0 0 1 \n",
            "API 0 0 0 0 1 \n",
            "algorithmic 0 0 0 0 1 \n",
            "trading 0 0 0 0 1 \n",
            "Time 0 0 0 0 2 \n",
            "Series 0 0 0 0 2 \n",
            "prediction 0 0 0 0 1 \n",
            "Adaptive 0 0 0 0 1 \n",
            "Gradient 0 0 0 0 3 \n",
            "Descent 0 0 0 0 3 \n",
            "Variants 0 0 0 0 1 \n",
            "Mathematical 0 0 0 0 1 \n",
            "Intuition 0 0 0 0 1 \n",
            "Which 0 0 0 0 1 \n",
            "Task 0 0 0 0 1 \n",
            "NOT 0 0 0 0 1 \n",
            "Benefit 0 0 0 0 1 \n",
            "Pre 0 0 0 0 1 \n",
            "Avremee 0 0 0 0 1 \n",
            "August 0 0 0 0 1 \n",
            "explanations 0 0 0 0 1 \n",
            "thank 0 0 0 0 1 \n",
            "Shivakumar 0 0 0 0 1 \n",
            "Kokkula 0 0 0 0 1 \n",
            "June 0 0 0 0 3 \n",
            "Superb 0 0 0 0 1 \n",
            "Hemalatha 0 0 0 0 1 \n",
            "Abothula 0 0 0 0 1 \n",
            "javed 0 0 0 0 1 \n",
            "khan 0 0 0 0 1 \n",
            "explanation 0 0 0 0 1 \n",
            "Feedback 0 0 0 0 1 \n",
            "Select 0 0 0 0 1 \n",
            "Category 0 0 0 0 1 \n",
            "Cryptography 0 0 0 0 1 \n",
            "Centric 0 0 0 0 1 \n",
            "Futuristic 0 0 0 0 1 \n",
            "Innovation 0 0 0 0 1 \n",
            "Internet 0 0 0 0 1 \n",
            "Things 0 0 0 0 1 \n",
            "Logic 0 0 0 0 1 \n",
            "Scientific 0 0 0 0 1 \n",
            "Weak 0 0 0 0 1 \n",
            "Supervision 0 0 0 0 1 \n",
            "Popular 0 0 0 0 1 \n",
            "Best 0 0 0 0 4 \n",
            "Workstations 0 0 0 0 1 \n",
            "Descriptive 0 0 0 0 1 \n",
            "Decision 0 0 0 0 2 \n",
            "Making 0 0 0 0 1 \n",
            "Paid 0 0 0 0 2 \n",
            "Recommendations 0 0 0 0 2 \n",
            "Laptops 0 0 0 0 1 \n",
            "newsletter 0 0 0 0 3 \n",
            "Seal 0 0 0 0 1 \n",
            "Containerized 0 0 0 0 1 \n",
            "Deal 0 0 0 0 1 \n",
            "Podman 0 0 0 0 1 \n",
            "March 0 0 0 0 5 \n",
            "Gaussian 0 0 0 0 1 \n",
            "Naive 0 0 0 0 1 \n",
            "Bayes 0 0 0 0 1 \n",
            "Hands 0 0 0 0 2 \n",
            "Scikit 0 0 0 0 1 \n",
            "Manipulating 0 0 0 0 1 \n",
            "Logistic 0 0 0 0 1 \n",
            "Regression 0 0 0 0 2 \n",
            "Binary 0 0 0 0 1 \n",
            "Multiclass 0 0 0 0 1 \n",
            "SciKit 0 0 0 0 1 \n",
            "vision 0 0 0 0 1 \n",
            "Convolutional 0 0 0 0 1 \n",
            "finder 0 0 0 0 2 \n",
            "demystifying 0 0 0 0 1 \n",
            "series 0 0 0 0 1 \n",
            "inventory 0 0 0 0 1 \n",
            "management 0 0 0 0 1 \n",
            "Japan 0 0 0 0 1 \n",
            "Linear 0 0 0 0 2 \n",
            "Algebra 0 0 0 0 1 \n",
            "Reinforcement 0 0 0 0 1 \n",
            "Yolo 0 0 0 0 2 \n",
            "V5 0 0 0 0 1 \n",
            "World 0 0 0 0 1 \n",
            "Leading 0 0 0 0 1 \n",
            "CompanyHome 0 0 0 0 1 \n",
            "228 0 0 0 0 1 \n",
            "Park 0 0 0 0 1 \n",
            "Avenue 0 0 0 0 1 \n",
            "South 0 0 0 0 1 \n",
            "PMB 0 0 0 0 1 \n",
            "99625 0 0 0 0 1 \n",
            "York 0 0 0 0 1 \n",
            "NY 0 0 0 0 1 \n",
            "10003 0 0 0 0 1 \n",
            "protected 0 0 0 0 3 \n",
            "offerings 0 0 0 0 1 \n",
            "650 0 0 0 0 1 \n",
            "246 0 0 0 0 1 \n",
            "9381 0 0 0 0 1 \n",
            "Workflow 0 0 0 0 0 \n",
            "mw 0 0 0 0 5 \n",
            "rjDeW8udu1 0 0 0 0 0 \n",
            "latest 0 0 0 0 3 \n",
            "486IDngdMK 0 0 0 0 0 \n",
            "gradientdescent 0 0 0 0 0 \n",
            "SqXqTxvqMi 0 0 0 0 0 \n",
            "Stop 0 0 0 0 0 \n",
            "Elbow 0 0 0 0 0 \n",
            "Diagram 0 0 0 0 0 \n",
            "Value 0 0 0 0 0 \n",
            "And 0 0 0 0 0 \n",
            "Instead 0 0 0 0 0 \n",
            "TowardsAI 0 0 0 0 0 \n",
            "pKJs82yICg 0 0 0 0 0 \n",
            "eLK2UheCQ4 0 0 0 0 0 \n",
            "Hypothesis 0 0 0 0 0 \n",
            "Testing 0 0 0 0 0 \n",
            "Examples 0 0 0 0 0 \n",
            "y12J8D8HGF 0 0 0 0 0 \n",
            "Public 0 0 0 0 0 \n",
            "Messages 0 0 0 0 0 \n",
            "Ethereum 0 0 0 0 0 \n",
            "fWF9WOrpex 0 0 0 0 0 \n",
            "Interesting 0 0 0 0 1 \n",
            "Facts 0 0 0 0 1 \n",
            "Are 0 0 0 0 0 \n",
            "Less 0 0 0 0 0 \n",
            "Familiar 0 0 0 0 0 \n",
            "Float 0 0 0 0 0 \n",
            "Type 0 0 0 0 0 \n",
            "2t40mKPDwz 0 0 0 0 0 \n",
            "programming 0 0 0 0 1 \n",
            "Rights 0 0 0 0 1 \n",
            "Reserved 0 0 0 0 1 \n",
            "remembering 0 0 0 0 1 \n",
            "preferences 0 0 0 0 1 \n",
            "visits 0 0 0 0 1 \n",
            "Accept 0 0 0 0 1 \n",
            "ALL 0 0 0 0 1 \n",
            "Do 0 0 0 0 1 \n",
            "sell 0 0 0 0 1 \n",
            "Cookie 0 0 0 0 6 \n",
            "SettingsAcceptManage 0 0 0 0 1 \n",
            "ensure 0 0 0 0 1 \n",
            "anonymously 0 0 0 0 1 \n",
            "CookieDurationDescriptioncookielawinfo 0 0 0 0 1 \n",
            "checkbox 0 0 0 0 5 \n",
            "analytics11 0 0 0 0 1 \n",
            "monthsThis 0 0 0 0 4 \n",
            "cookie 0 0 0 0 10 \n",
            "GDPR 0 0 0 0 6 \n",
            "Consent 0 0 0 0 5 \n",
            "plugin 0 0 0 0 5 \n",
            "cookielawinfo 0 0 0 0 4 \n",
            "functional11 0 0 0 0 1 \n",
            "monthsThe 0 0 0 0 2 \n",
            "record 0 0 0 0 1 \n",
            "Functional 0 0 0 0 4 \n",
            "necessary11 0 0 0 0 1 \n",
            "others11 0 0 0 0 1 \n",
            "performance11 0 0 0 0 1 \n",
            "Performance 0 0 0 0 4 \n",
            "viewed_cookie_policy11 0 0 0 0 1 \n",
            "consented 0 0 0 0 1 \n",
            "sharing 0 0 0 0 1 \n",
            "platforms 0 0 0 0 1 \n",
            "feedbacks 0 0 0 0 1 \n",
            "performance 0 0 0 0 1 \n",
            "delivering 0 0 0 0 1 \n",
            "visitors 0 0 0 0 5 \n",
            "Analytical 0 0 0 0 1 \n",
            "interact 0 0 0 0 1 \n",
            "bounce 0 0 0 0 1 \n",
            "rate 0 0 0 0 1 \n",
            "Advertisement 0 0 0 0 3 \n",
            "customized 0 0 0 0 1 \n",
            "Others 0 0 0 0 2 \n",
            "uncategorized 0 0 0 0 1 \n",
            "Hit 0 0 0 0 1 \n",
            "enter 0 0 0 0 1 \n",
            "ESC 0 0 0 0 1 \n",
            "close 0 0 0 0 1 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataframe da matriz de palavras extraídas dos textos, listando a frequência de termos por texto"
      ],
      "metadata": {
        "id": "K5nEOZmFAkux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "def generate_bag_of_words_dataframe(urls):\n",
        "  documents = get_all_texts(urls)\n",
        "  vocab = get_vocabulary_from_texts(urls)\n",
        "  dictionaries = []\n",
        "  for document in documents:\n",
        "    dictionary = dict.fromkeys(document, 0)\n",
        "    for word in dictionary:\n",
        "      dictionary[word] = document.count(word)\n",
        "    dictionaries.append(dictionary)\n",
        "  df = pd.DataFrame(dictionaries)\n",
        "  df = df.replace(np.nan, 0)\n",
        "  df = df.apply(np.int64)\n",
        "  display(df)\n",
        "generate_bag_of_words_dataframe(urls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "zdc0UupRS180",
        "outputId": "c5817f14-5577-461b-ccc0-5da9e55a2f19"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Skip   to  content  IBM  Cloud  Learn  Hub  What   is  Natural  ...  \\\n",
              "0     1   54        1   11      3      1    1     7   28       15  ...   \n",
              "1     1   65        2    0      2      0    0     1   26        2  ...   \n",
              "2     0   54        0    0      0      0    0     2   49        1  ...   \n",
              "3     1   24        2    0      1      0    0     0   13        3  ...   \n",
              "4     0  128        4    0     10      4    0     3  104       28  ...   \n",
              "\n",
              "   bounce  rate  Advertisement  customized  Others  uncategorized  Hit  enter  \\\n",
              "0       0     0              0           0       0              0    0      0   \n",
              "1       0     0              0           0       0              0    0      0   \n",
              "2       0     0              0           0       0              0    0      0   \n",
              "3       0     0              0           0       0              0    0      0   \n",
              "4       1     1              3           1       2              1    1      1   \n",
              "\n",
              "   ESC  close  \n",
              "0    0      0  \n",
              "1    0      0  \n",
              "2    0      0  \n",
              "3    0      0  \n",
              "4    1      1  \n",
              "\n",
              "[5 rows x 3472 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e872d67f-cd59-4f4a-9a3a-171541d27766\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Skip</th>\n",
              "      <th>to</th>\n",
              "      <th>content</th>\n",
              "      <th>IBM</th>\n",
              "      <th>Cloud</th>\n",
              "      <th>Learn</th>\n",
              "      <th>Hub</th>\n",
              "      <th>What</th>\n",
              "      <th>is</th>\n",
              "      <th>Natural</th>\n",
              "      <th>...</th>\n",
              "      <th>bounce</th>\n",
              "      <th>rate</th>\n",
              "      <th>Advertisement</th>\n",
              "      <th>customized</th>\n",
              "      <th>Others</th>\n",
              "      <th>uncategorized</th>\n",
              "      <th>Hit</th>\n",
              "      <th>enter</th>\n",
              "      <th>ESC</th>\n",
              "      <th>close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>28</td>\n",
              "      <td>15</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>65</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>49</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>13</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>128</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>104</td>\n",
              "      <td>28</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 3472 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e872d67f-cd59-4f4a-9a3a-171541d27766')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e872d67f-cd59-4f4a-9a3a-171541d27766 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e872d67f-cd59-4f4a-9a3a-171541d27766');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP6xOkQExa5CSniczIGHn4G",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}